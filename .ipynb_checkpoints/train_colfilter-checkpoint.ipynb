{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a573191-c50c-46a7-be39-8d9e9ad5ae8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from db.ipynb\n",
      "best_col_model_checkpoint.pth\t\tmain.py\n",
      "best_contentfiltermodel_checkpoint.pth\tmenv\n",
      "best_model_checkpoint.pth\t\tmlruns\n",
      "best_model_state.pth\t\t\tmodels\n",
      "cf_model.py\t\t\t\tmovie.py\n",
      "col_encoder.pkl\t\t\t\tpath_to_your_database.db\n",
      "config.py\t\t\t\t__pycache__\n",
      "dataset.py\t\t\t\tq.py\n",
      "db.ipynb\t\t\t\tREADME.md\n",
      "Dockerfile\t\t\t\trequirements.txt\n",
      "done.txt\t\t\t\tserver\n",
      "encoder.pkl\t\t\t\ttest.db\n",
      "fdstests.ipynb\t\t\t\ttestsources.ipynb\n",
      "filemanager.py\t\t\t\ttodo.txt\n",
      "holocenemodels\t\t\t\ttrain_colfilter.ipynb\n",
      "indie_letterboxd.db\t\t\ttrain_confilter.ipynb\n",
      "indie_letterboxd_v2.db\t\t\tuser.py\n",
      "letterboxd\t\t\t\twandb\n",
      "letterboxd.db\t\t\t\tweb_spider.py\n",
      "main.ipynb\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from cf_model import MLPCollaborativeFilter\n",
    "from dataset import ColFDataset, Encoder, split_data\n",
    "import import_ipynb\n",
    "import db\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4967b732-0b9e-4747-a84f-07e3e8f4dd71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users: 2400\n",
      "\n",
      "Movies: 771\n",
      "\n",
      "Reviews: 18504\n"
     ]
    }
   ],
   "source": [
    "DB_NAME = 'indie_letterboxd_v2'\n",
    "db.display_size(DB_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28be6c6f-cf28-486e-9aa5-78b8ff780a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(DB_NAME+'.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# SQL query to fetch user name and ID, movie name and ID, and rating from reviews\n",
    "query = '''\n",
    "    SELECT \n",
    "        u.user_id,\n",
    "        u.name AS user_name, \n",
    "        m.movie_id,\n",
    "        m.title AS movie_title, \n",
    "        r.rating\n",
    "    FROM \n",
    "        Reviews r\n",
    "    JOIN Users u ON r.user_id = u.user_id\n",
    "    JOIN Movies m ON r.movie_id = m.movie_id\n",
    "    '''\n",
    "\n",
    "try:\n",
    "    cursor.execute(query)\n",
    "    reviews = cursor.fetchall()\n",
    "    \n",
    "    # for review in reviews:\n",
    "    #     print(\"User ID:\", review[0], \"| User Name:\", review[1], \"| Movie ID:\", review[2], \"| Movie Title:\", review[3], \"| Rating:\", review[4])\n",
    "except sqlite3.Error as e:\n",
    "    print(\"An error occurred:\", e)\n",
    "finally:\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8802ab01-9d57-4fdf-8341-215f3e0212ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Michelle Hess', 'Sycorax', 0.6), ('Jeffrey Hill', 'Shadow of Fire', 0.9), ('Karin Petersson', 'Adam', 0.8), ('고성훈', 'Mother Couch', 0.6), ('Micheletto Paltrinieri', 'Aftersun', 0.9), ('Charlotte Perez', 'Hate to Love Nickelback', 0.4), ('Nichole Bray', 'Concrete Utopia', 0.7), ('Jessica Roman', 'Widow Clicquot', 0.6), ('ავთანდილ ხარატიშვილი', 'Weathering with You', 0.8), ('Helena Lappalainen', 'Synonyms', 0.7)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(db.display_size())\n",
    "#TODO change shuffle and not shuffle\n",
    "\n",
    "# Assuming 'reviews' is a list of tuples and you've already created 'data'\n",
    "data = [tuple([did[1], did[3], did[-1]]) for did in reviews]\n",
    "\n",
    "# Shuffle 'data' in place with random.shuffle()\n",
    "random.shuffle(data)\n",
    "\n",
    "# Now 'data' is shuffled, and you can work with it\n",
    "print(data[:10])\n",
    "#found invalid values earlier.\n",
    "\n",
    "data = [pre for pre in data if pre[-1] > 0]\n",
    "data.count(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a8d4986-83f4-4fbc-8bba-429cda6a865e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test data is holdout for Kfolds cross data validation evaluation.\n",
    "train_data, test_data = split_data(data)\n",
    "train_data, validation_data = split_data(train_data,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2eccd133-b107-4dfa-9b16-b07bc58b375b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13241"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2438744-302d-4f6b-9a63-d49b2eadbd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_users_data = [ud[0] for ud in train_data]\n",
    "train_movies_data = [md[1] for md in train_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1b2649-c1fb-48bd-ab55-030b640199e7",
   "metadata": {},
   "source": [
    "## after splitting, only use the training data --- for encoding!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfbc8886-5861-4526-9488-ed53e502a2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(users=train_users_data,movies=train_movies_data)###<---- important\n",
    "import pickle\n",
    "\n",
    "# Assuming 'encoder' is your encoder object\n",
    "with open('col_encoder.pkl', 'wb') as file:\n",
    "    pickle.dump(encoder, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "319e8a9c-44a3-44d4-99c7-ad3b042220e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ColFDataset(train_data,encoder)\n",
    "test_ds = ColFDataset(test_data,encoder)\n",
    "validation_ds = ColFDataset(validation_data,encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af64661d-4749-45e3-a3a9-dcfdb27b43ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1471"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds.movie_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a97f9cb-f0f1-45db-b175-4293529fa52d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.movie_ids.count(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae3212ab-ac1f-4efa-b29a-a509247f08d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13241"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validation_ds.movie_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75750116-45b4-4af9-be6e-d3b828a05311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7142"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_ds.user_ids.count(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2a65e0d-f096-4312-b02e-2ccbbb1a38af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2159"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_ds.movie_ids.count(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cbc125dc-8fca-445a-8f90-ea504b004900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3679"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_ds.movie_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83a82ced-863d-4bd4-b00d-a99a24047c89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "579"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds.movie_ids.count(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75616c10-0c39-460b-a33d-9507b9043ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "train_data_loader = DataLoader(train_ds,batch_size=BATCH_SIZE,shuffle=False,drop_last=True)\n",
    "validation_data_loader = DataLoader(validation_ds,batch_size=BATCH_SIZE,shuffle=False,drop_last=True)\n",
    "test_data_loader = DataLoader(test_ds,batch_size=BATCH_SIZE,shuffle=False,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36edabff-0087-49fd-9cbf-c22e06c27b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users = len(encoder.vocab_to_idx['users'])\n",
    "num_movies = len(encoder.vocab_to_idx['movies'])\n",
    "FEATURES=700\n",
    "model = MLPCollaborativeFilter(num_users + 1, num_movies + 1, embedding_dim=FEATURES)\n",
    "\n",
    "\n",
    "LEARNING_RATE = 0.001\n",
    "criterion = nn.MSELoss()\n",
    "#weight decay L2 regularization\n",
    "# optimiser = optim.SGD(model.parameters(), lr=0.001,weight_decay=1e-5)\\\n",
    "L2_REGULARIZATION=0.1\n",
    "optimiser = optim.SGD(model.parameters(), lr=LEARNING_RATE,weight_decay=L2_REGULARIZATION)\n",
    "EPOCHS = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "157cd204-76fe-473a-9e57-a7adda48b185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18391\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1471"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(data))\n",
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0aea2d-2597-4a50-9b6d-d65231ed49eb",
   "metadata": {},
   "source": [
    "# EDA "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadfc0f4-b4df-4835-85b1-ebae07f11741",
   "metadata": {},
   "source": [
    "    Examine the data for patterns, anomalies, or characteristics.\n",
    "    Check for data quality issues such as missing values, outliers, or incorrect data types. <--- either predropped or imputed.\n",
    "    Get a sense of the distributions of your variables and the relationships between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1228065-7071-4b93-b460-a4204e6fbbda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     user           movie  ratings\n",
      "0           Michelle Hess         Sycorax      0.6\n",
      "1            Jeffrey Hill  Shadow of Fire      0.9\n",
      "2         Karin Petersson            Adam      0.8\n",
      "3                     고성훈    Mother Couch      0.6\n",
      "4  Micheletto Paltrinieri        Aftersun      0.9\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 18391 entries, 0 to 18390\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   user     18391 non-null  object \n",
      " 1   movie    18391 non-null  object \n",
      " 2   ratings  18391 non-null  float64\n",
      "dtypes: float64(1), object(2)\n",
      "memory usage: 431.2+ KB\n",
      "None\n",
      "            ratings\n",
      "count  18391.000000\n",
      "mean       0.674751\n",
      "std        0.210540\n",
      "min        0.020000\n",
      "25%        0.540000\n",
      "50%        0.700000\n",
      "75%        0.800000\n",
      "max        1.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "column_names = ['user', 'movie','ratings']\n",
    "\n",
    "df = pd.DataFrame(data, columns=column_names)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(df.head())\n",
    "\n",
    "# Get a summary of the dataset\n",
    "print(df.info())\n",
    "\n",
    "# Generate descriptive statistics\n",
    "print(df.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c02bbd90-8966-44b8-9ddd-59d31ce3d82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user       0\n",
      "movie      0\n",
      "ratings    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "51da5313-6bec-42a9-835e-2c535197d382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x1500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 250x250 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Histograms for numeric features\n",
    "df.hist(bins=50, figsize=(20,15))\n",
    "plt.show()\n",
    "\n",
    "# Box plots for distributions and spotting outliers\n",
    "sns.boxplot(data=df)\n",
    "plt.show()\n",
    "\n",
    "# Pairplot to visualize pairwise relationships between features\n",
    "sns.pairplot(df)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6b05fd44-31ed-4c75-b0b2-5558d525bd0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>movie</th>\n",
       "      <th>ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [user, movie, ratings]\n",
       "Index: []"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['ratings'] == -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5fe39638-e29b-49ca-bf75-4d8d44ace1c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     7\n",
       "1    14\n",
       "2     4\n",
       "3    12\n",
       "4     8\n",
       "Name: title_length, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['title_length'] = df['movie'].str.len()\n",
    "df['title_length'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6c4fe575-c792-46de-9fac-c4e280e4889f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "df['title_length']\n",
    "# Correlation matrix\n",
    "\n",
    "corr_matrix = df[['title_length','ratings']].corr()\n",
    "\n",
    "# Heatmap of the correlation matrix\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\")\n",
    "plt.show()\n",
    "\n",
    "# Specifically look at correlations with the target variable if defined\n",
    "# target_corr = corr_matrix[\"YourTargetVariable\"].sort_values(ascending=False)\n",
    "# print(target_corr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dc364699-eaa9-4bd5-9e46-359b19c0271f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26\n",
      "                      user                          movie  ratings  \\\n",
      "15          Aldo Filipović  The Happiest Man in the World     0.10   \n",
      "215      Φανουρία Κατσούλα                    Falcon Lake     0.15   \n",
      "239         Coral de Campo                       Nightalk     0.10   \n",
      "385        Մաքսիմ Աթասունց                        Dashcam     0.10   \n",
      "517      Marjatta Kukkonen                      The Beast     0.10   \n",
      "...                    ...                            ...      ...   \n",
      "18083        Varvara Tudor         Navozande the Musician     0.02   \n",
      "18234    Войнка Пенджакова                     Saint Omer     0.06   \n",
      "18329  Elisabet Pettersson                    Aggro Dr1ft     0.10   \n",
      "18347       اميرمحمد جلیلی              The Human Surge 3     0.15   \n",
      "18358         Ιουλία Τσάκη            How to Build a Girl     0.10   \n",
      "\n",
      "       title_length  \n",
      "15               29  \n",
      "215              11  \n",
      "239               8  \n",
      "385               7  \n",
      "517               9  \n",
      "...             ...  \n",
      "18083            22  \n",
      "18234            10  \n",
      "18329            11  \n",
      "18347            17  \n",
      "18358            19  \n",
      "\n",
      "[300 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Example of finding outliers for a 'feature_name'\n",
    "Q1 = df['ratings'].quantile(0.25)\n",
    "Q3 = df['ratings'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "print(IQR)\n",
    "outlier_condition = ((df['ratings'] < (Q1 - 1.5 * IQR)) | (df['ratings'] > (Q3 + 1.5 * IQR)))\n",
    "outliers = df[outlier_condition]\n",
    "print(outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5dbea2-ea83-4f1a-b206-60a993873cfe",
   "metadata": {},
   "source": [
    "## MLFLOw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ae300159-0298-450a-b512-ddf65ad3d207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///home/james/Desktop/torchex/movie-users/mlruns/827152334394031969', creation_time=1710099404696, experiment_id='827152334394031969', last_update_time=1710099404696, lifecycle_stage='active', name='Finetuned Collaborative Filter', tags={}>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# Optional: Set MLflow experiment\n",
    "mlflow.set_experiment(\"Finetuned Collaborative Filter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5bebaeb2-02a1-47d8-8f20-28d488b69201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Log model parameters (example)\n",
    "mlflow.start_run()\n",
    "mlflow.log_param(\"epochs\", EPOCHS)\n",
    "mlflow.log_param(\"optimizer\", type(optimiser).__name__)\n",
    "mlflow.log_param(\"learning rate\", LEARNING_RATE)\n",
    "mlflow.log_param(\"batch size\", BATCH_SIZE)\n",
    "mlflow.log_param(\"L2 regularization\", L2_REGULARIZATION)\n",
    "mlflow.log_param(\"drop out\", 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623dc331-6d00-41db-b670-fe7c76a9d10b",
   "metadata": {},
   "source": [
    "# PRETEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1f13953d-a5bc-4fd2-8d48-3c323be56cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.44944204834469575\n"
     ]
    }
   ],
   "source": [
    "#test data\n",
    "model.eval()  # Switch to evaluation mode\n",
    "test_loss = 0\n",
    "correct_predictions = 0  # Fixed variable name for clarity\n",
    "total_contexts = 0\n",
    "\n",
    "with torch.no_grad():  # Disable gradient computation\n",
    "    for user_ids, movie_ids, ratings in test_data_loader:\n",
    "        predictions = model(user_ids,movie_ids)  # Generate predictions\n",
    "        # Calculate and accumulate loss\n",
    "        loss = criterion(predictions, ratings)\n",
    "        test_loss += loss.item()\n",
    "        \n",
    "\n",
    "test_loss /= len(test_data_loader)  # Average loss per batch\n",
    "\n",
    "print(test_loss)\n",
    "\n",
    "mlflow.log_metric('pretraining test loss',test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39e0f67-b1d4-4a01-826a-4e37d9fb5127",
   "metadata": {},
   "source": [
    "## wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "000ffd1c-94ac-4276-bede-a4b7f186794f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjcrich\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/james/Desktop/torchex/movie-users/wandb/run-20240312_102644-cq244pkg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jcrich/collaborative%20filter%20model/runs/cq244pkg' target=\"_blank\">upbeat-fire-38</a></strong> to <a href='https://wandb.ai/jcrich/collaborative%20filter%20model' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jcrich/collaborative%20filter%20model' target=\"_blank\">https://wandb.ai/jcrich/collaborative%20filter%20model</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jcrich/collaborative%20filter%20model/runs/cq244pkg' target=\"_blank\">https://wandb.ai/jcrich/collaborative%20filter%20model/runs/cq244pkg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/jcrich/collaborative%20filter%20model/runs/cq244pkg?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f6e0b337460>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "import random\n",
    "\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    entity=\"jcrich\",\n",
    "    project=\"collaborative filter model\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"architecture\": \"collaborative filter\",\n",
    "    \"dataset\": \"imdb\",\n",
    "    \"epochs\": EPOCHS,\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959003c7-6d39-4392-b3f5-e9a75a36d0b2",
   "metadata": {},
   "source": [
    "# KFOLDS CROSS VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8631385b-7cfe-427b-b2db-427241171649",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPLit\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Configuration\n",
    "num_folds = 10\n",
    "data_size = len(test_ds)  # Assuming 'dataset' is a PyTorch Dataset\n",
    "\n",
    "# Define the K-fold Cross Validator\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8b0830cd-43f1-454c-8e81-72e5d73549fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0\n",
      "--------------------------------\n",
      "Validation Loss for fold 0: 0.5569881002108256\n",
      "Validation Loss for fold 0: 0.42811007301012677\n",
      "Validation Loss for fold 0: 0.32452592253685\n",
      "Validation Loss for fold 0: 0.255699689189593\n",
      "Validation Loss for fold 0: 0.22702384988466898\n",
      "Validation Loss for fold 0: 0.1944545308748881\n",
      "Validation Loss for fold 0: 0.17264816661675772\n",
      "Validation Loss for fold 0: 0.16125629345575967\n",
      "Validation Loss for fold 0: 0.14474942038456598\n",
      "Validation Loss for fold 0: 0.14822782576084137\n",
      "Validation Loss for fold 0: 0.1478592405716578\n",
      "Validation Loss for fold 0: 0.15805101891358694\n",
      "Validation Loss for fold 0: 0.13767280181248984\n",
      "Validation Loss for fold 0: 0.14000499745210013\n",
      "Validation Loss for fold 0: 0.13477263102928796\n",
      "Validation Loss for fold 0: 0.1264224648475647\n",
      "Validation Loss for fold 0: 0.14361712336540222\n",
      "Validation Loss for fold 0: 0.12529591222604117\n",
      "Validation Loss for fold 0: 0.13746154556671777\n",
      "Validation Loss for fold 0: 0.12262752652168274\n",
      "Validation Loss for fold 0: 0.11829286068677902\n",
      "Validation Loss for fold 0: 0.13631487637758255\n",
      "Validation Loss for fold 0: 0.12483530243237813\n",
      "Validation Loss for fold 0: 0.1336784934004148\n",
      "Validation Loss for fold 0: 0.11521729081869125\n",
      "Validation Loss for fold 0: 0.12521695097287497\n",
      "Validation Loss for fold 0: 0.11621090521415074\n",
      "Validation Loss for fold 0: 0.13679955899715424\n",
      "Validation Loss for fold 0: 0.12390839805205663\n",
      "Validation Loss for fold 0: 0.12033671140670776\n",
      "Validation Loss for fold 0: 0.13550910850365958\n",
      "Validation Loss for fold 0: 0.12334686766068141\n",
      "Validation Loss for fold 0: 0.12160423149665196\n",
      "Validation Loss for fold 0: 0.11613240092992783\n",
      "Validation Loss for fold 0: 0.12281709909439087\n",
      "Validation Loss for fold 0: 0.12355015426874161\n",
      "Validation Loss for fold 0: 0.11514164755741756\n",
      "Validation Loss for fold 0: 0.12137605001529057\n",
      "Validation Loss for fold 0: 0.11695678532123566\n",
      "Validation Loss for fold 0: 0.12436210364103317\n",
      "Validation Loss for fold 0: 0.11367955307165782\n",
      "Validation Loss for fold 0: 0.12207067757844925\n",
      "Validation Loss for fold 0: 0.11511346946159999\n",
      "Validation Loss for fold 0: 0.11852125575145085\n",
      "Validation Loss for fold 0: 0.11570535103480022\n",
      "Validation Loss for fold 0: 0.10607712964216869\n",
      "Validation Loss for fold 0: 0.12418957551320393\n",
      "Validation Loss for fold 0: 0.1139209692676862\n",
      "Validation Loss for fold 0: 0.11401030669609706\n",
      "Validation Loss for fold 0: 0.11683924992879231\n",
      "Validation Loss for fold 0: 0.1167520930369695\n",
      "Validation Loss for fold 0: 0.11384438474973042\n",
      "Validation Loss for fold 0: 0.1151769831776619\n",
      "Validation Loss for fold 0: 0.11469818403323491\n",
      "Validation Loss for fold 0: 0.11986385782559712\n",
      "Validation Loss for fold 0: 0.11395724366108577\n",
      "Validation Loss for fold 0: 0.11167048662900925\n",
      "Validation Loss for fold 0: 0.11424955477317174\n",
      "Validation Loss for fold 0: 0.11486713339885075\n",
      "Validation Loss for fold 0: 0.1099039614200592\n",
      "Validation Loss for fold 0: 0.10727946211894353\n",
      "Validation Loss for fold 0: 0.10090443740288417\n",
      "Validation Loss for fold 0: 0.09291603912909825\n",
      "Validation Loss for fold 0: 0.1045592005054156\n",
      "Validation Loss for fold 0: 0.10011279582977295\n",
      "Validation Loss for fold 0: 0.10026916861534119\n",
      "Validation Loss for fold 0: 0.10468530903259914\n",
      "Validation Loss for fold 0: 0.1122497742374738\n",
      "Validation Loss for fold 0: 0.10927276561657588\n",
      "Validation Loss for fold 0: 0.09793536116679509\n",
      "Validation Loss for fold 0: 0.10176390161116917\n",
      "Validation Loss for fold 0: 0.09933508435885112\n",
      "Validation Loss for fold 0: 0.10170080761114757\n",
      "Validation Loss for fold 0: 0.10070553173621495\n",
      "Validation Loss for fold 0: 0.1057645579179128\n",
      "Validation Loss for fold 0: 0.10171080629030864\n",
      "Validation Loss for fold 0: 0.09702293078104655\n",
      "Validation Loss for fold 0: 0.111551138261954\n",
      "Validation Loss for fold 0: 0.10543987900018692\n",
      "Validation Loss for fold 0: 0.09592696030934651\n",
      "Validation Loss for fold 0: 0.0993456890185674\n",
      "Validation Loss for fold 0: 0.1019204705953598\n",
      "Validation Loss for fold 0: 0.09983019530773163\n",
      "Validation Loss for fold 0: 0.10576088478167851\n",
      "Validation Loss for fold 0: 0.09454259028037389\n",
      "Validation Loss for fold 0: 0.10208564748366673\n",
      "Validation Loss for fold 0: 0.09092552711566289\n",
      "Validation Loss for fold 0: 0.0886697384218375\n",
      "Validation Loss for fold 0: 0.09589985758066177\n",
      "Validation Loss for fold 0: 0.09736755738655727\n",
      "Validation Loss for fold 0: 0.09446256856123607\n",
      "Validation Loss for fold 0: 0.09469755738973618\n",
      "Validation Loss for fold 0: 0.0858466736972332\n",
      "Validation Loss for fold 0: 0.10082513093948364\n",
      "Validation Loss for fold 0: 0.10183503478765488\n",
      "Validation Loss for fold 0: 0.10056559989849727\n",
      "Validation Loss for fold 0: 0.09807094434897105\n",
      "Validation Loss for fold 0: 0.09541592001914978\n",
      "Validation Loss for fold 0: 0.0986604963739713\n",
      "Validation Loss for fold 0: 0.09445079664389293\n",
      "Validation Loss for fold 0: 0.0878114973505338\n",
      "Validation Loss for fold 0: 0.08951941132545471\n",
      "Validation Loss for fold 0: 0.08576253056526184\n",
      "Validation Loss for fold 0: 0.08663450181484222\n",
      "Validation Loss for fold 0: 0.08587841192881267\n",
      "Validation Loss for fold 0: 0.08250693107644717\n",
      "Validation Loss for fold 0: 0.08901648968458176\n",
      "Validation Loss for fold 0: 0.08990890781084697\n",
      "Validation Loss for fold 0: 0.08347363770008087\n",
      "Validation Loss for fold 0: 0.08849167575438817\n",
      "Validation Loss for fold 0: 0.08457000056902568\n",
      "Validation Loss for fold 0: 0.08944983532031377\n",
      "Validation Loss for fold 0: 0.0877766211827596\n",
      "Validation Loss for fold 0: 0.08541143437226613\n",
      "Validation Loss for fold 0: 0.08779916167259216\n",
      "Validation Loss for fold 0: 0.09130529065926869\n",
      "Validation Loss for fold 0: 0.0850584755341212\n",
      "Validation Loss for fold 0: 0.08540486047665279\n",
      "Validation Loss for fold 0: 0.08569962779680888\n",
      "Validation Loss for fold 0: 0.088819553454717\n",
      "Validation Loss for fold 0: 0.08834716429313023\n",
      "Validation Loss for fold 0: 0.08238367487986882\n",
      "Validation Loss for fold 0: 0.07930658260981242\n",
      "Validation Loss for fold 0: 0.09134749074776967\n",
      "Validation Loss for fold 0: 0.0821399986743927\n",
      "Validation Loss for fold 0: 0.08395694692929585\n",
      "Validation Loss for fold 0: 0.07596514374017715\n",
      "Validation Loss for fold 0: 0.08473101754983266\n",
      "Validation Loss for fold 0: 0.08317116399606068\n",
      "Validation Loss for fold 0: 0.08305924882491429\n",
      "Validation Loss for fold 0: 0.08911730845769246\n",
      "Validation Loss for fold 0: 0.08590468764305115\n",
      "Validation Loss for fold 0: 0.09065044422944386\n",
      "Validation Loss for fold 0: 0.09061208367347717\n",
      "Validation Loss for fold 0: 0.0899959107240041\n",
      "Validation Loss for fold 0: 0.07563652967413266\n",
      "Validation Loss for fold 0: 0.09549227356910706\n",
      "Validation Loss for fold 0: 0.08166402578353882\n",
      "Validation Loss for fold 0: 0.07989942779143651\n",
      "Validation Loss for fold 0: 0.09037822236617406\n",
      "Validation Loss for fold 0: 0.08405887335538864\n",
      "Validation Loss for fold 0: 0.08726488798856735\n",
      "Validation Loss for fold 0: 0.08500151087840398\n",
      "Validation Loss for fold 0: 0.09012092649936676\n",
      "Validation Loss for fold 0: 0.07664326826731364\n",
      "Validation Loss for fold 0: 0.09198345988988876\n",
      "Validation Loss for fold 0: 0.08610749244689941\n",
      "Validation Loss for fold 0: 0.07800524185101192\n",
      "Validation Loss for fold 0: 0.08072867741187413\n",
      "Validation Loss for fold 0: 0.07903460909922917\n",
      "Validation Loss for fold 0: 0.07737525800863902\n",
      "Validation Loss for fold 0: 0.07863221193353336\n",
      "Validation Loss for fold 0: 0.08599918832381566\n",
      "Validation Loss for fold 0: 0.08276617030302684\n",
      "Validation Loss for fold 0: 0.08139474938313167\n",
      "Validation Loss for fold 0: 0.08211379125714302\n",
      "Validation Loss for fold 0: 0.08376929660638173\n",
      "Validation Loss for fold 0: 0.08531772593657176\n",
      "Validation Loss for fold 0: 0.08242032428582509\n",
      "Validation Loss for fold 0: 0.08052826176087062\n",
      "Validation Loss for fold 0: 0.07975149154663086\n",
      "Validation Loss for fold 0: 0.08448754251003265\n",
      "Validation Loss for fold 0: 0.07609901825586955\n",
      "Validation Loss for fold 0: 0.0724111224214236\n",
      "Validation Loss for fold 0: 0.07605494062105815\n",
      "Validation Loss for fold 0: 0.09178056071201961\n",
      "Validation Loss for fold 0: 0.0762914518515269\n",
      "Validation Loss for fold 0: 0.0805137778321902\n",
      "Validation Loss for fold 0: 0.07335239152113597\n",
      "Validation Loss for fold 0: 0.07937184969584148\n",
      "Validation Loss for fold 0: 0.08477424085140228\n",
      "Validation Loss for fold 0: 0.07796375453472137\n",
      "Validation Loss for fold 0: 0.07946028312047322\n",
      "Validation Loss for fold 0: 0.08154340336720149\n",
      "Validation Loss for fold 0: 0.08592550208171208\n",
      "Validation Loss for fold 0: 0.0750648205478986\n",
      "Validation Loss for fold 0: 0.07212719569603603\n",
      "Validation Loss for fold 0: 0.07769669840733211\n",
      "Validation Loss for fold 0: 0.07365922133127849\n",
      "Validation Loss for fold 0: 0.07366537302732468\n",
      "Validation Loss for fold 0: 0.0744836578766505\n",
      "Validation Loss for fold 0: 0.0754474326968193\n",
      "Validation Loss for fold 0: 0.0803532029191653\n",
      "Validation Loss for fold 0: 0.0694316898783048\n",
      "Validation Loss for fold 0: 0.08073385556538899\n",
      "Validation Loss for fold 0: 0.07224638760089874\n",
      "Validation Loss for fold 0: 0.0729796290397644\n",
      "Validation Loss for fold 0: 0.0778750404715538\n",
      "Validation Loss for fold 0: 0.07082509746154149\n",
      "Validation Loss for fold 0: 0.07425582657257716\n",
      "Validation Loss for fold 0: 0.07222150017817815\n",
      "Validation Loss for fold 0: 0.07581065098444621\n",
      "Validation Loss for fold 0: 0.07255591203769048\n",
      "Validation Loss for fold 0: 0.06974360843499501\n",
      "Validation Loss for fold 0: 0.07157825057705243\n",
      "Validation Loss for fold 0: 0.0729428343474865\n",
      "Validation Loss for fold 0: 0.06637083118160565\n",
      "Validation Loss for fold 0: 0.07679794232050578\n",
      "Validation Loss for fold 0: 0.07208000620206197\n",
      "Validation Loss for fold 0: 0.07084689786036809\n",
      "Validation Loss for fold 0: 0.07334173719088237\n",
      "Validation Loss for fold 0: 0.07179513325293858\n",
      "Validation Loss for fold 0: 0.07630787293116252\n",
      "Validation Loss for fold 0: 0.06978980327645938\n",
      "Validation Loss for fold 0: 0.07057242095470428\n",
      "Validation Loss for fold 0: 0.07084645827611287\n",
      "Validation Loss for fold 0: 0.08157832672198613\n",
      "Validation Loss for fold 0: 0.07103573034207027\n",
      "Validation Loss for fold 0: 0.07561043401559193\n",
      "Validation Loss for fold 0: 0.06606205056111018\n",
      "Validation Loss for fold 0: 0.07286639511585236\n",
      "Validation Loss for fold 0: 0.07086265832185745\n",
      "Validation Loss for fold 0: 0.07192321121692657\n",
      "Validation Loss for fold 0: 0.07268685599168141\n",
      "Validation Loss for fold 0: 0.0692174273232619\n",
      "Validation Loss for fold 0: 0.06695141519109409\n",
      "Validation Loss for fold 0: 0.07006542136271794\n",
      "Validation Loss for fold 0: 0.07192836950222652\n",
      "Validation Loss for fold 0: 0.07898008326689403\n",
      "Validation Loss for fold 0: 0.06621874123811722\n",
      "Validation Loss for fold 0: 0.07723713914553325\n",
      "Validation Loss for fold 0: 0.07334280759096146\n",
      "Validation Loss for fold 0: 0.06906839956839879\n",
      "Validation Loss for fold 0: 0.06994581719239552\n",
      "Validation Loss for fold 0: 0.0653636281689008\n",
      "Validation Loss for fold 0: 0.08031518508990605\n",
      "Validation Loss for fold 0: 0.07243714481592178\n",
      "Validation Loss for fold 0: 0.07163000603516896\n",
      "Validation Loss for fold 0: 0.06137541060646375\n",
      "Validation Loss for fold 0: 0.07895576705535252\n",
      "Validation Loss for fold 0: 0.07101087768872578\n",
      "Validation Loss for fold 0: 0.06538810456792514\n",
      "Validation Loss for fold 0: 0.07179514318704605\n",
      "Validation Loss for fold 0: 0.07083440572023392\n",
      "Validation Loss for fold 0: 0.06800154844919841\n",
      "Validation Loss for fold 0: 0.07198188205560048\n",
      "Validation Loss for fold 0: 0.07244221121072769\n",
      "Validation Loss for fold 0: 0.07181938985983531\n",
      "Validation Loss for fold 0: 0.07483688617746036\n",
      "Validation Loss for fold 0: 0.07545980562766393\n",
      "Validation Loss for fold 0: 0.07203326870997746\n",
      "Validation Loss for fold 0: 0.0679366464416186\n",
      "Validation Loss for fold 0: 0.06848608205715816\n",
      "Validation Loss for fold 0: 0.06781574090321858\n",
      "Validation Loss for fold 0: 0.07267468546827634\n",
      "Validation Loss for fold 0: 0.06612901017069817\n",
      "Validation Loss for fold 0: 0.07119295249382655\n",
      "Validation Loss for fold 0: 0.07072332501411438\n",
      "Validation Loss for fold 0: 0.0695214495062828\n",
      "Validation Loss for fold 0: 0.06919573371609052\n",
      "Validation Loss for fold 0: 0.06455172846714656\n",
      "Validation Loss for fold 0: 0.06699954097469647\n",
      "Validation Loss for fold 0: 0.06320071220397949\n",
      "Validation Loss for fold 0: 0.07036320368448894\n",
      "Validation Loss for fold 0: 0.07267108311255772\n",
      "Validation Loss for fold 0: 0.06846063832441966\n",
      "Validation Loss for fold 0: 0.07110636681318283\n",
      "Validation Loss for fold 0: 0.061120166132847466\n",
      "Validation Loss for fold 0: 0.06957749277353287\n",
      "Validation Loss for fold 0: 0.07211786632736523\n",
      "Validation Loss for fold 0: 0.06782637784878413\n",
      "Validation Loss for fold 0: 0.06652191778024037\n",
      "Validation Loss for fold 0: 0.07108460615078609\n",
      "Validation Loss for fold 0: 0.06766457110643387\n",
      "Validation Loss for fold 0: 0.062475137412548065\n",
      "Validation Loss for fold 0: 0.07026473432779312\n",
      "Validation Loss for fold 0: 0.06604894250631332\n",
      "Validation Loss for fold 0: 0.06617988521854083\n",
      "Validation Loss for fold 0: 0.06648143629233043\n",
      "Validation Loss for fold 0: 0.0717897539337476\n",
      "Validation Loss for fold 0: 0.06724108507235844\n",
      "Validation Loss for fold 0: 0.06922164807717006\n",
      "Validation Loss for fold 0: 0.07364261398712794\n",
      "Validation Loss for fold 0: 0.06917126476764679\n",
      "Validation Loss for fold 0: 0.06835415214300156\n",
      "Validation Loss for fold 0: 0.0706343799829483\n",
      "Validation Loss for fold 0: 0.06343250721693039\n",
      "Validation Loss for fold 0: 0.06713006397088368\n",
      "Validation Loss for fold 0: 0.06728821247816086\n",
      "Validation Loss for fold 0: 0.06752774367729823\n",
      "Validation Loss for fold 0: 0.0669366071621577\n",
      "Validation Loss for fold 0: 0.06861748546361923\n",
      "Validation Loss for fold 0: 0.06872658431529999\n",
      "Validation Loss for fold 0: 0.0670797402660052\n",
      "Validation Loss for fold 0: 0.0651174324254195\n",
      "Validation Loss for fold 0: 0.061537099381287895\n",
      "Validation Loss for fold 0: 0.07139727845788002\n",
      "Validation Loss for fold 0: 0.0732853797574838\n",
      "Validation Loss for fold 0: 0.06729971369107564\n",
      "Validation Loss for fold 0: 0.0700266386071841\n",
      "Validation Loss for fold 0: 0.06864661971728007\n",
      "Validation Loss for fold 0: 0.06715209657947223\n",
      "Validation Loss for fold 0: 0.0693702648083369\n",
      "Validation Loss for fold 0: 0.06743915875752766\n",
      "Validation Loss for fold 0: 0.06719827031095822\n",
      "Validation Loss for fold 0: 0.0688194955388705\n",
      "Validation Loss for fold 0: 0.062499710669120155\n",
      "Validation Loss for fold 0: 0.06338178738951683\n",
      "Validation Loss for fold 0: 0.07200687130292256\n",
      "Validation Loss for fold 0: 0.07112264633178711\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Validation Loss for fold 1: 0.4199387828509013\n",
      "Validation Loss for fold 1: 0.34606881936391193\n",
      "Validation Loss for fold 1: 0.2627767523129781\n",
      "Validation Loss for fold 1: 0.23400906225045523\n",
      "Validation Loss for fold 1: 0.19864354034264883\n",
      "Validation Loss for fold 1: 0.16619662940502167\n",
      "Validation Loss for fold 1: 0.17690091331799826\n",
      "Validation Loss for fold 1: 0.17104457318782806\n",
      "Validation Loss for fold 1: 0.140920989215374\n",
      "Validation Loss for fold 1: 0.13466952244440714\n",
      "Validation Loss for fold 1: 0.146768386165301\n",
      "Validation Loss for fold 1: 0.1307516743739446\n",
      "Validation Loss for fold 1: 0.12287700921297073\n",
      "Validation Loss for fold 1: 0.13179380943377814\n",
      "Validation Loss for fold 1: 0.12426067640384038\n",
      "Validation Loss for fold 1: 0.1215374544262886\n",
      "Validation Loss for fold 1: 0.13369036465883255\n",
      "Validation Loss for fold 1: 0.13679408778746924\n",
      "Validation Loss for fold 1: 0.12266933917999268\n",
      "Validation Loss for fold 1: 0.11434887101252873\n",
      "Validation Loss for fold 1: 0.1144413302342097\n",
      "Validation Loss for fold 1: 0.10602445403734843\n",
      "Validation Loss for fold 1: 0.11514097203811009\n",
      "Validation Loss for fold 1: 0.11616987238327663\n",
      "Validation Loss for fold 1: 0.10627992699543636\n",
      "Validation Loss for fold 1: 0.1155143529176712\n",
      "Validation Loss for fold 1: 0.11022833983103435\n",
      "Validation Loss for fold 1: 0.11082485069831212\n",
      "Validation Loss for fold 1: 0.10748473554849625\n",
      "Validation Loss for fold 1: 0.10407108068466187\n",
      "Validation Loss for fold 1: 0.10465365648269653\n",
      "Validation Loss for fold 1: 0.09220132107535998\n",
      "Validation Loss for fold 1: 0.09871172159910202\n",
      "Validation Loss for fold 1: 0.10379929343859355\n",
      "Validation Loss for fold 1: 0.10356271515289943\n",
      "Validation Loss for fold 1: 0.1037400687734286\n",
      "Validation Loss for fold 1: 0.10363414883613586\n",
      "Validation Loss for fold 1: 0.09194073577721913\n",
      "Validation Loss for fold 1: 0.09129771093527476\n",
      "Validation Loss for fold 1: 0.10257006933291753\n",
      "Validation Loss for fold 1: 0.08704685047268867\n",
      "Validation Loss for fold 1: 0.10220320771137874\n",
      "Validation Loss for fold 1: 0.09402962774038315\n",
      "Validation Loss for fold 1: 0.09585793813069661\n",
      "Validation Loss for fold 1: 0.100321464240551\n",
      "Validation Loss for fold 1: 0.09392723441123962\n",
      "Validation Loss for fold 1: 0.08669635901848476\n",
      "Validation Loss for fold 1: 0.08726736903190613\n",
      "Validation Loss for fold 1: 0.0933378537495931\n",
      "Validation Loss for fold 1: 0.09464299927155177\n",
      "Validation Loss for fold 1: 0.10782020042339961\n",
      "Validation Loss for fold 1: 0.0939759910106659\n",
      "Validation Loss for fold 1: 0.08825680365165074\n",
      "Validation Loss for fold 1: 0.087465633948644\n",
      "Validation Loss for fold 1: 0.07914677386482556\n",
      "Validation Loss for fold 1: 0.09018983443578084\n",
      "Validation Loss for fold 1: 0.09109732260306676\n",
      "Validation Loss for fold 1: 0.08561715235312779\n",
      "Validation Loss for fold 1: 0.08307822172840436\n",
      "Validation Loss for fold 1: 0.09556259711583455\n",
      "Validation Loss for fold 1: 0.08679018914699554\n",
      "Validation Loss for fold 1: 0.08862237880627315\n",
      "Validation Loss for fold 1: 0.08120017250378926\n",
      "Validation Loss for fold 1: 0.08524983376264572\n",
      "Validation Loss for fold 1: 0.08831387509902318\n",
      "Validation Loss for fold 1: 0.08405079940954845\n",
      "Validation Loss for fold 1: 0.0840399240454038\n",
      "Validation Loss for fold 1: 0.08146009842554729\n",
      "Validation Loss for fold 1: 0.08268462121486664\n",
      "Validation Loss for fold 1: 0.08943675955136617\n",
      "Validation Loss for fold 1: 0.07607035463054974\n",
      "Validation Loss for fold 1: 0.0808681771159172\n",
      "Validation Loss for fold 1: 0.08141467720270157\n",
      "Validation Loss for fold 1: 0.08019665877024333\n",
      "Validation Loss for fold 1: 0.08010454724232356\n",
      "Validation Loss for fold 1: 0.07850432644287746\n",
      "Validation Loss for fold 1: 0.07505971317489941\n",
      "Validation Loss for fold 1: 0.07755834857622783\n",
      "Validation Loss for fold 1: 0.0814347763856252\n",
      "Validation Loss for fold 1: 0.08177988231182098\n",
      "Validation Loss for fold 1: 0.08924800405899684\n",
      "Validation Loss for fold 1: 0.09289279580116272\n",
      "Validation Loss for fold 1: 0.08243076999982198\n",
      "Validation Loss for fold 1: 0.07847420871257782\n",
      "Validation Loss for fold 1: 0.07641437649726868\n",
      "Validation Loss for fold 1: 0.08976352959871292\n",
      "Validation Loss for fold 1: 0.0795021007458369\n",
      "Validation Loss for fold 1: 0.08028058707714081\n",
      "Validation Loss for fold 1: 0.08103569100300471\n",
      "Validation Loss for fold 1: 0.08153096586465836\n",
      "Validation Loss for fold 1: 0.07720995942751567\n",
      "Validation Loss for fold 1: 0.08642389376958211\n",
      "Validation Loss for fold 1: 0.0796455591917038\n",
      "Validation Loss for fold 1: 0.07361731802423795\n",
      "Validation Loss for fold 1: 0.07939050594965617\n",
      "Validation Loss for fold 1: 0.08011264353990555\n",
      "Validation Loss for fold 1: 0.07732872416575749\n",
      "Validation Loss for fold 1: 0.0806058297554652\n",
      "Validation Loss for fold 1: 0.07295998806754748\n",
      "Validation Loss for fold 1: 0.07334049294392268\n",
      "Validation Loss for fold 1: 0.06936528657873471\n",
      "Validation Loss for fold 1: 0.07987025131781895\n",
      "Validation Loss for fold 1: 0.08205583815773328\n",
      "Validation Loss for fold 1: 0.07715944449106853\n",
      "Validation Loss for fold 1: 0.08213549355665843\n",
      "Validation Loss for fold 1: 0.07267755021651585\n",
      "Validation Loss for fold 1: 0.07531669487555821\n",
      "Validation Loss for fold 1: 0.07443422079086304\n",
      "Validation Loss for fold 1: 0.07881678392489751\n",
      "Validation Loss for fold 1: 0.08012666056553523\n",
      "Validation Loss for fold 1: 0.07139957572023074\n",
      "Validation Loss for fold 1: 0.08501682430505753\n",
      "Validation Loss for fold 1: 0.06895729651053746\n",
      "Validation Loss for fold 1: 0.07937491188446681\n",
      "Validation Loss for fold 1: 0.0738266905148824\n",
      "Validation Loss for fold 1: 0.07741862535476685\n",
      "Validation Loss for fold 1: 0.07949321220318477\n",
      "Validation Loss for fold 1: 0.07941153893868129\n",
      "Validation Loss for fold 1: 0.07749862472216289\n",
      "Validation Loss for fold 1: 0.07299970338741939\n",
      "Validation Loss for fold 1: 0.07220246891180675\n",
      "Validation Loss for fold 1: 0.06936076159278552\n",
      "Validation Loss for fold 1: 0.07913132756948471\n",
      "Validation Loss for fold 1: 0.07174895952145259\n",
      "Validation Loss for fold 1: 0.06855657572547595\n",
      "Validation Loss for fold 1: 0.07928625370065372\n",
      "Validation Loss for fold 1: 0.07428533087174098\n",
      "Validation Loss for fold 1: 0.06898351510365804\n",
      "Validation Loss for fold 1: 0.07953334599733353\n",
      "Validation Loss for fold 1: 0.07406502217054367\n",
      "Validation Loss for fold 1: 0.07040078192949295\n",
      "Validation Loss for fold 1: 0.07790802046656609\n",
      "Validation Loss for fold 1: 0.06926431879401207\n",
      "Validation Loss for fold 1: 0.07214421778917313\n",
      "Validation Loss for fold 1: 0.06881830841302872\n",
      "Validation Loss for fold 1: 0.06703081106146176\n",
      "Validation Loss for fold 1: 0.06750081231196721\n",
      "Validation Loss for fold 1: 0.07314011951287587\n",
      "Validation Loss for fold 1: 0.07383514940738678\n",
      "Validation Loss for fold 1: 0.07157810280720393\n",
      "Validation Loss for fold 1: 0.07290831580758095\n",
      "Validation Loss for fold 1: 0.06783314297596614\n",
      "Validation Loss for fold 1: 0.07332206269105275\n",
      "Validation Loss for fold 1: 0.0710182140270869\n",
      "Validation Loss for fold 1: 0.06511885300278664\n",
      "Validation Loss for fold 1: 0.0669691947599252\n",
      "Validation Loss for fold 1: 0.06822214399774869\n",
      "Validation Loss for fold 1: 0.07156125704447429\n",
      "Validation Loss for fold 1: 0.06532083824276924\n",
      "Validation Loss for fold 1: 0.06674136221408844\n",
      "Validation Loss for fold 1: 0.06752717743317287\n",
      "Validation Loss for fold 1: 0.07003304362297058\n",
      "Validation Loss for fold 1: 0.07304008305072784\n",
      "Validation Loss for fold 1: 0.07486972957849503\n",
      "Validation Loss for fold 1: 0.07019403825203578\n",
      "Validation Loss for fold 1: 0.06967559953530629\n",
      "Validation Loss for fold 1: 0.06739206984639168\n",
      "Validation Loss for fold 1: 0.0671590802570184\n",
      "Validation Loss for fold 1: 0.06825651476780574\n",
      "Validation Loss for fold 1: 0.06811285267273585\n",
      "Validation Loss for fold 1: 0.06791522353887558\n",
      "Validation Loss for fold 1: 0.07033497840166092\n",
      "Validation Loss for fold 1: 0.06610436365008354\n",
      "Validation Loss for fold 1: 0.06443185855944951\n",
      "Validation Loss for fold 1: 0.07991705710689227\n",
      "Validation Loss for fold 1: 0.06681962062915166\n",
      "Validation Loss for fold 1: 0.06636421382427216\n",
      "Validation Loss for fold 1: 0.06949426233768463\n",
      "Validation Loss for fold 1: 0.06777279203136762\n",
      "Validation Loss for fold 1: 0.06928201019763947\n",
      "Validation Loss for fold 1: 0.0647036408384641\n",
      "Validation Loss for fold 1: 0.06958950931827228\n",
      "Validation Loss for fold 1: 0.0654296949505806\n",
      "Validation Loss for fold 1: 0.06652164955933888\n",
      "Validation Loss for fold 1: 0.06853384524583817\n",
      "Validation Loss for fold 1: 0.0655654010673364\n",
      "Validation Loss for fold 1: 0.06639403104782104\n",
      "Validation Loss for fold 1: 0.060361918061971664\n",
      "Validation Loss for fold 1: 0.06632155925035477\n",
      "Validation Loss for fold 1: 0.06496154020229976\n",
      "Validation Loss for fold 1: 0.07571809738874435\n",
      "Validation Loss for fold 1: 0.0718489612142245\n",
      "Validation Loss for fold 1: 0.0640133408208688\n",
      "Validation Loss for fold 1: 0.07085036486387253\n",
      "Validation Loss for fold 1: 0.06590529531240463\n",
      "Validation Loss for fold 1: 0.07059894998868306\n",
      "Validation Loss for fold 1: 0.06950591504573822\n",
      "Validation Loss for fold 1: 0.06848808874686559\n",
      "Validation Loss for fold 1: 0.06611200670401256\n",
      "Validation Loss for fold 1: 0.06948501120011012\n",
      "Validation Loss for fold 1: 0.06807761391003926\n",
      "Validation Loss for fold 1: 0.06343691671888034\n",
      "Validation Loss for fold 1: 0.062119041879971824\n",
      "Validation Loss for fold 1: 0.06957358742753665\n",
      "Validation Loss for fold 1: 0.0654885582625866\n",
      "Validation Loss for fold 1: 0.0653293381134669\n",
      "Validation Loss for fold 1: 0.06385339672366779\n",
      "Validation Loss for fold 1: 0.06604183961947759\n",
      "Validation Loss for fold 1: 0.06409350285927455\n",
      "Validation Loss for fold 1: 0.07123869533340137\n",
      "Validation Loss for fold 1: 0.0680144876241684\n",
      "Validation Loss for fold 1: 0.06823816647132237\n",
      "Validation Loss for fold 1: 0.06900493055582047\n",
      "Validation Loss for fold 1: 0.06316892181833585\n",
      "Validation Loss for fold 1: 0.06671372552712758\n",
      "Validation Loss for fold 1: 0.06609053164720535\n",
      "Validation Loss for fold 1: 0.06484996154904366\n",
      "Validation Loss for fold 1: 0.058933086693286896\n",
      "Validation Loss for fold 1: 0.06817117085059483\n",
      "Validation Loss for fold 1: 0.0635283887386322\n",
      "Validation Loss for fold 1: 0.06687772025664647\n",
      "Validation Loss for fold 1: 0.06621501967310905\n",
      "Validation Loss for fold 1: 0.06705693403879802\n",
      "Validation Loss for fold 1: 0.06406768411397934\n",
      "Validation Loss for fold 1: 0.06086246048410734\n",
      "Validation Loss for fold 1: 0.07061004514495532\n",
      "Validation Loss for fold 1: 0.06162444377938906\n",
      "Validation Loss for fold 1: 0.06466152022282283\n",
      "Validation Loss for fold 1: 0.0699664478500684\n",
      "Validation Loss for fold 1: 0.06506233910719554\n",
      "Validation Loss for fold 1: 0.0611743318537871\n",
      "Validation Loss for fold 1: 0.06664015476902325\n",
      "Validation Loss for fold 1: 0.06234592944383621\n",
      "Validation Loss for fold 1: 0.06853492309649785\n",
      "Validation Loss for fold 1: 0.0658847838640213\n",
      "Validation Loss for fold 1: 0.06659740954637527\n",
      "Validation Loss for fold 1: 0.06408750514189403\n",
      "Validation Loss for fold 1: 0.058059774339199066\n",
      "Validation Loss for fold 1: 0.05939816186825434\n",
      "Validation Loss for fold 1: 0.06309985121091206\n",
      "Validation Loss for fold 1: 0.055959041540821396\n",
      "Validation Loss for fold 1: 0.06329851721723874\n",
      "Validation Loss for fold 1: 0.06708420813083649\n",
      "Validation Loss for fold 1: 0.05973123634854952\n",
      "Validation Loss for fold 1: 0.05862983067830404\n",
      "Validation Loss for fold 1: 0.06959543749690056\n",
      "Validation Loss for fold 1: 0.06083131209015846\n",
      "Validation Loss for fold 1: 0.06482752660910289\n",
      "Validation Loss for fold 1: 0.06022234261035919\n",
      "Validation Loss for fold 1: 0.06664558624227841\n",
      "Validation Loss for fold 1: 0.06567147374153137\n",
      "Validation Loss for fold 1: 0.062019120901823044\n",
      "Validation Loss for fold 1: 0.06313091516494751\n",
      "Validation Loss for fold 1: 0.06724924966692924\n",
      "Validation Loss for fold 1: 0.0650281732281049\n",
      "Validation Loss for fold 1: 0.07054776822527249\n",
      "Validation Loss for fold 1: 0.05982455859581629\n",
      "Validation Loss for fold 1: 0.05733249833186468\n",
      "Validation Loss for fold 1: 0.0608221727112929\n",
      "Validation Loss for fold 1: 0.06269244725505511\n",
      "Validation Loss for fold 1: 0.06586989760398865\n",
      "Validation Loss for fold 1: 0.06274642050266266\n",
      "Validation Loss for fold 1: 0.06133483350276947\n",
      "Validation Loss for fold 1: 0.06027438988288244\n",
      "Validation Loss for fold 1: 0.06874136999249458\n",
      "Validation Loss for fold 1: 0.06532905250787735\n",
      "Validation Loss for fold 1: 0.06732832143704097\n",
      "Validation Loss for fold 1: 0.06580191105604172\n",
      "Validation Loss for fold 1: 0.06342648342251778\n",
      "Validation Loss for fold 1: 0.06287942826747894\n",
      "Validation Loss for fold 1: 0.06253281856576602\n",
      "Validation Loss for fold 1: 0.06171488637725512\n",
      "Validation Loss for fold 1: 0.060965518156687416\n",
      "Validation Loss for fold 1: 0.066181813677152\n",
      "Validation Loss for fold 1: 0.0637750228246053\n",
      "Validation Loss for fold 1: 0.06575117136041324\n",
      "Validation Loss for fold 1: 0.06679145743449529\n",
      "Validation Loss for fold 1: 0.06176066274444262\n",
      "Validation Loss for fold 1: 0.06195887674887975\n",
      "Validation Loss for fold 1: 0.062395708014567695\n",
      "Validation Loss for fold 1: 0.060941098878781\n",
      "Validation Loss for fold 1: 0.07021842276056607\n",
      "Validation Loss for fold 1: 0.06605817750096321\n",
      "Validation Loss for fold 1: 0.06047031904260317\n",
      "Validation Loss for fold 1: 0.06397459656000137\n",
      "Validation Loss for fold 1: 0.062291872998078666\n",
      "Validation Loss for fold 1: 0.06303424760699272\n",
      "Validation Loss for fold 1: 0.0633409284055233\n",
      "Validation Loss for fold 1: 0.05789548779527346\n",
      "Validation Loss for fold 1: 0.06545739496747653\n",
      "Validation Loss for fold 1: 0.06380613148212433\n",
      "Validation Loss for fold 1: 0.06672500198086102\n",
      "Validation Loss for fold 1: 0.06611210977037747\n",
      "Validation Loss for fold 1: 0.06265457098682721\n",
      "Validation Loss for fold 1: 0.06264077126979828\n",
      "Validation Loss for fold 1: 0.05657635008295377\n",
      "Validation Loss for fold 1: 0.062621109187603\n",
      "Validation Loss for fold 1: 0.05959751829504967\n",
      "Validation Loss for fold 1: 0.05803592875599861\n",
      "Validation Loss for fold 1: 0.061538223177194595\n",
      "Validation Loss for fold 1: 0.059586518754561744\n",
      "Validation Loss for fold 1: 0.06321526442964871\n",
      "Validation Loss for fold 1: 0.06123376886049906\n",
      "Validation Loss for fold 1: 0.06705462311704953\n",
      "Validation Loss for fold 1: 0.06144344309965769\n",
      "Validation Loss for fold 1: 0.05717336138089498\n",
      "Validation Loss for fold 1: 0.05690940096974373\n",
      "Validation Loss for fold 1: 0.06300029779473941\n",
      "Validation Loss for fold 1: 0.05651651198665301\n",
      "Validation Loss for fold 1: 0.06705586363871892\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Validation Loss for fold 2: 0.5689504345258077\n",
      "Validation Loss for fold 2: 0.4455244342486064\n",
      "Validation Loss for fold 2: 0.36801260709762573\n",
      "Validation Loss for fold 2: 0.315748006105423\n",
      "Validation Loss for fold 2: 0.29578981300195056\n",
      "Validation Loss for fold 2: 0.23287725945313772\n",
      "Validation Loss for fold 2: 0.21649488310019174\n",
      "Validation Loss for fold 2: 0.21235570311546326\n",
      "Validation Loss for fold 2: 0.19851072132587433\n",
      "Validation Loss for fold 2: 0.19283509254455566\n",
      "Validation Loss for fold 2: 0.16621689001719156\n",
      "Validation Loss for fold 2: 0.17905517419179282\n",
      "Validation Loss for fold 2: 0.17141405244668326\n",
      "Validation Loss for fold 2: 0.15142723421255747\n",
      "Validation Loss for fold 2: 0.16510379811127981\n",
      "Validation Loss for fold 2: 0.1563508758942286\n",
      "Validation Loss for fold 2: 0.14442578703165054\n",
      "Validation Loss for fold 2: 0.14424699544906616\n",
      "Validation Loss for fold 2: 0.15074438601732254\n",
      "Validation Loss for fold 2: 0.13613453010718027\n",
      "Validation Loss for fold 2: 0.12724175055821738\n",
      "Validation Loss for fold 2: 0.12553333242734274\n",
      "Validation Loss for fold 2: 0.11889018615086873\n",
      "Validation Loss for fold 2: 0.11544850965340932\n",
      "Validation Loss for fold 2: 0.12017130355040233\n",
      "Validation Loss for fold 2: 0.11512431750694911\n",
      "Validation Loss for fold 2: 0.11138984312613805\n",
      "Validation Loss for fold 2: 0.1069543461004893\n",
      "Validation Loss for fold 2: 0.11366425702969234\n",
      "Validation Loss for fold 2: 0.11347820361455281\n",
      "Validation Loss for fold 2: 0.11050972590843837\n",
      "Validation Loss for fold 2: 0.0998203232884407\n",
      "Validation Loss for fold 2: 0.11374655862649281\n",
      "Validation Loss for fold 2: 0.1026241530974706\n",
      "Validation Loss for fold 2: 0.105962373316288\n",
      "Validation Loss for fold 2: 0.09797966480255127\n",
      "Validation Loss for fold 2: 0.09621688971916835\n",
      "Validation Loss for fold 2: 0.09376218914985657\n",
      "Validation Loss for fold 2: 0.10859533896048863\n",
      "Validation Loss for fold 2: 0.10373685260613759\n",
      "Validation Loss for fold 2: 0.09142772356669109\n",
      "Validation Loss for fold 2: 0.1041205922762553\n",
      "Validation Loss for fold 2: 0.09603945165872574\n",
      "Validation Loss for fold 2: 0.09893149137496948\n",
      "Validation Loss for fold 2: 0.09725118180116017\n",
      "Validation Loss for fold 2: 0.08682061980168025\n",
      "Validation Loss for fold 2: 0.09969612459341685\n",
      "Validation Loss for fold 2: 0.09491707384586334\n",
      "Validation Loss for fold 2: 0.09195975462595622\n",
      "Validation Loss for fold 2: 0.08368449161450069\n",
      "Validation Loss for fold 2: 0.08393546690543492\n",
      "Validation Loss for fold 2: 0.09781496971845627\n",
      "Validation Loss for fold 2: 0.08994787931442261\n",
      "Validation Loss for fold 2: 0.09000219156344731\n",
      "Validation Loss for fold 2: 0.08568763484557469\n",
      "Validation Loss for fold 2: 0.08414312203725179\n",
      "Validation Loss for fold 2: 0.0851500357190768\n",
      "Validation Loss for fold 2: 0.0889211967587471\n",
      "Validation Loss for fold 2: 0.08825864146153133\n",
      "Validation Loss for fold 2: 0.08956576387087505\n",
      "Validation Loss for fold 2: 0.08333780119816463\n",
      "Validation Loss for fold 2: 0.08182225873072942\n",
      "Validation Loss for fold 2: 0.07680404807130496\n",
      "Validation Loss for fold 2: 0.07953042536973953\n",
      "Validation Loss for fold 2: 0.07852304975191753\n",
      "Validation Loss for fold 2: 0.07644543300072353\n",
      "Validation Loss for fold 2: 0.07412425676981609\n",
      "Validation Loss for fold 2: 0.08359876275062561\n",
      "Validation Loss for fold 2: 0.07497839505473773\n",
      "Validation Loss for fold 2: 0.072672122468551\n",
      "Validation Loss for fold 2: 0.08776447301109631\n",
      "Validation Loss for fold 2: 0.08687366048494975\n",
      "Validation Loss for fold 2: 0.08135139445463817\n",
      "Validation Loss for fold 2: 0.07726369301478068\n",
      "Validation Loss for fold 2: 0.0705078939596812\n",
      "Validation Loss for fold 2: 0.07874078055222829\n",
      "Validation Loss for fold 2: 0.0781904806693395\n",
      "Validation Loss for fold 2: 0.07856598248084386\n",
      "Validation Loss for fold 2: 0.0770585189263026\n",
      "Validation Loss for fold 2: 0.07034531980752945\n",
      "Validation Loss for fold 2: 0.0713630144794782\n",
      "Validation Loss for fold 2: 0.06686841572324435\n",
      "Validation Loss for fold 2: 0.07464409867922465\n",
      "Validation Loss for fold 2: 0.072809303800265\n",
      "Validation Loss for fold 2: 0.07368689278761546\n",
      "Validation Loss for fold 2: 0.07168788959582646\n",
      "Validation Loss for fold 2: 0.07237077752749126\n",
      "Validation Loss for fold 2: 0.07010730604330699\n",
      "Validation Loss for fold 2: 0.07086271544297536\n",
      "Validation Loss for fold 2: 0.0751405010620753\n",
      "Validation Loss for fold 2: 0.07581984500090282\n",
      "Validation Loss for fold 2: 0.07252925386031468\n",
      "Validation Loss for fold 2: 0.07617205381393433\n",
      "Validation Loss for fold 2: 0.07255961125095685\n",
      "Validation Loss for fold 2: 0.07031169533729553\n",
      "Validation Loss for fold 2: 0.0685679738720258\n",
      "Validation Loss for fold 2: 0.07133336116870244\n",
      "Validation Loss for fold 2: 0.06598087896903355\n",
      "Validation Loss for fold 2: 0.07131440316637357\n",
      "Validation Loss for fold 2: 0.06236053009827932\n",
      "Validation Loss for fold 2: 0.06767766550183296\n",
      "Validation Loss for fold 2: 0.07380396624406178\n",
      "Validation Loss for fold 2: 0.06562674666444461\n",
      "Validation Loss for fold 2: 0.07121005654335022\n",
      "Validation Loss for fold 2: 0.06943802783886592\n",
      "Validation Loss for fold 2: 0.06822936981916428\n",
      "Validation Loss for fold 2: 0.06344690918922424\n",
      "Validation Loss for fold 2: 0.06567260126272838\n",
      "Validation Loss for fold 2: 0.06888639678557713\n",
      "Validation Loss for fold 2: 0.0642566221455733\n",
      "Validation Loss for fold 2: 0.06608386958638827\n",
      "Validation Loss for fold 2: 0.06630115831891696\n",
      "Validation Loss for fold 2: 0.06610677763819695\n",
      "Validation Loss for fold 2: 0.06514279296000798\n",
      "Validation Loss for fold 2: 0.06440877541899681\n",
      "Validation Loss for fold 2: 0.06491585075855255\n",
      "Validation Loss for fold 2: 0.06645417958498001\n",
      "Validation Loss for fold 2: 0.060008556892474495\n",
      "Validation Loss for fold 2: 0.061481866985559464\n",
      "Validation Loss for fold 2: 0.06717059388756752\n",
      "Validation Loss for fold 2: 0.06546903898318608\n",
      "Validation Loss for fold 2: 0.06328860049446423\n",
      "Validation Loss for fold 2: 0.06956074511011441\n",
      "Validation Loss for fold 2: 0.06317922472953796\n",
      "Validation Loss for fold 2: 0.06194851671655973\n",
      "Validation Loss for fold 2: 0.06642009317874908\n",
      "Validation Loss for fold 2: 0.06250741332769394\n",
      "Validation Loss for fold 2: 0.06519765531023343\n",
      "Validation Loss for fold 2: 0.06168657044569651\n",
      "Validation Loss for fold 2: 0.0662779410680135\n",
      "Validation Loss for fold 2: 0.0635744221508503\n",
      "Validation Loss for fold 2: 0.06723487252990405\n",
      "Validation Loss for fold 2: 0.0629311998685201\n",
      "Validation Loss for fold 2: 0.05862540006637573\n",
      "Validation Loss for fold 2: 0.06422360241413116\n",
      "Validation Loss for fold 2: 0.06296506772438686\n",
      "Validation Loss for fold 2: 0.06763184443116188\n",
      "Validation Loss for fold 2: 0.060494220505158104\n",
      "Validation Loss for fold 2: 0.06144847969214121\n",
      "Validation Loss for fold 2: 0.058578805377086006\n",
      "Validation Loss for fold 2: 0.06311465923984845\n",
      "Validation Loss for fold 2: 0.06150548284252485\n",
      "Validation Loss for fold 2: 0.05716892580191294\n",
      "Validation Loss for fold 2: 0.060761393358310066\n",
      "Validation Loss for fold 2: 0.057618516186873116\n",
      "Validation Loss for fold 2: 0.060669511556625366\n",
      "Validation Loss for fold 2: 0.061159662902355194\n",
      "Validation Loss for fold 2: 0.06368891273935635\n",
      "Validation Loss for fold 2: 0.06729552770654361\n",
      "Validation Loss for fold 2: 0.06057141969601313\n",
      "Validation Loss for fold 2: 0.05735529214143753\n",
      "Validation Loss for fold 2: 0.05655544251203537\n",
      "Validation Loss for fold 2: 0.0595130038758119\n",
      "Validation Loss for fold 2: 0.05507105961441994\n",
      "Validation Loss for fold 2: 0.06216135621070862\n",
      "Validation Loss for fold 2: 0.062415268272161484\n",
      "Validation Loss for fold 2: 0.056107385704914726\n",
      "Validation Loss for fold 2: 0.0608972522119681\n",
      "Validation Loss for fold 2: 0.056576951096455254\n",
      "Validation Loss for fold 2: 0.0642174060146014\n",
      "Validation Loss for fold 2: 0.06473425403237343\n",
      "Validation Loss for fold 2: 0.05128064565360546\n",
      "Validation Loss for fold 2: 0.057963756223519645\n",
      "Validation Loss for fold 2: 0.06421233216921489\n",
      "Validation Loss for fold 2: 0.054502468556165695\n",
      "Validation Loss for fold 2: 0.06160650278131167\n",
      "Validation Loss for fold 2: 0.06353848055005074\n",
      "Validation Loss for fold 2: 0.05599416668216387\n",
      "Validation Loss for fold 2: 0.06770450621843338\n",
      "Validation Loss for fold 2: 0.054889711240927376\n",
      "Validation Loss for fold 2: 0.06080244109034538\n",
      "Validation Loss for fold 2: 0.05528317019343376\n",
      "Validation Loss for fold 2: 0.05560102934638659\n",
      "Validation Loss for fold 2: 0.05510582774877548\n",
      "Validation Loss for fold 2: 0.059585911532243095\n",
      "Validation Loss for fold 2: 0.05847534413139025\n",
      "Validation Loss for fold 2: 0.056371044367551804\n",
      "Validation Loss for fold 2: 0.05590995028614998\n",
      "Validation Loss for fold 2: 0.0608278289437294\n",
      "Validation Loss for fold 2: 0.06194842606782913\n",
      "Validation Loss for fold 2: 0.05657932783166567\n",
      "Validation Loss for fold 2: 0.05947510153055191\n",
      "Validation Loss for fold 2: 0.05399351567029953\n",
      "Validation Loss for fold 2: 0.05423619349797567\n",
      "Validation Loss for fold 2: 0.06118021408716837\n",
      "Validation Loss for fold 2: 0.0586647130548954\n",
      "Validation Loss for fold 2: 0.06430812180042267\n",
      "Validation Loss for fold 2: 0.06261014565825462\n",
      "Validation Loss for fold 2: 0.053401860098044075\n",
      "Validation Loss for fold 2: 0.061016915986935295\n",
      "Validation Loss for fold 2: 0.06017044559121132\n",
      "Validation Loss for fold 2: 0.057380144794782005\n",
      "Validation Loss for fold 2: 0.06001041208704313\n",
      "Validation Loss for fold 2: 0.05599300811688105\n",
      "Validation Loss for fold 2: 0.05835842962066332\n",
      "Validation Loss for fold 2: 0.05684426799416542\n",
      "Validation Loss for fold 2: 0.05564358209570249\n",
      "Validation Loss for fold 2: 0.052622453620036445\n",
      "Validation Loss for fold 2: 0.05710213755567869\n",
      "Validation Loss for fold 2: 0.05677273000280062\n",
      "Validation Loss for fold 2: 0.05477824186285337\n",
      "Validation Loss for fold 2: 0.05512462308009466\n",
      "Validation Loss for fold 2: 0.05055409545699755\n",
      "Validation Loss for fold 2: 0.05386381347974142\n",
      "Validation Loss for fold 2: 0.05684290702144305\n",
      "Validation Loss for fold 2: 0.05774850274125735\n",
      "Validation Loss for fold 2: 0.052639301866292953\n",
      "Validation Loss for fold 2: 0.05234555651744207\n",
      "Validation Loss for fold 2: 0.06216088185707728\n",
      "Validation Loss for fold 2: 0.049161881829301514\n",
      "Validation Loss for fold 2: 0.04940020106732845\n",
      "Validation Loss for fold 2: 0.055629465728998184\n",
      "Validation Loss for fold 2: 0.05733267590403557\n",
      "Validation Loss for fold 2: 0.058281811575094856\n",
      "Validation Loss for fold 2: 0.05942788968483607\n",
      "Validation Loss for fold 2: 0.054636672139167786\n",
      "Validation Loss for fold 2: 0.054681157072385154\n",
      "Validation Loss for fold 2: 0.06116762012243271\n",
      "Validation Loss for fold 2: 0.052862522502740227\n",
      "Validation Loss for fold 2: 0.05050284663836161\n",
      "Validation Loss for fold 2: 0.05688941230376562\n",
      "Validation Loss for fold 2: 0.05672798429926237\n",
      "Validation Loss for fold 2: 0.05052035798629125\n",
      "Validation Loss for fold 2: 0.05501272901892662\n",
      "Validation Loss for fold 2: 0.05591415365537008\n",
      "Validation Loss for fold 2: 0.05343685671687126\n",
      "Validation Loss for fold 2: 0.0556604266166687\n",
      "Validation Loss for fold 2: 0.050916494180758796\n",
      "Validation Loss for fold 2: 0.05933809156219164\n",
      "Validation Loss for fold 2: 0.0569894810517629\n",
      "Validation Loss for fold 2: 0.05453551312287649\n",
      "Validation Loss for fold 2: 0.05237319444616636\n",
      "Validation Loss for fold 2: 0.053114532182614006\n",
      "Validation Loss for fold 2: 0.05588210622469584\n",
      "Validation Loss for fold 2: 0.057652801275253296\n",
      "Validation Loss for fold 2: 0.054818337162335716\n",
      "Validation Loss for fold 2: 0.055570228646198906\n",
      "Validation Loss for fold 2: 0.05836815635363261\n",
      "Validation Loss for fold 2: 0.05826819067200025\n",
      "Validation Loss for fold 2: 0.05416517953077952\n",
      "Validation Loss for fold 2: 0.05688132345676422\n",
      "Validation Loss for fold 2: 0.048574221630891166\n",
      "Validation Loss for fold 2: 0.05665376037359238\n",
      "Validation Loss for fold 2: 0.05086031804482142\n",
      "Validation Loss for fold 2: 0.05744947244723638\n",
      "Validation Loss for fold 2: 0.058108311146497726\n",
      "Validation Loss for fold 2: 0.0533152644832929\n",
      "Validation Loss for fold 2: 0.054912650336821876\n",
      "Validation Loss for fold 2: 0.05649679899215698\n",
      "Validation Loss for fold 2: 0.057046595960855484\n",
      "Validation Loss for fold 2: 0.053638619681199394\n",
      "Validation Loss for fold 2: 0.06117560466130575\n",
      "Validation Loss for fold 2: 0.05185858284433683\n",
      "Validation Loss for fold 2: 0.05207798629999161\n",
      "Validation Loss for fold 2: 0.05918909112612406\n",
      "Validation Loss for fold 2: 0.054691734413305916\n",
      "Validation Loss for fold 2: 0.05288213739792506\n",
      "Validation Loss for fold 2: 0.05067190279563268\n",
      "Validation Loss for fold 2: 0.05200407778223356\n",
      "Validation Loss for fold 2: 0.05256715416908264\n",
      "Validation Loss for fold 2: 0.05216481536626816\n",
      "Validation Loss for fold 2: 0.050671208649873734\n",
      "Validation Loss for fold 2: 0.04726943684120973\n",
      "Validation Loss for fold 2: 0.05258101473251978\n",
      "Validation Loss for fold 2: 0.05481832226117452\n",
      "Validation Loss for fold 2: 0.05255666375160217\n",
      "Validation Loss for fold 2: 0.05411833648880323\n",
      "Validation Loss for fold 2: 0.051167414834102\n",
      "Validation Loss for fold 2: 0.05156131088733673\n",
      "Validation Loss for fold 2: 0.05230151613553365\n",
      "Validation Loss for fold 2: 0.05123560751477877\n",
      "Validation Loss for fold 2: 0.052135987828175225\n",
      "Validation Loss for fold 2: 0.0560128316283226\n",
      "Validation Loss for fold 2: 0.048511356115341187\n",
      "Validation Loss for fold 2: 0.050420732547839485\n",
      "Validation Loss for fold 2: 0.05295322835445404\n",
      "Validation Loss for fold 2: 0.053484960148731865\n",
      "Validation Loss for fold 2: 0.05197541912396749\n",
      "Validation Loss for fold 2: 0.04865653316179911\n",
      "Validation Loss for fold 2: 0.05104734003543854\n",
      "Validation Loss for fold 2: 0.05697085832556089\n",
      "Validation Loss for fold 2: 0.05329492191473643\n",
      "Validation Loss for fold 2: 0.05108159904678663\n",
      "Validation Loss for fold 2: 0.04676532497008642\n",
      "Validation Loss for fold 2: 0.05374689151843389\n",
      "Validation Loss for fold 2: 0.052823840330044426\n",
      "Validation Loss for fold 2: 0.05007147664825121\n",
      "Validation Loss for fold 2: 0.05493491391340891\n",
      "Validation Loss for fold 2: 0.052218783646821976\n",
      "Validation Loss for fold 2: 0.053255329529444374\n",
      "Validation Loss for fold 2: 0.053083863109350204\n",
      "Validation Loss for fold 2: 0.05381640543540319\n",
      "Validation Loss for fold 2: 0.054320244739452996\n",
      "Validation Loss for fold 2: 0.05385632564624151\n",
      "Validation Loss for fold 2: 0.051270210494597755\n",
      "Validation Loss for fold 2: 0.05149859314163526\n",
      "Validation Loss for fold 2: 0.056737943241993584\n",
      "Validation Loss for fold 2: 0.04984815915425619\n",
      "Validation Loss for fold 2: 0.05031062910954157\n",
      "Validation Loss for fold 2: 0.051402702927589417\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "Validation Loss for fold 3: 0.490678350130717\n",
      "Validation Loss for fold 3: 0.3671505848566691\n",
      "Validation Loss for fold 3: 0.284195472796758\n",
      "Validation Loss for fold 3: 0.2212029000123342\n",
      "Validation Loss for fold 3: 0.1899582395950953\n",
      "Validation Loss for fold 3: 0.16082330296436945\n",
      "Validation Loss for fold 3: 0.16573547820250192\n",
      "Validation Loss for fold 3: 0.14371821284294128\n",
      "Validation Loss for fold 3: 0.13618803024291992\n",
      "Validation Loss for fold 3: 0.11793575932582219\n",
      "Validation Loss for fold 3: 0.12673977514108023\n",
      "Validation Loss for fold 3: 0.12302593638499577\n",
      "Validation Loss for fold 3: 0.12080751607815425\n",
      "Validation Loss for fold 3: 0.09997735420862834\n",
      "Validation Loss for fold 3: 0.10713830838600795\n",
      "Validation Loss for fold 3: 0.10570323715607326\n",
      "Validation Loss for fold 3: 0.10006432731946309\n",
      "Validation Loss for fold 3: 0.10477904478708903\n",
      "Validation Loss for fold 3: 0.09655589361985524\n",
      "Validation Loss for fold 3: 0.10757621377706528\n",
      "Validation Loss for fold 3: 0.10230988760789235\n",
      "Validation Loss for fold 3: 0.10281412800153096\n",
      "Validation Loss for fold 3: 0.09470135470231374\n",
      "Validation Loss for fold 3: 0.08794810498754184\n",
      "Validation Loss for fold 3: 0.08993008484443028\n",
      "Validation Loss for fold 3: 0.09077527622381847\n",
      "Validation Loss for fold 3: 0.09316971401373546\n",
      "Validation Loss for fold 3: 0.10291320830583572\n",
      "Validation Loss for fold 3: 0.09703329702218373\n",
      "Validation Loss for fold 3: 0.09023084739844005\n",
      "Validation Loss for fold 3: 0.08050272737940152\n",
      "Validation Loss for fold 3: 0.0949057787656784\n",
      "Validation Loss for fold 3: 0.08309217542409897\n",
      "Validation Loss for fold 3: 0.08846805493036906\n",
      "Validation Loss for fold 3: 0.09101061522960663\n",
      "Validation Loss for fold 3: 0.08310402184724808\n",
      "Validation Loss for fold 3: 0.0873129665851593\n",
      "Validation Loss for fold 3: 0.08573077619075775\n",
      "Validation Loss for fold 3: 0.09339015185832977\n",
      "Validation Loss for fold 3: 0.07941394299268723\n",
      "Validation Loss for fold 3: 0.09058557699124019\n",
      "Validation Loss for fold 3: 0.08178746203581493\n",
      "Validation Loss for fold 3: 0.08893323689699173\n",
      "Validation Loss for fold 3: 0.08984455714623134\n",
      "Validation Loss for fold 3: 0.09047960738341014\n",
      "Validation Loss for fold 3: 0.08531618863344193\n",
      "Validation Loss for fold 3: 0.08333409329255421\n",
      "Validation Loss for fold 3: 0.08450592309236526\n",
      "Validation Loss for fold 3: 0.0933139647046725\n",
      "Validation Loss for fold 3: 0.08035977681477864\n",
      "Validation Loss for fold 3: 0.08004694680372874\n",
      "Validation Loss for fold 3: 0.08338489880164464\n",
      "Validation Loss for fold 3: 0.08561025808254878\n",
      "Validation Loss for fold 3: 0.08578560004631679\n",
      "Validation Loss for fold 3: 0.08617392679055531\n",
      "Validation Loss for fold 3: 0.0790938859184583\n",
      "Validation Loss for fold 3: 0.07899339248736699\n",
      "Validation Loss for fold 3: 0.07988803585370381\n",
      "Validation Loss for fold 3: 0.07494740560650826\n",
      "Validation Loss for fold 3: 0.08185371508200963\n",
      "Validation Loss for fold 3: 0.08026062945524852\n",
      "Validation Loss for fold 3: 0.07812354962031047\n",
      "Validation Loss for fold 3: 0.08126852164665858\n",
      "Validation Loss for fold 3: 0.0830933153629303\n",
      "Validation Loss for fold 3: 0.07789847006400426\n",
      "Validation Loss for fold 3: 0.07583008209864299\n",
      "Validation Loss for fold 3: 0.0718192532658577\n",
      "Validation Loss for fold 3: 0.07739678770303726\n",
      "Validation Loss for fold 3: 0.08201535046100616\n",
      "Validation Loss for fold 3: 0.07957425713539124\n",
      "Validation Loss for fold 3: 0.07368654757738113\n",
      "Validation Loss for fold 3: 0.07389867305755615\n",
      "Validation Loss for fold 3: 0.07833495984474818\n",
      "Validation Loss for fold 3: 0.07589355359474818\n",
      "Validation Loss for fold 3: 0.07725957036018372\n",
      "Validation Loss for fold 3: 0.07793134699265163\n",
      "Validation Loss for fold 3: 0.07226474955677986\n",
      "Validation Loss for fold 3: 0.08308517932891846\n",
      "Validation Loss for fold 3: 0.07000563169519107\n",
      "Validation Loss for fold 3: 0.07762252291043599\n",
      "Validation Loss for fold 3: 0.07723382860422134\n",
      "Validation Loss for fold 3: 0.08155499150355656\n",
      "Validation Loss for fold 3: 0.0807156041264534\n",
      "Validation Loss for fold 3: 0.0712879238029321\n",
      "Validation Loss for fold 3: 0.07163986315329869\n",
      "Validation Loss for fold 3: 0.07050203035275142\n",
      "Validation Loss for fold 3: 0.07049136608839035\n",
      "Validation Loss for fold 3: 0.0747322216629982\n",
      "Validation Loss for fold 3: 0.07671758532524109\n",
      "Validation Loss for fold 3: 0.07164214551448822\n",
      "Validation Loss for fold 3: 0.07208411643902461\n",
      "Validation Loss for fold 3: 0.07781121879816055\n",
      "Validation Loss for fold 3: 0.07618656009435654\n",
      "Validation Loss for fold 3: 0.07197265326976776\n",
      "Validation Loss for fold 3: 0.06355410069227219\n",
      "Validation Loss for fold 3: 0.07826252033313115\n",
      "Validation Loss for fold 3: 0.072250597178936\n",
      "Validation Loss for fold 3: 0.07912095884482066\n",
      "Validation Loss for fold 3: 0.07183379183212917\n",
      "Validation Loss for fold 3: 0.06773960838715236\n",
      "Validation Loss for fold 3: 0.06977268556753795\n",
      "Validation Loss for fold 3: 0.07620712866385777\n",
      "Validation Loss for fold 3: 0.07269076257944107\n",
      "Validation Loss for fold 3: 0.0731841226418813\n",
      "Validation Loss for fold 3: 0.06887598459919293\n",
      "Validation Loss for fold 3: 0.0720677599310875\n",
      "Validation Loss for fold 3: 0.07065191616614659\n",
      "Validation Loss for fold 3: 0.06778660044074059\n",
      "Validation Loss for fold 3: 0.06667873139182727\n",
      "Validation Loss for fold 3: 0.06996309508879979\n",
      "Validation Loss for fold 3: 0.071637029449145\n",
      "Validation Loss for fold 3: 0.069712795317173\n",
      "Validation Loss for fold 3: 0.0698081726829211\n",
      "Validation Loss for fold 3: 0.07442279408375423\n",
      "Validation Loss for fold 3: 0.06879319747289021\n",
      "Validation Loss for fold 3: 0.0668327808380127\n",
      "Validation Loss for fold 3: 0.06882476806640625\n",
      "Validation Loss for fold 3: 0.06865988671779633\n",
      "Validation Loss for fold 3: 0.07130577166875203\n",
      "Validation Loss for fold 3: 0.06574774036804835\n",
      "Validation Loss for fold 3: 0.06975143526991208\n",
      "Validation Loss for fold 3: 0.07133673131465912\n",
      "Validation Loss for fold 3: 0.06614297131697337\n",
      "Validation Loss for fold 3: 0.07088472694158554\n",
      "Validation Loss for fold 3: 0.07251430302858353\n",
      "Validation Loss for fold 3: 0.06924077123403549\n",
      "Validation Loss for fold 3: 0.0667759155233701\n",
      "Validation Loss for fold 3: 0.06412963072458903\n",
      "Validation Loss for fold 3: 0.07079180578390758\n",
      "Validation Loss for fold 3: 0.06570013985037804\n",
      "Validation Loss for fold 3: 0.06787241746981938\n",
      "Validation Loss for fold 3: 0.06050994743903478\n",
      "Validation Loss for fold 3: 0.06693092236916225\n",
      "Validation Loss for fold 3: 0.07121049736936887\n",
      "Validation Loss for fold 3: 0.06628269329667091\n",
      "Validation Loss for fold 3: 0.0670434981584549\n",
      "Validation Loss for fold 3: 0.06587049613396327\n",
      "Validation Loss for fold 3: 0.06518992657462756\n",
      "Validation Loss for fold 3: 0.06931180755297343\n",
      "Validation Loss for fold 3: 0.06690729906161626\n",
      "Validation Loss for fold 3: 0.06639916201432546\n",
      "Validation Loss for fold 3: 0.06178566192587217\n",
      "Validation Loss for fold 3: 0.06897384176651637\n",
      "Validation Loss for fold 3: 0.06181889399886131\n",
      "Validation Loss for fold 3: 0.0648585061232249\n",
      "Validation Loss for fold 3: 0.06360987325509389\n",
      "Validation Loss for fold 3: 0.0671505555510521\n",
      "Validation Loss for fold 3: 0.06511999169985454\n",
      "Validation Loss for fold 3: 0.06687733282645543\n",
      "Validation Loss for fold 3: 0.06154195343454679\n",
      "Validation Loss for fold 3: 0.06187300508220991\n",
      "Validation Loss for fold 3: 0.06697491059700648\n",
      "Validation Loss for fold 3: 0.06906563291947047\n",
      "Validation Loss for fold 3: 0.06188266848524412\n",
      "Validation Loss for fold 3: 0.06072027484575907\n",
      "Validation Loss for fold 3: 0.06467257191737492\n",
      "Validation Loss for fold 3: 0.07242026676734288\n",
      "Validation Loss for fold 3: 0.06722701092561086\n",
      "Validation Loss for fold 3: 0.06194361671805382\n",
      "Validation Loss for fold 3: 0.07114466900626819\n",
      "Validation Loss for fold 3: 0.06424561391274135\n",
      "Validation Loss for fold 3: 0.05731553335984548\n",
      "Validation Loss for fold 3: 0.06356953456997871\n",
      "Validation Loss for fold 3: 0.06984888762235641\n",
      "Validation Loss for fold 3: 0.06453383093078931\n",
      "Validation Loss for fold 3: 0.06107793375849724\n",
      "Validation Loss for fold 3: 0.06394321471452713\n",
      "Validation Loss for fold 3: 0.0617423877120018\n",
      "Validation Loss for fold 3: 0.06397814055283864\n",
      "Validation Loss for fold 3: 0.07091642171144485\n",
      "Validation Loss for fold 3: 0.06093881279230118\n",
      "Validation Loss for fold 3: 0.06637107580900192\n",
      "Validation Loss for fold 3: 0.06205817560354868\n",
      "Validation Loss for fold 3: 0.06600999583800633\n",
      "Validation Loss for fold 3: 0.0678246592481931\n",
      "Validation Loss for fold 3: 0.058470201989014946\n",
      "Validation Loss for fold 3: 0.06099467724561691\n",
      "Validation Loss for fold 3: 0.06270657107234001\n",
      "Validation Loss for fold 3: 0.06077185148994128\n",
      "Validation Loss for fold 3: 0.06312213465571404\n",
      "Validation Loss for fold 3: 0.06868387386202812\n",
      "Validation Loss for fold 3: 0.06115928664803505\n",
      "Validation Loss for fold 3: 0.06251277898748715\n",
      "Validation Loss for fold 3: 0.06128045668204626\n",
      "Validation Loss for fold 3: 0.06052816907564799\n",
      "Validation Loss for fold 3: 0.06363521019617717\n",
      "Validation Loss for fold 3: 0.06098972136775652\n",
      "Validation Loss for fold 3: 0.07216928030053775\n",
      "Validation Loss for fold 3: 0.06575806066393852\n",
      "Validation Loss for fold 3: 0.0619880681236585\n",
      "Validation Loss for fold 3: 0.06612906108299892\n",
      "Validation Loss for fold 3: 0.06223133827249209\n",
      "Validation Loss for fold 3: 0.060035968820254006\n",
      "Validation Loss for fold 3: 0.06371460358301799\n",
      "Validation Loss for fold 3: 0.06003042683005333\n",
      "Validation Loss for fold 3: 0.05957347402969996\n",
      "Validation Loss for fold 3: 0.06029891098539034\n",
      "Validation Loss for fold 3: 0.061957597732543945\n",
      "Validation Loss for fold 3: 0.06408962483207385\n",
      "Validation Loss for fold 3: 0.06290620813767116\n",
      "Validation Loss for fold 3: 0.06668453787763913\n",
      "Validation Loss for fold 3: 0.06667961304386456\n",
      "Validation Loss for fold 3: 0.06211559846997261\n",
      "Validation Loss for fold 3: 0.06456768264373143\n",
      "Validation Loss for fold 3: 0.0570851465066274\n",
      "Validation Loss for fold 3: 0.05811225871245066\n",
      "Validation Loss for fold 3: 0.07024602467815082\n",
      "Validation Loss for fold 3: 0.05767161647478739\n",
      "Validation Loss for fold 3: 0.06306894620259602\n",
      "Validation Loss for fold 3: 0.06417752305666606\n",
      "Validation Loss for fold 3: 0.06464297324419022\n",
      "Validation Loss for fold 3: 0.0600150798757871\n",
      "Validation Loss for fold 3: 0.065331036845843\n",
      "Validation Loss for fold 3: 0.06393373633424441\n",
      "Validation Loss for fold 3: 0.06263330578804016\n",
      "Validation Loss for fold 3: 0.06825243930021922\n",
      "Validation Loss for fold 3: 0.061292255918184914\n",
      "Validation Loss for fold 3: 0.06509623552362125\n",
      "Validation Loss for fold 3: 0.06319000323613484\n",
      "Validation Loss for fold 3: 0.06393967817227046\n",
      "Validation Loss for fold 3: 0.06611077984174092\n",
      "Validation Loss for fold 3: 0.0645886870721976\n",
      "Validation Loss for fold 3: 0.06230657051006953\n",
      "Validation Loss for fold 3: 0.0683170681198438\n",
      "Validation Loss for fold 3: 0.060269574324289955\n",
      "Validation Loss for fold 3: 0.06126414239406586\n",
      "Validation Loss for fold 3: 0.055868279188871384\n",
      "Validation Loss for fold 3: 0.06355048343539238\n",
      "Validation Loss for fold 3: 0.06094881519675255\n",
      "Validation Loss for fold 3: 0.06050517037510872\n",
      "Validation Loss for fold 3: 0.06277548273404439\n",
      "Validation Loss for fold 3: 0.06051104019085566\n",
      "Validation Loss for fold 3: 0.05459722628196081\n",
      "Validation Loss for fold 3: 0.05557890236377716\n",
      "Validation Loss for fold 3: 0.061081082870562874\n",
      "Validation Loss for fold 3: 0.06791338572899501\n",
      "Validation Loss for fold 3: 0.0650806116561095\n",
      "Validation Loss for fold 3: 0.06178200369079908\n",
      "Validation Loss for fold 3: 0.06176941841840744\n",
      "Validation Loss for fold 3: 0.06073013817270597\n",
      "Validation Loss for fold 3: 0.06500688443581264\n",
      "Validation Loss for fold 3: 0.06464907030264537\n",
      "Validation Loss for fold 3: 0.05802834282318751\n",
      "Validation Loss for fold 3: 0.05738785987099012\n",
      "Validation Loss for fold 3: 0.06774495169520378\n",
      "Validation Loss for fold 3: 0.0617941419283549\n",
      "Validation Loss for fold 3: 0.06545207773645718\n",
      "Validation Loss for fold 3: 0.05367681942880154\n",
      "Validation Loss for fold 3: 0.06178813179334005\n",
      "Validation Loss for fold 3: 0.061797761668761574\n",
      "Validation Loss for fold 3: 0.055008143186569214\n",
      "Validation Loss for fold 3: 0.06024147445956866\n",
      "Validation Loss for fold 3: 0.061079212774833046\n",
      "Validation Loss for fold 3: 0.0618418405453364\n",
      "Validation Loss for fold 3: 0.060331994046767555\n",
      "Validation Loss for fold 3: 0.061806743343671165\n",
      "Validation Loss for fold 3: 0.05555325374007225\n",
      "Validation Loss for fold 3: 0.05720351760586103\n",
      "Validation Loss for fold 3: 0.06032208725810051\n",
      "Validation Loss for fold 3: 0.05658952643473943\n",
      "Validation Loss for fold 3: 0.05608982592821121\n",
      "Validation Loss for fold 3: 0.06482084716359775\n",
      "Validation Loss for fold 3: 0.06648404399553935\n",
      "Validation Loss for fold 3: 0.05964550624291102\n",
      "Validation Loss for fold 3: 0.061911359429359436\n",
      "Validation Loss for fold 3: 0.06253660346070926\n",
      "Validation Loss for fold 3: 0.06313437968492508\n",
      "Validation Loss for fold 3: 0.06418926517168681\n",
      "Validation Loss for fold 3: 0.0626648577551047\n",
      "Validation Loss for fold 3: 0.06315319736798604\n",
      "Validation Loss for fold 3: 0.05766098449627558\n",
      "Validation Loss for fold 3: 0.06211612621943156\n",
      "Validation Loss for fold 3: 0.05791709075371424\n",
      "Validation Loss for fold 3: 0.060378403713305794\n",
      "Validation Loss for fold 3: 0.05992744490504265\n",
      "Validation Loss for fold 3: 0.06341588993867238\n",
      "Validation Loss for fold 3: 0.057412002235651016\n",
      "Validation Loss for fold 3: 0.05623466894030571\n",
      "Validation Loss for fold 3: 0.06134419764081637\n",
      "Validation Loss for fold 3: 0.06040619189540545\n",
      "Validation Loss for fold 3: 0.05912983665863673\n",
      "Validation Loss for fold 3: 0.05892329663038254\n",
      "Validation Loss for fold 3: 0.06406646097699802\n",
      "Validation Loss for fold 3: 0.061943174650271736\n",
      "Validation Loss for fold 3: 0.05980444326996803\n",
      "Validation Loss for fold 3: 0.05831064780553182\n",
      "Validation Loss for fold 3: 0.05651920040448507\n",
      "Validation Loss for fold 3: 0.05992692584792773\n",
      "Validation Loss for fold 3: 0.0614468976855278\n",
      "Validation Loss for fold 3: 0.05910850316286087\n",
      "Validation Loss for fold 3: 0.05931036050120989\n",
      "Validation Loss for fold 3: 0.05953781182567278\n",
      "Validation Loss for fold 3: 0.05472330003976822\n",
      "Validation Loss for fold 3: 0.06052397936582565\n",
      "Validation Loss for fold 3: 0.05844029039144516\n",
      "Validation Loss for fold 3: 0.05812855809926987\n",
      "Validation Loss for fold 3: 0.05643913522362709\n",
      "Validation Loss for fold 3: 0.060048618664344154\n",
      "Validation Loss for fold 3: 0.05847674980759621\n",
      "Validation Loss for fold 3: 0.06846596921483676\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "Validation Loss for fold 4: 0.62833172082901\n",
      "Validation Loss for fold 4: 0.5101828972498575\n",
      "Validation Loss for fold 4: 0.36929742495218915\n",
      "Validation Loss for fold 4: 0.28583423296610516\n",
      "Validation Loss for fold 4: 0.27457185586293537\n",
      "Validation Loss for fold 4: 0.21944520870844522\n",
      "Validation Loss for fold 4: 0.20628617207209268\n",
      "Validation Loss for fold 4: 0.2091703861951828\n",
      "Validation Loss for fold 4: 0.1613828937212626\n",
      "Validation Loss for fold 4: 0.1538381576538086\n",
      "Validation Loss for fold 4: 0.14604075501362482\n",
      "Validation Loss for fold 4: 0.13581511874993643\n",
      "Validation Loss for fold 4: 0.15065987408161163\n",
      "Validation Loss for fold 4: 0.1450995405515035\n",
      "Validation Loss for fold 4: 0.1303564334909121\n",
      "Validation Loss for fold 4: 0.13580443461736044\n",
      "Validation Loss for fold 4: 0.12062440564235051\n",
      "Validation Loss for fold 4: 0.13039705157279968\n",
      "Validation Loss for fold 4: 0.10725970317920049\n",
      "Validation Loss for fold 4: 0.11595980077981949\n",
      "Validation Loss for fold 4: 0.12401752670605977\n",
      "Validation Loss for fold 4: 0.10786823431650798\n",
      "Validation Loss for fold 4: 0.11610914021730423\n",
      "Validation Loss for fold 4: 0.12399838368097942\n",
      "Validation Loss for fold 4: 0.11057929943005244\n",
      "Validation Loss for fold 4: 0.09929069379965465\n",
      "Validation Loss for fold 4: 0.11792456358671188\n",
      "Validation Loss for fold 4: 0.11294318983952205\n",
      "Validation Loss for fold 4: 0.12589415411154428\n",
      "Validation Loss for fold 4: 0.10847648978233337\n",
      "Validation Loss for fold 4: 0.10000768800576527\n",
      "Validation Loss for fold 4: 0.10176321615775426\n",
      "Validation Loss for fold 4: 0.09697680920362473\n",
      "Validation Loss for fold 4: 0.09981675446033478\n",
      "Validation Loss for fold 4: 0.09793540587027867\n",
      "Validation Loss for fold 4: 0.0977809304992358\n",
      "Validation Loss for fold 4: 0.09757479280233383\n",
      "Validation Loss for fold 4: 0.09578073024749756\n",
      "Validation Loss for fold 4: 0.09305486579736073\n",
      "Validation Loss for fold 4: 0.09366073707739513\n",
      "Validation Loss for fold 4: 0.08790310968955357\n",
      "Validation Loss for fold 4: 0.08821086585521698\n",
      "Validation Loss for fold 4: 0.09770099073648453\n",
      "Validation Loss for fold 4: 0.08288184677561124\n",
      "Validation Loss for fold 4: 0.08704230189323425\n",
      "Validation Loss for fold 4: 0.09020044654607773\n",
      "Validation Loss for fold 4: 0.08499622096618016\n",
      "Validation Loss for fold 4: 0.08524424582719803\n",
      "Validation Loss for fold 4: 0.095516932507356\n",
      "Validation Loss for fold 4: 0.09857220947742462\n",
      "Validation Loss for fold 4: 0.09225452939669292\n",
      "Validation Loss for fold 4: 0.08477020760377248\n",
      "Validation Loss for fold 4: 0.09441515803337097\n",
      "Validation Loss for fold 4: 0.08441510051488876\n",
      "Validation Loss for fold 4: 0.09350996961196263\n",
      "Validation Loss for fold 4: 0.08783383667469025\n",
      "Validation Loss for fold 4: 0.08429121722777684\n",
      "Validation Loss for fold 4: 0.08772603174050649\n",
      "Validation Loss for fold 4: 0.09404270599285762\n",
      "Validation Loss for fold 4: 0.09346386541922887\n",
      "Validation Loss for fold 4: 0.08582036942243576\n",
      "Validation Loss for fold 4: 0.07980886101722717\n",
      "Validation Loss for fold 4: 0.08953735729058583\n",
      "Validation Loss for fold 4: 0.08728493253389995\n",
      "Validation Loss for fold 4: 0.07618353764216106\n",
      "Validation Loss for fold 4: 0.0816688984632492\n",
      "Validation Loss for fold 4: 0.08355206499497096\n",
      "Validation Loss for fold 4: 0.07150871555010478\n",
      "Validation Loss for fold 4: 0.07895837724208832\n",
      "Validation Loss for fold 4: 0.07531290377179782\n",
      "Validation Loss for fold 4: 0.08067329476277034\n",
      "Validation Loss for fold 4: 0.0728066513935725\n",
      "Validation Loss for fold 4: 0.0867139274875323\n",
      "Validation Loss for fold 4: 0.08573758353789647\n",
      "Validation Loss for fold 4: 0.07726890842119853\n",
      "Validation Loss for fold 4: 0.08164440592130025\n",
      "Validation Loss for fold 4: 0.07524148126443227\n",
      "Validation Loss for fold 4: 0.07462165007988612\n",
      "Validation Loss for fold 4: 0.06979096680879593\n",
      "Validation Loss for fold 4: 0.07842018951972325\n",
      "Validation Loss for fold 4: 0.07622379561265309\n",
      "Validation Loss for fold 4: 0.08194748312234879\n",
      "Validation Loss for fold 4: 0.0722414826353391\n",
      "Validation Loss for fold 4: 0.0731165458758672\n",
      "Validation Loss for fold 4: 0.07312086845437686\n",
      "Validation Loss for fold 4: 0.07701487590869267\n",
      "Validation Loss for fold 4: 0.07728827248016994\n",
      "Validation Loss for fold 4: 0.0763127754131953\n",
      "Validation Loss for fold 4: 0.0691360669831435\n",
      "Validation Loss for fold 4: 0.07093753789861997\n",
      "Validation Loss for fold 4: 0.07410081724325816\n",
      "Validation Loss for fold 4: 0.07507434487342834\n",
      "Validation Loss for fold 4: 0.07099900394678116\n",
      "Validation Loss for fold 4: 0.07780773565173149\n",
      "Validation Loss for fold 4: 0.07044681906700134\n",
      "Validation Loss for fold 4: 0.06617942576607068\n",
      "Validation Loss for fold 4: 0.0726499358812968\n",
      "Validation Loss for fold 4: 0.08215679228305817\n",
      "Validation Loss for fold 4: 0.06568179403742154\n",
      "Validation Loss for fold 4: 0.07082656522591908\n",
      "Validation Loss for fold 4: 0.06833862016598384\n",
      "Validation Loss for fold 4: 0.06572755550344785\n",
      "Validation Loss for fold 4: 0.06981233010689418\n",
      "Validation Loss for fold 4: 0.07061961044867833\n",
      "Validation Loss for fold 4: 0.07103319714466731\n",
      "Validation Loss for fold 4: 0.06433182830611865\n",
      "Validation Loss for fold 4: 0.07086528837680817\n",
      "Validation Loss for fold 4: 0.07194479306538899\n",
      "Validation Loss for fold 4: 0.06670258442560832\n",
      "Validation Loss for fold 4: 0.06888485203186671\n",
      "Validation Loss for fold 4: 0.06345564251144727\n",
      "Validation Loss for fold 4: 0.061507166673739753\n",
      "Validation Loss for fold 4: 0.0705743742485841\n",
      "Validation Loss for fold 4: 0.06551523258288701\n",
      "Validation Loss for fold 4: 0.061321381479501724\n",
      "Validation Loss for fold 4: 0.06574110438426335\n",
      "Validation Loss for fold 4: 0.07699582229057948\n",
      "Validation Loss for fold 4: 0.06685425092776616\n",
      "Validation Loss for fold 4: 0.06105176731944084\n",
      "Validation Loss for fold 4: 0.06114963814616203\n",
      "Validation Loss for fold 4: 0.06979445119698842\n",
      "Validation Loss for fold 4: 0.07593286285797755\n",
      "Validation Loss for fold 4: 0.07184998318552971\n",
      "Validation Loss for fold 4: 0.0673377513885498\n",
      "Validation Loss for fold 4: 0.06862833971778552\n",
      "Validation Loss for fold 4: 0.06963805109262466\n",
      "Validation Loss for fold 4: 0.0702587974568208\n",
      "Validation Loss for fold 4: 0.06042019526163737\n",
      "Validation Loss for fold 4: 0.06409550334016482\n",
      "Validation Loss for fold 4: 0.061836760491132736\n",
      "Validation Loss for fold 4: 0.06277141720056534\n",
      "Validation Loss for fold 4: 0.06322307387987773\n",
      "Validation Loss for fold 4: 0.06519996002316475\n",
      "Validation Loss for fold 4: 0.06408899029095967\n",
      "Validation Loss for fold 4: 0.06804324686527252\n",
      "Validation Loss for fold 4: 0.0676236276825269\n",
      "Validation Loss for fold 4: 0.06245510776837667\n",
      "Validation Loss for fold 4: 0.0710548684000969\n",
      "Validation Loss for fold 4: 0.06025920187433561\n",
      "Validation Loss for fold 4: 0.06286045163869858\n",
      "Validation Loss for fold 4: 0.06205421810348829\n",
      "Validation Loss for fold 4: 0.06746327628691991\n",
      "Validation Loss for fold 4: 0.058802386124928795\n",
      "Validation Loss for fold 4: 0.06113655989368757\n",
      "Validation Loss for fold 4: 0.057824451476335526\n",
      "Validation Loss for fold 4: 0.058204516768455505\n",
      "Validation Loss for fold 4: 0.059797012557586036\n",
      "Validation Loss for fold 4: 0.06753318135937054\n",
      "Validation Loss for fold 4: 0.06321794788042705\n",
      "Validation Loss for fold 4: 0.06368569160501163\n",
      "Validation Loss for fold 4: 0.057204414159059525\n",
      "Validation Loss for fold 4: 0.06320977831880252\n",
      "Validation Loss for fold 4: 0.06080997362732887\n",
      "Validation Loss for fold 4: 0.0662316766877969\n",
      "Validation Loss for fold 4: 0.06533203770716985\n",
      "Validation Loss for fold 4: 0.06081141158938408\n",
      "Validation Loss for fold 4: 0.06258344526092212\n",
      "Validation Loss for fold 4: 0.06473906710743904\n",
      "Validation Loss for fold 4: 0.061761751770973206\n",
      "Validation Loss for fold 4: 0.05910957852999369\n",
      "Validation Loss for fold 4: 0.06469883521397908\n",
      "Validation Loss for fold 4: 0.058690776427586876\n",
      "Validation Loss for fold 4: 0.061726501832405724\n",
      "Validation Loss for fold 4: 0.05955773591995239\n",
      "Validation Loss for fold 4: 0.06142701581120491\n",
      "Validation Loss for fold 4: 0.06096748635172844\n",
      "Validation Loss for fold 4: 0.06090970834096273\n",
      "Validation Loss for fold 4: 0.06499320020278294\n",
      "Validation Loss for fold 4: 0.06333038210868835\n",
      "Validation Loss for fold 4: 0.05957969898978869\n",
      "Validation Loss for fold 4: 0.05377793684601784\n",
      "Validation Loss for fold 4: 0.06527931119004886\n",
      "Validation Loss for fold 4: 0.055736386527617775\n",
      "Validation Loss for fold 4: 0.05778582518299421\n",
      "Validation Loss for fold 4: 0.06111243863900503\n",
      "Validation Loss for fold 4: 0.05732080588738123\n",
      "Validation Loss for fold 4: 0.06282500674327214\n",
      "Validation Loss for fold 4: 0.051481449976563454\n",
      "Validation Loss for fold 4: 0.058366006861130394\n",
      "Validation Loss for fold 4: 0.06291267524162929\n",
      "Validation Loss for fold 4: 0.063236765563488\n",
      "Validation Loss for fold 4: 0.06360163042942683\n",
      "Validation Loss for fold 4: 0.06322130809227626\n",
      "Validation Loss for fold 4: 0.05828287824988365\n",
      "Validation Loss for fold 4: 0.0592819390197595\n",
      "Validation Loss for fold 4: 0.05937990670402845\n",
      "Validation Loss for fold 4: 0.06400660177071889\n",
      "Validation Loss for fold 4: 0.05581914260983467\n",
      "Validation Loss for fold 4: 0.06516784181197484\n",
      "Validation Loss for fold 4: 0.05864432826638222\n",
      "Validation Loss for fold 4: 0.06103132416804632\n",
      "Validation Loss for fold 4: 0.05388371894756953\n",
      "Validation Loss for fold 4: 0.057525262236595154\n",
      "Validation Loss for fold 4: 0.0576678899427255\n",
      "Validation Loss for fold 4: 0.0614198682208856\n",
      "Validation Loss for fold 4: 0.055375351260105767\n",
      "Validation Loss for fold 4: 0.05842298517624537\n",
      "Validation Loss for fold 4: 0.05939865733186404\n",
      "Validation Loss for fold 4: 0.05847508211930593\n",
      "Validation Loss for fold 4: 0.05860772977272669\n",
      "Validation Loss for fold 4: 0.0628276417652766\n",
      "Validation Loss for fold 4: 0.055944001923004784\n",
      "Validation Loss for fold 4: 0.056256054590145745\n",
      "Validation Loss for fold 4: 0.05809438352783521\n",
      "Validation Loss for fold 4: 0.05672784149646759\n",
      "Validation Loss for fold 4: 0.05904580404361089\n",
      "Validation Loss for fold 4: 0.055900175124406815\n",
      "Validation Loss for fold 4: 0.062017244597276054\n",
      "Validation Loss for fold 4: 0.05887488151590029\n",
      "Validation Loss for fold 4: 0.05816434447964033\n",
      "Validation Loss for fold 4: 0.05771141623457273\n",
      "Validation Loss for fold 4: 0.05558106179038683\n",
      "Validation Loss for fold 4: 0.057462383061647415\n",
      "Validation Loss for fold 4: 0.05910956859588623\n",
      "Validation Loss for fold 4: 0.05316715563337008\n",
      "Validation Loss for fold 4: 0.052272104968627296\n",
      "Validation Loss for fold 4: 0.05638551836212476\n",
      "Validation Loss for fold 4: 0.05412434786558151\n",
      "Validation Loss for fold 4: 0.05372430011630058\n",
      "Validation Loss for fold 4: 0.0546039417386055\n",
      "Validation Loss for fold 4: 0.059204887598752975\n",
      "Validation Loss for fold 4: 0.061075251549482346\n",
      "Validation Loss for fold 4: 0.06566674013932546\n",
      "Validation Loss for fold 4: 0.05205009877681732\n",
      "Validation Loss for fold 4: 0.05365088830391566\n",
      "Validation Loss for fold 4: 0.05418882022301356\n",
      "Validation Loss for fold 4: 0.05368648966153463\n",
      "Validation Loss for fold 4: 0.06197154025236765\n",
      "Validation Loss for fold 4: 0.05545633286237717\n",
      "Validation Loss for fold 4: 0.054340675473213196\n",
      "Validation Loss for fold 4: 0.05104530851046244\n",
      "Validation Loss for fold 4: 0.05533089985450109\n",
      "Validation Loss for fold 4: 0.05736419061819712\n",
      "Validation Loss for fold 4: 0.05691747491558393\n",
      "Validation Loss for fold 4: 0.0542127899825573\n",
      "Validation Loss for fold 4: 0.053615337858597435\n",
      "Validation Loss for fold 4: 0.05446121469140053\n",
      "Validation Loss for fold 4: 0.05824662744998932\n",
      "Validation Loss for fold 4: 0.05625162646174431\n",
      "Validation Loss for fold 4: 0.0550125427544117\n",
      "Validation Loss for fold 4: 0.05305009335279465\n",
      "Validation Loss for fold 4: 0.06059714655081431\n",
      "Validation Loss for fold 4: 0.06513373802105586\n",
      "Validation Loss for fold 4: 0.060372049609820046\n",
      "Validation Loss for fold 4: 0.06112514932950338\n",
      "Validation Loss for fold 4: 0.05266983682910601\n",
      "Validation Loss for fold 4: 0.058355157574017845\n",
      "Validation Loss for fold 4: 0.05168544501066208\n",
      "Validation Loss for fold 4: 0.05809154361486435\n",
      "Validation Loss for fold 4: 0.05288024991750717\n",
      "Validation Loss for fold 4: 0.06038494904836019\n",
      "Validation Loss for fold 4: 0.05485638231039047\n",
      "Validation Loss for fold 4: 0.05380775406956673\n",
      "Validation Loss for fold 4: 0.05255823458234469\n",
      "Validation Loss for fold 4: 0.05693963790933291\n",
      "Validation Loss for fold 4: 0.05465326954921087\n",
      "Validation Loss for fold 4: 0.05399711678425471\n",
      "Validation Loss for fold 4: 0.05356412877639135\n",
      "Validation Loss for fold 4: 0.05244250471393267\n",
      "Validation Loss for fold 4: 0.05480281636118889\n",
      "Validation Loss for fold 4: 0.060002567867437996\n",
      "Validation Loss for fold 4: 0.052272528409957886\n",
      "Validation Loss for fold 4: 0.0535701351861159\n",
      "Validation Loss for fold 4: 0.05240797499815623\n",
      "Validation Loss for fold 4: 0.055123829593261085\n",
      "Validation Loss for fold 4: 0.050767881174882255\n",
      "Validation Loss for fold 4: 0.054688502103090286\n",
      "Validation Loss for fold 4: 0.05225139235456785\n",
      "Validation Loss for fold 4: 0.05308729906876882\n",
      "Validation Loss for fold 4: 0.05205698559681574\n",
      "Validation Loss for fold 4: 0.059234313666820526\n",
      "Validation Loss for fold 4: 0.049790083120266594\n",
      "Validation Loss for fold 4: 0.06058002511660258\n",
      "Validation Loss for fold 4: 0.05217890193065008\n",
      "Validation Loss for fold 4: 0.051655981689691544\n",
      "Validation Loss for fold 4: 0.054932720959186554\n",
      "Validation Loss for fold 4: 0.05235311885674795\n",
      "Validation Loss for fold 4: 0.06325254340966542\n",
      "Validation Loss for fold 4: 0.05563565840323766\n",
      "Validation Loss for fold 4: 0.05460029716293017\n",
      "Validation Loss for fold 4: 0.054063587139050163\n",
      "Validation Loss for fold 4: 0.05498652532696724\n",
      "Validation Loss for fold 4: 0.050953211883703865\n",
      "Validation Loss for fold 4: 0.05912689616282781\n",
      "Validation Loss for fold 4: 0.05895953128735224\n",
      "Validation Loss for fold 4: 0.05525923023621241\n",
      "Validation Loss for fold 4: 0.05040085315704346\n",
      "Validation Loss for fold 4: 0.05195500701665878\n",
      "Validation Loss for fold 4: 0.05764127026001612\n",
      "Validation Loss for fold 4: 0.049951594322919846\n",
      "Validation Loss for fold 4: 0.05602735777695974\n",
      "Validation Loss for fold 4: 0.06026411677400271\n",
      "Validation Loss for fold 4: 0.05245372653007507\n",
      "Validation Loss for fold 4: 0.05580387016137441\n",
      "Validation Loss for fold 4: 0.054947360108296074\n",
      "Validation Loss for fold 4: 0.05170470724503199\n",
      "Validation Loss for fold 4: 0.055074976136287056\n",
      "Validation Loss for fold 4: 0.05479791139562925\n",
      "Validation Loss for fold 4: 0.053734918435414634\n",
      "Validation Loss for fold 4: 0.06123302628596624\n",
      "--------------------------------\n",
      "FOLD 5\n",
      "--------------------------------\n",
      "Validation Loss for fold 5: 0.8367508053779602\n",
      "Validation Loss for fold 5: 0.6634587446848551\n",
      "Validation Loss for fold 5: 0.526592363913854\n",
      "Validation Loss for fold 5: 0.41461535294850665\n",
      "Validation Loss for fold 5: 0.3486170768737793\n",
      "Validation Loss for fold 5: 0.3068861961364746\n",
      "Validation Loss for fold 5: 0.30605162183443707\n",
      "Validation Loss for fold 5: 0.2513551165660222\n",
      "Validation Loss for fold 5: 0.22530611356099448\n",
      "Validation Loss for fold 5: 0.216961070895195\n",
      "Validation Loss for fold 5: 0.21201742192109427\n",
      "Validation Loss for fold 5: 0.19637211163838705\n",
      "Validation Loss for fold 5: 0.18260948856671652\n",
      "Validation Loss for fold 5: 0.18625406920909882\n",
      "Validation Loss for fold 5: 0.18876475592454275\n",
      "Validation Loss for fold 5: 0.16398856043815613\n",
      "Validation Loss for fold 5: 0.1727603276570638\n",
      "Validation Loss for fold 5: 0.16945291062196097\n",
      "Validation Loss for fold 5: 0.1708679348230362\n",
      "Validation Loss for fold 5: 0.14921478430430093\n",
      "Validation Loss for fold 5: 0.16231281558672586\n",
      "Validation Loss for fold 5: 0.1530421475569407\n",
      "Validation Loss for fold 5: 0.15524553755919138\n",
      "Validation Loss for fold 5: 0.15254072348276773\n",
      "Validation Loss for fold 5: 0.13612747440735498\n",
      "Validation Loss for fold 5: 0.15091006457805634\n",
      "Validation Loss for fold 5: 0.13220052172740301\n",
      "Validation Loss for fold 5: 0.14210332930088043\n",
      "Validation Loss for fold 5: 0.1300270309050878\n",
      "Validation Loss for fold 5: 0.13233337303002676\n",
      "Validation Loss for fold 5: 0.12620632102092108\n",
      "Validation Loss for fold 5: 0.12207821756601334\n",
      "Validation Loss for fold 5: 0.13880755503972372\n",
      "Validation Loss for fold 5: 0.1201042930285136\n",
      "Validation Loss for fold 5: 0.136758491396904\n",
      "Validation Loss for fold 5: 0.12222602715094884\n",
      "Validation Loss for fold 5: 0.11504145711660385\n",
      "Validation Loss for fold 5: 0.12920242796341577\n",
      "Validation Loss for fold 5: 0.11809297154347102\n",
      "Validation Loss for fold 5: 0.12593299398819605\n",
      "Validation Loss for fold 5: 0.1130244458715121\n",
      "Validation Loss for fold 5: 0.12054589887460072\n",
      "Validation Loss for fold 5: 0.11482573548952739\n",
      "Validation Loss for fold 5: 0.11756968994935353\n",
      "Validation Loss for fold 5: 0.11631009976069133\n",
      "Validation Loss for fold 5: 0.11267958829800288\n",
      "Validation Loss for fold 5: 0.11371661722660065\n",
      "Validation Loss for fold 5: 0.12960521131753922\n",
      "Validation Loss for fold 5: 0.11147757122913997\n",
      "Validation Loss for fold 5: 0.11286621789137523\n",
      "Validation Loss for fold 5: 0.1085851863026619\n",
      "Validation Loss for fold 5: 0.11229698856671651\n",
      "Validation Loss for fold 5: 0.09790009260177612\n",
      "Validation Loss for fold 5: 0.10332632809877396\n",
      "Validation Loss for fold 5: 0.11093476663033168\n",
      "Validation Loss for fold 5: 0.09844479709863663\n",
      "Validation Loss for fold 5: 0.102448803683122\n",
      "Validation Loss for fold 5: 0.11343627919753392\n",
      "Validation Loss for fold 5: 0.09728656709194183\n",
      "Validation Loss for fold 5: 0.10341624915599823\n",
      "Validation Loss for fold 5: 0.10323451956113179\n",
      "Validation Loss for fold 5: 0.1008055532972018\n",
      "Validation Loss for fold 5: 0.09288113315900166\n",
      "Validation Loss for fold 5: 0.10843205700318019\n",
      "Validation Loss for fold 5: 0.10105124612649281\n",
      "Validation Loss for fold 5: 0.09017709642648697\n",
      "Validation Loss for fold 5: 0.09179961184660594\n",
      "Validation Loss for fold 5: 0.09959697226683299\n",
      "Validation Loss for fold 5: 0.08992823213338852\n",
      "Validation Loss for fold 5: 0.09582875669002533\n",
      "Validation Loss for fold 5: 0.08995513121287028\n",
      "Validation Loss for fold 5: 0.09839356442292531\n",
      "Validation Loss for fold 5: 0.09527774900197983\n",
      "Validation Loss for fold 5: 0.08871990938981374\n",
      "Validation Loss for fold 5: 0.09286896139383316\n",
      "Validation Loss for fold 5: 0.08914945522944133\n",
      "Validation Loss for fold 5: 0.09835131218036015\n",
      "Validation Loss for fold 5: 0.09667372951904933\n",
      "Validation Loss for fold 5: 0.09653809418280919\n",
      "Validation Loss for fold 5: 0.08556403716405232\n",
      "Validation Loss for fold 5: 0.09354776640733083\n",
      "Validation Loss for fold 5: 0.08969503392775853\n",
      "Validation Loss for fold 5: 0.08970134208599727\n",
      "Validation Loss for fold 5: 0.08731979876756668\n",
      "Validation Loss for fold 5: 0.09456032017866771\n",
      "Validation Loss for fold 5: 0.09132456282774608\n",
      "Validation Loss for fold 5: 0.08541735510031383\n",
      "Validation Loss for fold 5: 0.08263019224007924\n",
      "Validation Loss for fold 5: 0.08683805912733078\n",
      "Validation Loss for fold 5: 0.0870887612303098\n",
      "Validation Loss for fold 5: 0.09178074697653453\n",
      "Validation Loss for fold 5: 0.08615422497193019\n",
      "Validation Loss for fold 5: 0.08418570707241695\n",
      "Validation Loss for fold 5: 0.08261122802893321\n",
      "Validation Loss for fold 5: 0.0888332004348437\n",
      "Validation Loss for fold 5: 0.08844594905773799\n",
      "Validation Loss for fold 5: 0.08484727889299393\n",
      "Validation Loss for fold 5: 0.08526684840520223\n",
      "Validation Loss for fold 5: 0.0841327781478564\n",
      "Validation Loss for fold 5: 0.08022334674994151\n",
      "Validation Loss for fold 5: 0.08220945050319035\n",
      "Validation Loss for fold 5: 0.07638873532414436\n",
      "Validation Loss for fold 5: 0.07783543815215428\n",
      "Validation Loss for fold 5: 0.07840640594561894\n",
      "Validation Loss for fold 5: 0.0777897263566653\n",
      "Validation Loss for fold 5: 0.077726644774278\n",
      "Validation Loss for fold 5: 0.07234911248087883\n",
      "Validation Loss for fold 5: 0.07568263510862987\n",
      "Validation Loss for fold 5: 0.07748861610889435\n",
      "Validation Loss for fold 5: 0.073588361342748\n",
      "Validation Loss for fold 5: 0.07827505717674892\n",
      "Validation Loss for fold 5: 0.07551802943150203\n",
      "Validation Loss for fold 5: 0.0852466548482577\n",
      "Validation Loss for fold 5: 0.0800508384903272\n",
      "Validation Loss for fold 5: 0.08063843101263046\n",
      "Validation Loss for fold 5: 0.07471185425917308\n",
      "Validation Loss for fold 5: 0.0777963971098264\n",
      "Validation Loss for fold 5: 0.07143597553173701\n",
      "Validation Loss for fold 5: 0.07301131387551625\n",
      "Validation Loss for fold 5: 0.07071005801359813\n",
      "Validation Loss for fold 5: 0.07117152338226636\n",
      "Validation Loss for fold 5: 0.07726768776774406\n",
      "Validation Loss for fold 5: 0.07090248291691144\n",
      "Validation Loss for fold 5: 0.07122097661097844\n",
      "Validation Loss for fold 5: 0.0711393766105175\n",
      "Validation Loss for fold 5: 0.07743578404188156\n",
      "Validation Loss for fold 5: 0.06879682838916779\n",
      "Validation Loss for fold 5: 0.0766571934024493\n",
      "Validation Loss for fold 5: 0.08029912412166595\n",
      "Validation Loss for fold 5: 0.07041773200035095\n",
      "Validation Loss for fold 5: 0.07755278795957565\n",
      "Validation Loss for fold 5: 0.06796797861655553\n",
      "Validation Loss for fold 5: 0.07337009161710739\n",
      "Validation Loss for fold 5: 0.06775984540581703\n",
      "Validation Loss for fold 5: 0.0712725172440211\n",
      "Validation Loss for fold 5: 0.07567539562781651\n",
      "Validation Loss for fold 5: 0.07611174881458282\n",
      "Validation Loss for fold 5: 0.06949504216512044\n",
      "Validation Loss for fold 5: 0.0650840662419796\n",
      "Validation Loss for fold 5: 0.07792649666468303\n",
      "Validation Loss for fold 5: 0.07296256969372432\n",
      "Validation Loss for fold 5: 0.07642252867420514\n",
      "Validation Loss for fold 5: 0.073782945672671\n",
      "Validation Loss for fold 5: 0.06956670433282852\n",
      "Validation Loss for fold 5: 0.06797745451331139\n",
      "Validation Loss for fold 5: 0.07489064087470372\n",
      "Validation Loss for fold 5: 0.06910886367162068\n",
      "Validation Loss for fold 5: 0.0691645120580991\n",
      "Validation Loss for fold 5: 0.06455282618602116\n",
      "Validation Loss for fold 5: 0.06842979416251183\n",
      "Validation Loss for fold 5: 0.06348813697695732\n",
      "Validation Loss for fold 5: 0.0654589980840683\n",
      "Validation Loss for fold 5: 0.07273561383287112\n",
      "Validation Loss for fold 5: 0.06596042091647784\n",
      "Validation Loss for fold 5: 0.06981770445903142\n",
      "Validation Loss for fold 5: 0.07257429013649623\n",
      "Validation Loss for fold 5: 0.07232005894184113\n",
      "Validation Loss for fold 5: 0.07436726490656535\n",
      "Validation Loss for fold 5: 0.06809857239325841\n",
      "Validation Loss for fold 5: 0.06853156785170238\n",
      "Validation Loss for fold 5: 0.06855181852976482\n",
      "Validation Loss for fold 5: 0.07234310607115428\n",
      "Validation Loss for fold 5: 0.06982130308945973\n",
      "Validation Loss for fold 5: 0.06581396112839381\n",
      "Validation Loss for fold 5: 0.06944255282481511\n",
      "Validation Loss for fold 5: 0.07324231788516045\n",
      "Validation Loss for fold 5: 0.06324043621619542\n",
      "Validation Loss for fold 5: 0.07123960182070732\n",
      "Validation Loss for fold 5: 0.06035576264063517\n",
      "Validation Loss for fold 5: 0.06418626755475998\n",
      "Validation Loss for fold 5: 0.06859630967179935\n",
      "Validation Loss for fold 5: 0.06519374251365662\n",
      "Validation Loss for fold 5: 0.05979489659269651\n",
      "Validation Loss for fold 5: 0.06727641200025876\n",
      "Validation Loss for fold 5: 0.06205715363224348\n",
      "Validation Loss for fold 5: 0.06458233917752902\n",
      "Validation Loss for fold 5: 0.06300382812817891\n",
      "Validation Loss for fold 5: 0.06213258703549703\n",
      "Validation Loss for fold 5: 0.0680498480796814\n",
      "Validation Loss for fold 5: 0.06419863551855087\n",
      "Validation Loss for fold 5: 0.0631849505007267\n",
      "Validation Loss for fold 5: 0.0664571076631546\n",
      "Validation Loss for fold 5: 0.06649250288804372\n",
      "Validation Loss for fold 5: 0.06316396097342174\n",
      "Validation Loss for fold 5: 0.06137547641992569\n",
      "Validation Loss for fold 5: 0.06347412367661794\n",
      "Validation Loss for fold 5: 0.06918823719024658\n",
      "Validation Loss for fold 5: 0.06139247491955757\n",
      "Validation Loss for fold 5: 0.06421980261802673\n",
      "Validation Loss for fold 5: 0.06279532487193744\n",
      "Validation Loss for fold 5: 0.06669904415806134\n",
      "Validation Loss for fold 5: 0.0599371592203776\n",
      "Validation Loss for fold 5: 0.06270977109670639\n",
      "Validation Loss for fold 5: 0.06730536619822185\n",
      "Validation Loss for fold 5: 0.06320612256725629\n",
      "Validation Loss for fold 5: 0.06461434563000996\n",
      "Validation Loss for fold 5: 0.06039246544241905\n",
      "Validation Loss for fold 5: 0.06808519611756007\n",
      "Validation Loss for fold 5: 0.06909868245323499\n",
      "Validation Loss for fold 5: 0.06006158764163653\n",
      "Validation Loss for fold 5: 0.0659022902448972\n",
      "Validation Loss for fold 5: 0.0627164530257384\n",
      "Validation Loss for fold 5: 0.06569235026836395\n",
      "Validation Loss for fold 5: 0.06370711823304494\n",
      "Validation Loss for fold 5: 0.06251574928561847\n",
      "Validation Loss for fold 5: 0.06640696028868358\n",
      "Validation Loss for fold 5: 0.0602368600666523\n",
      "Validation Loss for fold 5: 0.06486245865623157\n",
      "Validation Loss for fold 5: 0.06448996067047119\n",
      "Validation Loss for fold 5: 0.06642195458213489\n",
      "Validation Loss for fold 5: 0.06634557122985522\n",
      "Validation Loss for fold 5: 0.06491859008868535\n",
      "Validation Loss for fold 5: 0.06013550112644831\n",
      "Validation Loss for fold 5: 0.06353775908549626\n",
      "Validation Loss for fold 5: 0.06498240679502487\n",
      "Validation Loss for fold 5: 0.06615603466828664\n",
      "Validation Loss for fold 5: 0.06574895729621251\n",
      "Validation Loss for fold 5: 0.0618033359448115\n",
      "Validation Loss for fold 5: 0.06214681143561999\n",
      "Validation Loss for fold 5: 0.06200361500183741\n",
      "Validation Loss for fold 5: 0.06313210229078929\n",
      "Validation Loss for fold 5: 0.06743928790092468\n",
      "Validation Loss for fold 5: 0.060512819637854896\n",
      "Validation Loss for fold 5: 0.06068118537465731\n",
      "Validation Loss for fold 5: 0.06273190428813298\n",
      "Validation Loss for fold 5: 0.06858744348088901\n",
      "Validation Loss for fold 5: 0.05863451212644577\n",
      "Validation Loss for fold 5: 0.06279349078734715\n",
      "Validation Loss for fold 5: 0.06066003690163294\n",
      "Validation Loss for fold 5: 0.06181960180401802\n",
      "Validation Loss for fold 5: 0.06025586649775505\n",
      "Validation Loss for fold 5: 0.061389906952778496\n",
      "Validation Loss for fold 5: 0.06146901225050291\n",
      "Validation Loss for fold 5: 0.062425425897041954\n",
      "Validation Loss for fold 5: 0.05985468253493309\n",
      "Validation Loss for fold 5: 0.06195368990302086\n",
      "Validation Loss for fold 5: 0.05429464392364025\n",
      "Validation Loss for fold 5: 0.061536580324172974\n",
      "Validation Loss for fold 5: 0.05930642659465472\n",
      "Validation Loss for fold 5: 0.06653338174025218\n",
      "Validation Loss for fold 5: 0.060623822112878166\n",
      "Validation Loss for fold 5: 0.0606529526412487\n",
      "Validation Loss for fold 5: 0.06799898048241933\n",
      "Validation Loss for fold 5: 0.06117696935931841\n",
      "Validation Loss for fold 5: 0.054829392582178116\n",
      "Validation Loss for fold 5: 0.0695085550347964\n",
      "Validation Loss for fold 5: 0.06063024575511614\n",
      "Validation Loss for fold 5: 0.06687748432159424\n",
      "Validation Loss for fold 5: 0.06277594342827797\n",
      "Validation Loss for fold 5: 0.060209975888331733\n",
      "Validation Loss for fold 5: 0.05943030367294947\n",
      "Validation Loss for fold 5: 0.06254062056541443\n",
      "Validation Loss for fold 5: 0.056712242464224495\n",
      "Validation Loss for fold 5: 0.05879308531681696\n",
      "Validation Loss for fold 5: 0.06040181964635849\n",
      "Validation Loss for fold 5: 0.06243038301666578\n",
      "Validation Loss for fold 5: 0.0649599867562453\n",
      "Validation Loss for fold 5: 0.05593064924081167\n",
      "Validation Loss for fold 5: 0.05948918933669726\n",
      "Validation Loss for fold 5: 0.06011816362539927\n",
      "Validation Loss for fold 5: 0.057305581867694855\n",
      "Validation Loss for fold 5: 0.06229093795021375\n",
      "Validation Loss for fold 5: 0.05540546774864197\n",
      "Validation Loss for fold 5: 0.0610777015487353\n",
      "Validation Loss for fold 5: 0.06072885046402613\n",
      "Validation Loss for fold 5: 0.059962425380945206\n",
      "Validation Loss for fold 5: 0.0648362785577774\n",
      "Validation Loss for fold 5: 0.05810812239845594\n",
      "Validation Loss for fold 5: 0.06311427180965741\n",
      "Validation Loss for fold 5: 0.061393791188796364\n",
      "Validation Loss for fold 5: 0.06272253766655922\n",
      "Validation Loss for fold 5: 0.05961706613500913\n",
      "Validation Loss for fold 5: 0.05657701939344406\n",
      "Validation Loss for fold 5: 0.056310673554738365\n",
      "Validation Loss for fold 5: 0.060535404831171036\n",
      "Validation Loss for fold 5: 0.05364698047439257\n",
      "Validation Loss for fold 5: 0.055314345906178154\n",
      "Validation Loss for fold 5: 0.058387755105892815\n",
      "Validation Loss for fold 5: 0.060838488241036735\n",
      "Validation Loss for fold 5: 0.056789430479208626\n",
      "Validation Loss for fold 5: 0.05603319654862086\n",
      "Validation Loss for fold 5: 0.058439026276270546\n",
      "Validation Loss for fold 5: 0.06058582291007042\n",
      "Validation Loss for fold 5: 0.06166000912586848\n",
      "Validation Loss for fold 5: 0.06710544601082802\n",
      "Validation Loss for fold 5: 0.06489459176858266\n",
      "Validation Loss for fold 5: 0.05962610120574633\n",
      "Validation Loss for fold 5: 0.05939310044050217\n",
      "Validation Loss for fold 5: 0.054244428873062134\n",
      "Validation Loss for fold 5: 0.05775385598341624\n",
      "Validation Loss for fold 5: 0.05929246048132578\n",
      "Validation Loss for fold 5: 0.059227182219425835\n",
      "Validation Loss for fold 5: 0.062285772214333214\n",
      "Validation Loss for fold 5: 0.05721372117598852\n",
      "Validation Loss for fold 5: 0.052095224459966026\n",
      "Validation Loss for fold 5: 0.057203260560830436\n",
      "Validation Loss for fold 5: 0.06302007660269737\n",
      "Validation Loss for fold 5: 0.05789580196142197\n",
      "Validation Loss for fold 5: 0.05793506527940432\n",
      "Validation Loss for fold 5: 0.05513979370395342\n",
      "--------------------------------\n",
      "FOLD 6\n",
      "--------------------------------\n",
      "Validation Loss for fold 6: 0.6495649615923563\n",
      "Validation Loss for fold 6: 0.4631864130496979\n",
      "Validation Loss for fold 6: 0.36798010269800824\n",
      "Validation Loss for fold 6: 0.31296659509340924\n",
      "Validation Loss for fold 6: 0.23475699126720428\n",
      "Validation Loss for fold 6: 0.20266054073969522\n",
      "Validation Loss for fold 6: 0.17962624629338583\n",
      "Validation Loss for fold 6: 0.17123481134573618\n",
      "Validation Loss for fold 6: 0.16708746552467346\n",
      "Validation Loss for fold 6: 0.14317290484905243\n",
      "Validation Loss for fold 6: 0.1386504645148913\n",
      "Validation Loss for fold 6: 0.14202212790648142\n",
      "Validation Loss for fold 6: 0.1363797833522161\n",
      "Validation Loss for fold 6: 0.12063317249218623\n",
      "Validation Loss for fold 6: 0.1281778266032537\n",
      "Validation Loss for fold 6: 0.12447007248799007\n",
      "Validation Loss for fold 6: 0.12359682718912761\n",
      "Validation Loss for fold 6: 0.11814124137163162\n",
      "Validation Loss for fold 6: 0.11035249878962834\n",
      "Validation Loss for fold 6: 0.12208177397648494\n",
      "Validation Loss for fold 6: 0.11547612895568211\n",
      "Validation Loss for fold 6: 0.12176771213610967\n",
      "Validation Loss for fold 6: 0.11960251877705257\n",
      "Validation Loss for fold 6: 0.10455426822106044\n",
      "Validation Loss for fold 6: 0.10772659132877986\n",
      "Validation Loss for fold 6: 0.10966953883568446\n",
      "Validation Loss for fold 6: 0.10341479380925496\n",
      "Validation Loss for fold 6: 0.10879026353359222\n",
      "Validation Loss for fold 6: 0.10709770768880844\n",
      "Validation Loss for fold 6: 0.10485779494047165\n",
      "Validation Loss for fold 6: 0.10603317866722743\n",
      "Validation Loss for fold 6: 0.11735906451940536\n",
      "Validation Loss for fold 6: 0.09218630194664001\n",
      "Validation Loss for fold 6: 0.12017337729533513\n",
      "Validation Loss for fold 6: 0.09478020171324412\n",
      "Validation Loss for fold 6: 0.09661785513162613\n",
      "Validation Loss for fold 6: 0.10259002695480983\n",
      "Validation Loss for fold 6: 0.10753192752599716\n",
      "Validation Loss for fold 6: 0.09923001378774643\n",
      "Validation Loss for fold 6: 0.09479888776938121\n",
      "Validation Loss for fold 6: 0.0979345366358757\n",
      "Validation Loss for fold 6: 0.09252030154069264\n",
      "Validation Loss for fold 6: 0.09464314331610997\n",
      "Validation Loss for fold 6: 0.08582688743869464\n",
      "Validation Loss for fold 6: 0.0919191117088\n",
      "Validation Loss for fold 6: 0.09369679788748424\n",
      "Validation Loss for fold 6: 0.09770405540863673\n",
      "Validation Loss for fold 6: 0.09201942880948384\n",
      "Validation Loss for fold 6: 0.0946732188264529\n",
      "Validation Loss for fold 6: 0.08885298669338226\n",
      "Validation Loss for fold 6: 0.08689596752325694\n",
      "Validation Loss for fold 6: 0.08092007289330165\n",
      "Validation Loss for fold 6: 0.08995630592107773\n",
      "Validation Loss for fold 6: 0.08467457070946693\n",
      "Validation Loss for fold 6: 0.08999297519524892\n",
      "Validation Loss for fold 6: 0.0951128030816714\n",
      "Validation Loss for fold 6: 0.08886583894491196\n",
      "Validation Loss for fold 6: 0.0897800475358963\n",
      "Validation Loss for fold 6: 0.08456430832544963\n",
      "Validation Loss for fold 6: 0.08541510005791982\n",
      "Validation Loss for fold 6: 0.08360088616609573\n",
      "Validation Loss for fold 6: 0.0804641122619311\n",
      "Validation Loss for fold 6: 0.09069042404492696\n",
      "Validation Loss for fold 6: 0.08477848519881566\n",
      "Validation Loss for fold 6: 0.08157393087943395\n",
      "Validation Loss for fold 6: 0.08677842219670613\n",
      "Validation Loss for fold 6: 0.09024267395337422\n",
      "Validation Loss for fold 6: 0.09008991469939549\n",
      "Validation Loss for fold 6: 0.08319310347239177\n",
      "Validation Loss for fold 6: 0.08357405662536621\n",
      "Validation Loss for fold 6: 0.08039935181538264\n",
      "Validation Loss for fold 6: 0.07863382250070572\n",
      "Validation Loss for fold 6: 0.07927922283609708\n",
      "Validation Loss for fold 6: 0.07879724353551865\n",
      "Validation Loss for fold 6: 0.08956953634818395\n",
      "Validation Loss for fold 6: 0.08367991199096043\n",
      "Validation Loss for fold 6: 0.08538221567869186\n",
      "Validation Loss for fold 6: 0.07880047212044398\n",
      "Validation Loss for fold 6: 0.07743265479803085\n",
      "Validation Loss for fold 6: 0.07805753747622173\n",
      "Validation Loss for fold 6: 0.07688014954328537\n",
      "Validation Loss for fold 6: 0.07401624694466591\n",
      "Validation Loss for fold 6: 0.07484680786728859\n",
      "Validation Loss for fold 6: 0.08584704498449962\n",
      "Validation Loss for fold 6: 0.08185663322607677\n",
      "Validation Loss for fold 6: 0.0844270686308543\n",
      "Validation Loss for fold 6: 0.08271776139736176\n",
      "Validation Loss for fold 6: 0.07019241278370221\n",
      "Validation Loss for fold 6: 0.07778771221637726\n",
      "Validation Loss for fold 6: 0.07188134888807933\n",
      "Validation Loss for fold 6: 0.07391836742560069\n",
      "Validation Loss for fold 6: 0.07739487787087758\n",
      "Validation Loss for fold 6: 0.0713368294139703\n",
      "Validation Loss for fold 6: 0.07980911433696747\n",
      "Validation Loss for fold 6: 0.07550373176733653\n",
      "Validation Loss for fold 6: 0.0751198058327039\n",
      "Validation Loss for fold 6: 0.0730689416329066\n",
      "Validation Loss for fold 6: 0.07791663457949956\n",
      "Validation Loss for fold 6: 0.06756226842602094\n",
      "Validation Loss for fold 6: 0.06788790598511696\n",
      "Validation Loss for fold 6: 0.07248898098866145\n",
      "Validation Loss for fold 6: 0.06811755150556564\n",
      "Validation Loss for fold 6: 0.0730446328719457\n",
      "Validation Loss for fold 6: 0.06459280227621396\n",
      "Validation Loss for fold 6: 0.06988206878304482\n",
      "Validation Loss for fold 6: 0.0761651632686456\n",
      "Validation Loss for fold 6: 0.07454014321168263\n",
      "Validation Loss for fold 6: 0.06209953625996908\n",
      "Validation Loss for fold 6: 0.07322344928979874\n",
      "Validation Loss for fold 6: 0.07400544236103694\n",
      "Validation Loss for fold 6: 0.07152434686819713\n",
      "Validation Loss for fold 6: 0.06800536190470059\n",
      "Validation Loss for fold 6: 0.07629696031411488\n",
      "Validation Loss for fold 6: 0.07033748676379521\n",
      "Validation Loss for fold 6: 0.06858620544274648\n",
      "Validation Loss for fold 6: 0.06562437862157822\n",
      "Validation Loss for fold 6: 0.06093879913290342\n",
      "Validation Loss for fold 6: 0.06538292889793713\n",
      "Validation Loss for fold 6: 0.07167535771926244\n",
      "Validation Loss for fold 6: 0.06593447923660278\n",
      "Validation Loss for fold 6: 0.07142927994330724\n",
      "Validation Loss for fold 6: 0.07391000042359035\n",
      "Validation Loss for fold 6: 0.06007102380196253\n",
      "Validation Loss for fold 6: 0.06375414381424586\n",
      "Validation Loss for fold 6: 0.06666891897718112\n",
      "Validation Loss for fold 6: 0.06856055061022441\n",
      "Validation Loss for fold 6: 0.06863463297486305\n",
      "Validation Loss for fold 6: 0.0658574178814888\n",
      "Validation Loss for fold 6: 0.06765028089284897\n",
      "Validation Loss for fold 6: 0.06469910343488057\n",
      "Validation Loss for fold 6: 0.06205753609538078\n",
      "Validation Loss for fold 6: 0.06977185606956482\n",
      "Validation Loss for fold 6: 0.07052374134461085\n",
      "Validation Loss for fold 6: 0.05997242281834284\n",
      "Validation Loss for fold 6: 0.06346234679222107\n",
      "Validation Loss for fold 6: 0.06281887615720431\n",
      "Validation Loss for fold 6: 0.06080972651640574\n",
      "Validation Loss for fold 6: 0.06901044150193532\n",
      "Validation Loss for fold 6: 0.07236454635858536\n",
      "Validation Loss for fold 6: 0.06222748011350632\n",
      "Validation Loss for fold 6: 0.06604068726301193\n",
      "Validation Loss for fold 6: 0.06234519431988398\n",
      "Validation Loss for fold 6: 0.06525682657957077\n",
      "Validation Loss for fold 6: 0.061485217263301216\n",
      "Validation Loss for fold 6: 0.06113119547565778\n",
      "Validation Loss for fold 6: 0.06415775666634242\n",
      "Validation Loss for fold 6: 0.060797158628702164\n",
      "Validation Loss for fold 6: 0.06491812815268834\n",
      "Validation Loss for fold 6: 0.05686731884876887\n",
      "Validation Loss for fold 6: 0.06084690367182096\n",
      "Validation Loss for fold 6: 0.06466226900617282\n",
      "Validation Loss for fold 6: 0.06409223253528278\n",
      "Validation Loss for fold 6: 0.06949748223026593\n",
      "Validation Loss for fold 6: 0.05646290133396784\n",
      "Validation Loss for fold 6: 0.06407115856806438\n",
      "Validation Loss for fold 6: 0.062097166975339256\n",
      "Validation Loss for fold 6: 0.06258170555035274\n",
      "Validation Loss for fold 6: 0.05924633393685023\n",
      "Validation Loss for fold 6: 0.05978866790731748\n",
      "Validation Loss for fold 6: 0.06367641935745876\n",
      "Validation Loss for fold 6: 0.06017246345678965\n",
      "Validation Loss for fold 6: 0.06193957229455312\n",
      "Validation Loss for fold 6: 0.06313018252452214\n",
      "Validation Loss for fold 6: 0.0652179444829623\n",
      "Validation Loss for fold 6: 0.059655038019021354\n",
      "Validation Loss for fold 6: 0.05751613030831019\n",
      "Validation Loss for fold 6: 0.05928324287136396\n",
      "Validation Loss for fold 6: 0.05896939585606257\n",
      "Validation Loss for fold 6: 0.05871098240216573\n",
      "Validation Loss for fold 6: 0.05705447494983673\n",
      "Validation Loss for fold 6: 0.06206437200307846\n",
      "Validation Loss for fold 6: 0.05789151042699814\n",
      "Validation Loss for fold 6: 0.06434738636016846\n",
      "Validation Loss for fold 6: 0.0597720667719841\n",
      "Validation Loss for fold 6: 0.05813046793142954\n",
      "Validation Loss for fold 6: 0.05685259401798248\n",
      "Validation Loss for fold 6: 0.06294904525081317\n",
      "Validation Loss for fold 6: 0.05152896667520205\n",
      "Validation Loss for fold 6: 0.06118712077538172\n",
      "Validation Loss for fold 6: 0.05919664974013964\n",
      "Validation Loss for fold 6: 0.06117971862355868\n",
      "Validation Loss for fold 6: 0.05896564076344172\n",
      "Validation Loss for fold 6: 0.06185393532117208\n",
      "Validation Loss for fold 6: 0.05974463497598966\n",
      "Validation Loss for fold 6: 0.05884713431199392\n",
      "Validation Loss for fold 6: 0.05814756453037262\n",
      "Validation Loss for fold 6: 0.05141501811643442\n",
      "Validation Loss for fold 6: 0.05598882709940275\n",
      "Validation Loss for fold 6: 0.05849497144420942\n",
      "Validation Loss for fold 6: 0.056873682886362076\n",
      "Validation Loss for fold 6: 0.055862108866373696\n",
      "Validation Loss for fold 6: 0.056616115073362984\n",
      "Validation Loss for fold 6: 0.05919551104307175\n",
      "Validation Loss for fold 6: 0.05490009735027949\n",
      "Validation Loss for fold 6: 0.055329843113819756\n",
      "Validation Loss for fold 6: 0.057406278947989144\n",
      "Validation Loss for fold 6: 0.06362472847104073\n",
      "Validation Loss for fold 6: 0.055871800829966865\n",
      "Validation Loss for fold 6: 0.05951912701129913\n",
      "Validation Loss for fold 6: 0.05652873093883196\n",
      "Validation Loss for fold 6: 0.05812264730532964\n",
      "Validation Loss for fold 6: 0.057805305967728295\n",
      "Validation Loss for fold 6: 0.05537443359692892\n",
      "Validation Loss for fold 6: 0.057714659720659256\n",
      "Validation Loss for fold 6: 0.05609652648369471\n",
      "Validation Loss for fold 6: 0.053010636319716774\n",
      "Validation Loss for fold 6: 0.05491745099425316\n",
      "Validation Loss for fold 6: 0.057646363973617554\n",
      "Validation Loss for fold 6: 0.05753037457664808\n",
      "Validation Loss for fold 6: 0.05432617664337158\n",
      "Validation Loss for fold 6: 0.05670995761950811\n",
      "Validation Loss for fold 6: 0.06304169322053592\n",
      "Validation Loss for fold 6: 0.055659731229146324\n",
      "Validation Loss for fold 6: 0.05443114290634791\n",
      "Validation Loss for fold 6: 0.059178292751312256\n",
      "Validation Loss for fold 6: 0.053985994309186935\n",
      "Validation Loss for fold 6: 0.05891163647174835\n",
      "Validation Loss for fold 6: 0.0539467620352904\n",
      "Validation Loss for fold 6: 0.05185511087377866\n",
      "Validation Loss for fold 6: 0.057433780282735825\n",
      "Validation Loss for fold 6: 0.0553793932000796\n",
      "Validation Loss for fold 6: 0.054982672135035195\n",
      "Validation Loss for fold 6: 0.05286173646648725\n",
      "Validation Loss for fold 6: 0.054732002317905426\n",
      "Validation Loss for fold 6: 0.057230159640312195\n",
      "Validation Loss for fold 6: 0.052196880181630455\n",
      "Validation Loss for fold 6: 0.05439121648669243\n",
      "Validation Loss for fold 6: 0.05176592245697975\n",
      "Validation Loss for fold 6: 0.052952722956736885\n",
      "Validation Loss for fold 6: 0.05550835654139519\n",
      "Validation Loss for fold 6: 0.05742286269863447\n",
      "Validation Loss for fold 6: 0.0599872370560964\n",
      "Validation Loss for fold 6: 0.05551824842890104\n",
      "Validation Loss for fold 6: 0.05758825068672498\n",
      "Validation Loss for fold 6: 0.058238825450340904\n",
      "Validation Loss for fold 6: 0.05527867252628008\n",
      "Validation Loss for fold 6: 0.05786521608630816\n",
      "Validation Loss for fold 6: 0.060229105254014335\n",
      "Validation Loss for fold 6: 0.051989092181126274\n",
      "Validation Loss for fold 6: 0.05629421646396319\n",
      "Validation Loss for fold 6: 0.05434080461661021\n",
      "Validation Loss for fold 6: 0.056023272375265755\n",
      "Validation Loss for fold 6: 0.05304388081034025\n",
      "Validation Loss for fold 6: 0.051004103074471153\n",
      "Validation Loss for fold 6: 0.057566072791814804\n",
      "Validation Loss for fold 6: 0.05219245950380961\n",
      "Validation Loss for fold 6: 0.05055983612934748\n",
      "Validation Loss for fold 6: 0.05680540452400843\n",
      "Validation Loss for fold 6: 0.053101220478614174\n",
      "Validation Loss for fold 6: 0.058422221491734184\n",
      "Validation Loss for fold 6: 0.05600624283154806\n",
      "Validation Loss for fold 6: 0.05102187519272169\n",
      "Validation Loss for fold 6: 0.05144175514578819\n",
      "Validation Loss for fold 6: 0.05213082581758499\n",
      "Validation Loss for fold 6: 0.05233198528488477\n",
      "Validation Loss for fold 6: 0.056423868983983994\n",
      "Validation Loss for fold 6: 0.05107764030496279\n",
      "Validation Loss for fold 6: 0.04695116480191549\n",
      "Validation Loss for fold 6: 0.051919968177874885\n",
      "Validation Loss for fold 6: 0.05330436552564303\n",
      "Validation Loss for fold 6: 0.04833132897814115\n",
      "Validation Loss for fold 6: 0.05247993394732475\n",
      "Validation Loss for fold 6: 0.057151795675357185\n",
      "Validation Loss for fold 6: 0.053607530891895294\n",
      "Validation Loss for fold 6: 0.05189977089564005\n",
      "Validation Loss for fold 6: 0.05363523215055466\n",
      "Validation Loss for fold 6: 0.059147427479426064\n",
      "Validation Loss for fold 6: 0.05546197295188904\n",
      "Validation Loss for fold 6: 0.05102107177178065\n",
      "Validation Loss for fold 6: 0.052893550445636116\n",
      "Validation Loss for fold 6: 0.05162729819615682\n",
      "Validation Loss for fold 6: 0.0465016762415568\n",
      "Validation Loss for fold 6: 0.052334426591793694\n",
      "Validation Loss for fold 6: 0.05666390433907509\n",
      "Validation Loss for fold 6: 0.04917039598027865\n",
      "Validation Loss for fold 6: 0.047717360158761345\n",
      "Validation Loss for fold 6: 0.04745540829996268\n",
      "Validation Loss for fold 6: 0.05548899993300438\n",
      "Validation Loss for fold 6: 0.05049770697951317\n",
      "Validation Loss for fold 6: 0.05617273971438408\n",
      "Validation Loss for fold 6: 0.04971929391225179\n",
      "Validation Loss for fold 6: 0.05351745585600535\n",
      "Validation Loss for fold 6: 0.05058016503850619\n",
      "Validation Loss for fold 6: 0.05460529401898384\n",
      "Validation Loss for fold 6: 0.05035755162437757\n",
      "Validation Loss for fold 6: 0.05056546131769816\n",
      "Validation Loss for fold 6: 0.0526138519247373\n",
      "Validation Loss for fold 6: 0.05155997847517332\n",
      "Validation Loss for fold 6: 0.05245686570803324\n",
      "Validation Loss for fold 6: 0.0540735957523187\n",
      "Validation Loss for fold 6: 0.05526005104184151\n",
      "Validation Loss for fold 6: 0.04965916772683462\n",
      "Validation Loss for fold 6: 0.04969246437152227\n",
      "Validation Loss for fold 6: 0.0522198478380839\n",
      "Validation Loss for fold 6: 0.04841875905791918\n",
      "Validation Loss for fold 6: 0.05280615637699763\n",
      "Validation Loss for fold 6: 0.05259711047013601\n",
      "Validation Loss for fold 6: 0.051957747588555016\n",
      "Validation Loss for fold 6: 0.050621144473552704\n",
      "Validation Loss for fold 6: 0.05112748717268308\n",
      "--------------------------------\n",
      "FOLD 7\n",
      "--------------------------------\n",
      "Validation Loss for fold 7: 0.4782010118166606\n",
      "Validation Loss for fold 7: 0.29970674713452655\n",
      "Validation Loss for fold 7: 0.21086967984835306\n",
      "Validation Loss for fold 7: 0.13691658526659012\n",
      "Validation Loss for fold 7: 0.1193490872780482\n",
      "Validation Loss for fold 7: 0.1229605774084727\n",
      "Validation Loss for fold 7: 0.09862510611613591\n",
      "Validation Loss for fold 7: 0.11170828839143117\n",
      "Validation Loss for fold 7: 0.07932945340871811\n",
      "Validation Loss for fold 7: 0.0722862146794796\n",
      "Validation Loss for fold 7: 0.07942754775285721\n",
      "Validation Loss for fold 7: 0.07504113266865413\n",
      "Validation Loss for fold 7: 0.0765219156940778\n",
      "Validation Loss for fold 7: 0.06622118875384331\n",
      "Validation Loss for fold 7: 0.07243431111176808\n",
      "Validation Loss for fold 7: 0.06487969681620598\n",
      "Validation Loss for fold 7: 0.06302513306339581\n",
      "Validation Loss for fold 7: 0.05944666638970375\n",
      "Validation Loss for fold 7: 0.06077757974465688\n",
      "Validation Loss for fold 7: 0.06275585542122523\n",
      "Validation Loss for fold 7: 0.06352761139472325\n",
      "Validation Loss for fold 7: 0.06405055895447731\n",
      "Validation Loss for fold 7: 0.0682328740755717\n",
      "Validation Loss for fold 7: 0.06676660850644112\n",
      "Validation Loss for fold 7: 0.061519348373015724\n",
      "Validation Loss for fold 7: 0.05346611700952053\n",
      "Validation Loss for fold 7: 0.05565849567453066\n",
      "Validation Loss for fold 7: 0.062471034626166023\n",
      "Validation Loss for fold 7: 0.055322730292876564\n",
      "Validation Loss for fold 7: 0.06117014090220133\n",
      "Validation Loss for fold 7: 0.06917821367581685\n",
      "Validation Loss for fold 7: 0.06001607080300649\n",
      "Validation Loss for fold 7: 0.061198445657889046\n",
      "Validation Loss for fold 7: 0.05552913248538971\n",
      "Validation Loss for fold 7: 0.057719664027293525\n",
      "Validation Loss for fold 7: 0.05668304239710172\n",
      "Validation Loss for fold 7: 0.05390127748250961\n",
      "Validation Loss for fold 7: 0.058915574103593826\n",
      "Validation Loss for fold 7: 0.054259871443112694\n",
      "Validation Loss for fold 7: 0.052709225565195084\n",
      "Validation Loss for fold 7: 0.05105313348273436\n",
      "Validation Loss for fold 7: 0.058803170919418335\n",
      "Validation Loss for fold 7: 0.05879561478892962\n",
      "Validation Loss for fold 7: 0.05421384796500206\n",
      "Validation Loss for fold 7: 0.05541566510995229\n",
      "Validation Loss for fold 7: 0.05601928383111954\n",
      "Validation Loss for fold 7: 0.05483114222685496\n",
      "Validation Loss for fold 7: 0.054219825814167656\n",
      "Validation Loss for fold 7: 0.05853814507524172\n",
      "Validation Loss for fold 7: 0.056995066503683724\n",
      "Validation Loss for fold 7: 0.05533722663919131\n",
      "Validation Loss for fold 7: 0.052006964882214866\n",
      "Validation Loss for fold 7: 0.05904066562652588\n",
      "Validation Loss for fold 7: 0.060517728328704834\n",
      "Validation Loss for fold 7: 0.06426080192128818\n",
      "Validation Loss for fold 7: 0.05109183490276337\n",
      "Validation Loss for fold 7: 0.05164142698049545\n",
      "Validation Loss for fold 7: 0.05264112974206606\n",
      "Validation Loss for fold 7: 0.05106498301029205\n",
      "Validation Loss for fold 7: 0.0634919951359431\n",
      "Validation Loss for fold 7: 0.056539672116438545\n",
      "Validation Loss for fold 7: 0.05536701033512751\n",
      "Validation Loss for fold 7: 0.0513872466981411\n",
      "Validation Loss for fold 7: 0.04964045559366544\n",
      "Validation Loss for fold 7: 0.05727464954058329\n",
      "Validation Loss for fold 7: 0.051047101616859436\n",
      "Validation Loss for fold 7: 0.060631523529688515\n",
      "Validation Loss for fold 7: 0.057077933102846146\n",
      "Validation Loss for fold 7: 0.05139266947905222\n",
      "Validation Loss for fold 7: 0.05415208265185356\n",
      "Validation Loss for fold 7: 0.050843523194392524\n",
      "Validation Loss for fold 7: 0.05378859117627144\n",
      "Validation Loss for fold 7: 0.051137104630470276\n",
      "Validation Loss for fold 7: 0.05413951352238655\n",
      "Validation Loss for fold 7: 0.052794856329758964\n",
      "Validation Loss for fold 7: 0.050808666894833245\n",
      "Validation Loss for fold 7: 0.050465224931637444\n",
      "Validation Loss for fold 7: 0.0613505095243454\n",
      "Validation Loss for fold 7: 0.05448925867676735\n",
      "Validation Loss for fold 7: 0.052428241819143295\n",
      "Validation Loss for fold 7: 0.05790221691131592\n",
      "Validation Loss for fold 7: 0.05202114333709081\n",
      "Validation Loss for fold 7: 0.051140633722146354\n",
      "Validation Loss for fold 7: 0.05387906233469645\n",
      "Validation Loss for fold 7: 0.049079228192567825\n",
      "Validation Loss for fold 7: 0.04932217548290888\n",
      "Validation Loss for fold 7: 0.052417619774738945\n",
      "Validation Loss for fold 7: 0.049905442943175636\n",
      "Validation Loss for fold 7: 0.04868000124891599\n",
      "Validation Loss for fold 7: 0.055120378732681274\n",
      "Validation Loss for fold 7: 0.046064923206965126\n",
      "Validation Loss for fold 7: 0.043513454496860504\n",
      "Validation Loss for fold 7: 0.05291302874684334\n",
      "Validation Loss for fold 7: 0.0531891422967116\n",
      "Validation Loss for fold 7: 0.05810198187828064\n",
      "Validation Loss for fold 7: 0.051175872484842934\n",
      "Validation Loss for fold 7: 0.04665670543909073\n",
      "Validation Loss for fold 7: 0.05315614864230156\n",
      "Validation Loss for fold 7: 0.049964793026447296\n",
      "Validation Loss for fold 7: 0.04950374364852905\n",
      "Validation Loss for fold 7: 0.0478101447224617\n",
      "Validation Loss for fold 7: 0.043967099860310555\n",
      "Validation Loss for fold 7: 0.053733142713705696\n",
      "Validation Loss for fold 7: 0.05777357270320257\n",
      "Validation Loss for fold 7: 0.04505658460160097\n",
      "Validation Loss for fold 7: 0.05120822538932165\n",
      "Validation Loss for fold 7: 0.05381660660107931\n",
      "Validation Loss for fold 7: 0.0487497424085935\n",
      "Validation Loss for fold 7: 0.048921833435694374\n",
      "Validation Loss for fold 7: 0.05373393495877584\n",
      "Validation Loss for fold 7: 0.053352115054925285\n",
      "Validation Loss for fold 7: 0.04815558592478434\n",
      "Validation Loss for fold 7: 0.047094409664471946\n",
      "Validation Loss for fold 7: 0.05339220414559046\n",
      "Validation Loss for fold 7: 0.055875957012176514\n",
      "Validation Loss for fold 7: 0.04819264014561971\n",
      "Validation Loss for fold 7: 0.05183864384889603\n",
      "Validation Loss for fold 7: 0.05127838750680288\n",
      "Validation Loss for fold 7: 0.05400146419803301\n",
      "Validation Loss for fold 7: 0.048395062486330666\n",
      "Validation Loss for fold 7: 0.04596666867534319\n",
      "Validation Loss for fold 7: 0.04926101739207903\n",
      "Validation Loss for fold 7: 0.053137004375457764\n",
      "Validation Loss for fold 7: 0.05755157147844633\n",
      "Validation Loss for fold 7: 0.05305051679412524\n",
      "Validation Loss for fold 7: 0.04965717097123464\n",
      "Validation Loss for fold 7: 0.05027796203891436\n",
      "Validation Loss for fold 7: 0.05225124582648277\n",
      "Validation Loss for fold 7: 0.044940476616223655\n",
      "Validation Loss for fold 7: 0.051917615036169686\n",
      "Validation Loss for fold 7: 0.04827527577678362\n",
      "Validation Loss for fold 7: 0.04341420034567515\n",
      "Validation Loss for fold 7: 0.048307410130898155\n",
      "Validation Loss for fold 7: 0.0439507265885671\n",
      "Validation Loss for fold 7: 0.049186901499827705\n",
      "Validation Loss for fold 7: 0.04571286713083585\n",
      "Validation Loss for fold 7: 0.0539503258963426\n",
      "Validation Loss for fold 7: 0.04530972242355347\n",
      "Validation Loss for fold 7: 0.050478529185056686\n",
      "Validation Loss for fold 7: 0.04757645602027575\n",
      "Validation Loss for fold 7: 0.04782067363460859\n",
      "Validation Loss for fold 7: 0.04766420150796572\n",
      "Validation Loss for fold 7: 0.05002559473117193\n",
      "Validation Loss for fold 7: 0.04954128464063009\n",
      "Validation Loss for fold 7: 0.057039683063824974\n",
      "Validation Loss for fold 7: 0.05102933198213577\n",
      "Validation Loss for fold 7: 0.04301703286667665\n",
      "Validation Loss for fold 7: 0.04522705326477686\n",
      "Validation Loss for fold 7: 0.04448231930534045\n",
      "Validation Loss for fold 7: 0.04912551368276278\n",
      "Validation Loss for fold 7: 0.04946328202883402\n",
      "Validation Loss for fold 7: 0.04739945257703463\n",
      "Validation Loss for fold 7: 0.05013297746578852\n",
      "Validation Loss for fold 7: 0.04985085129737854\n",
      "Validation Loss for fold 7: 0.05065040166179339\n",
      "Validation Loss for fold 7: 0.04259744348625342\n",
      "Validation Loss for fold 7: 0.05274227261543274\n",
      "Validation Loss for fold 7: 0.044037213549017906\n",
      "Validation Loss for fold 7: 0.04598987971742948\n",
      "Validation Loss for fold 7: 0.04505660509069761\n",
      "Validation Loss for fold 7: 0.04510203873117765\n",
      "Validation Loss for fold 7: 0.05071903516848882\n",
      "Validation Loss for fold 7: 0.04677153751254082\n",
      "Validation Loss for fold 7: 0.050433860470851265\n",
      "Validation Loss for fold 7: 0.05034661913911501\n",
      "Validation Loss for fold 7: 0.05013748009999593\n",
      "Validation Loss for fold 7: 0.04879650101065636\n",
      "Validation Loss for fold 7: 0.04741427550713221\n",
      "Validation Loss for fold 7: 0.04442798967162768\n",
      "Validation Loss for fold 7: 0.04898178204894066\n",
      "Validation Loss for fold 7: 0.04302435554563999\n",
      "Validation Loss for fold 7: 0.04577650502324104\n",
      "Validation Loss for fold 7: 0.049781190852324166\n",
      "Validation Loss for fold 7: 0.05016173794865608\n",
      "Validation Loss for fold 7: 0.04908817385633787\n",
      "Validation Loss for fold 7: 0.04817722241083781\n",
      "Validation Loss for fold 7: 0.04344472847878933\n",
      "Validation Loss for fold 7: 0.04845115914940834\n",
      "Validation Loss for fold 7: 0.04362360450128714\n",
      "Validation Loss for fold 7: 0.04914527138074239\n",
      "Validation Loss for fold 7: 0.04630416134993235\n",
      "Validation Loss for fold 7: 0.048056056102116905\n",
      "Validation Loss for fold 7: 0.04580459122856458\n",
      "Validation Loss for fold 7: 0.04984994108478228\n",
      "Validation Loss for fold 7: 0.051228913168112435\n",
      "Validation Loss for fold 7: 0.05284443497657776\n",
      "Validation Loss for fold 7: 0.04807441060741743\n",
      "Validation Loss for fold 7: 0.04653714473048846\n",
      "Validation Loss for fold 7: 0.04319932498037815\n",
      "Validation Loss for fold 7: 0.04458528384566307\n",
      "Validation Loss for fold 7: 0.04288249214490255\n",
      "Validation Loss for fold 7: 0.04832934960722923\n",
      "Validation Loss for fold 7: 0.045735922952493034\n",
      "Validation Loss for fold 7: 0.040253158658742905\n",
      "Validation Loss for fold 7: 0.0476229153573513\n",
      "Validation Loss for fold 7: 0.04383214314778646\n",
      "Validation Loss for fold 7: 0.04654102772474289\n",
      "Validation Loss for fold 7: 0.051541956762472786\n",
      "Validation Loss for fold 7: 0.044481140871842705\n",
      "Validation Loss for fold 7: 0.045708443969488144\n",
      "Validation Loss for fold 7: 0.042998469124237694\n",
      "Validation Loss for fold 7: 0.04856663445631663\n",
      "Validation Loss for fold 7: 0.04684704293807348\n",
      "Validation Loss for fold 7: 0.04848733295996984\n",
      "Validation Loss for fold 7: 0.046360290298859276\n",
      "Validation Loss for fold 7: 0.050483950724204384\n",
      "Validation Loss for fold 7: 0.040961396570007004\n",
      "Validation Loss for fold 7: 0.048300836235284805\n",
      "Validation Loss for fold 7: 0.05290249859293302\n",
      "Validation Loss for fold 7: 0.04961196705698967\n",
      "Validation Loss for fold 7: 0.046412511418263115\n",
      "Validation Loss for fold 7: 0.04760262990991274\n",
      "Validation Loss for fold 7: 0.05512781317035357\n",
      "Validation Loss for fold 7: 0.04805037875970205\n",
      "Validation Loss for fold 7: 0.052270436038573585\n",
      "Validation Loss for fold 7: 0.04928206652402878\n",
      "Validation Loss for fold 7: 0.044821858406066895\n",
      "Validation Loss for fold 7: 0.049790250758330025\n",
      "Validation Loss for fold 7: 0.047936749955018364\n",
      "Validation Loss for fold 7: 0.05190731460849444\n",
      "Validation Loss for fold 7: 0.04415596276521683\n",
      "Validation Loss for fold 7: 0.04820121203859647\n",
      "Validation Loss for fold 7: 0.045710371186335884\n",
      "Validation Loss for fold 7: 0.04377983137965202\n",
      "Validation Loss for fold 7: 0.04456417759259542\n",
      "Validation Loss for fold 7: 0.049338217824697495\n",
      "Validation Loss for fold 7: 0.04794234658281008\n",
      "Validation Loss for fold 7: 0.04728292425473531\n",
      "Validation Loss for fold 7: 0.04366420085231463\n",
      "Validation Loss for fold 7: 0.05193287879228592\n",
      "Validation Loss for fold 7: 0.05024094507098198\n",
      "Validation Loss for fold 7: 0.04191673484941324\n",
      "Validation Loss for fold 7: 0.04627379278341929\n",
      "Validation Loss for fold 7: 0.048479306201140084\n",
      "Validation Loss for fold 7: 0.04651183510820071\n",
      "Validation Loss for fold 7: 0.054007074485222496\n",
      "Validation Loss for fold 7: 0.04651451110839844\n",
      "Validation Loss for fold 7: 0.04931535944342613\n",
      "Validation Loss for fold 7: 0.045946634064118065\n",
      "Validation Loss for fold 7: 0.04356170321504275\n",
      "Validation Loss for fold 7: 0.04571810985604922\n",
      "Validation Loss for fold 7: 0.04372028509775797\n",
      "Validation Loss for fold 7: 0.04704562077919642\n",
      "Validation Loss for fold 7: 0.043611244608958565\n",
      "Validation Loss for fold 7: 0.045264756927887596\n",
      "Validation Loss for fold 7: 0.04892989372213682\n",
      "Validation Loss for fold 7: 0.042577351133028664\n",
      "Validation Loss for fold 7: 0.042694096763928734\n",
      "Validation Loss for fold 7: 0.0455732469757398\n",
      "Validation Loss for fold 7: 0.0485360249876976\n",
      "Validation Loss for fold 7: 0.040226154029369354\n",
      "Validation Loss for fold 7: 0.04324496661623319\n",
      "Validation Loss for fold 7: 0.045092845956484474\n",
      "Validation Loss for fold 7: 0.04365866631269455\n",
      "Validation Loss for fold 7: 0.04782382150491079\n",
      "Validation Loss for fold 7: 0.047211967408657074\n",
      "Validation Loss for fold 7: 0.04833245029052099\n",
      "Validation Loss for fold 7: 0.046547929445902504\n",
      "Validation Loss for fold 7: 0.046916086226701736\n",
      "Validation Loss for fold 7: 0.04444398358464241\n",
      "Validation Loss for fold 7: 0.05330428356925646\n",
      "Validation Loss for fold 7: 0.04471505433320999\n",
      "Validation Loss for fold 7: 0.04140857607126236\n",
      "Validation Loss for fold 7: 0.05013184746106466\n",
      "Validation Loss for fold 7: 0.04723295941948891\n",
      "Validation Loss for fold 7: 0.04434321572383245\n",
      "Validation Loss for fold 7: 0.04293219745159149\n",
      "Validation Loss for fold 7: 0.04584948097666105\n",
      "Validation Loss for fold 7: 0.04604293157656988\n",
      "Validation Loss for fold 7: 0.04472615197300911\n",
      "Validation Loss for fold 7: 0.044502957413593926\n",
      "Validation Loss for fold 7: 0.04953524221976598\n",
      "Validation Loss for fold 7: 0.04233247103790442\n",
      "Validation Loss for fold 7: 0.0483719768623511\n",
      "Validation Loss for fold 7: 0.05061814064780871\n",
      "Validation Loss for fold 7: 0.04350173597534498\n",
      "Validation Loss for fold 7: 0.04618215188384056\n",
      "Validation Loss for fold 7: 0.04667715479930242\n",
      "Validation Loss for fold 7: 0.04834996288021406\n",
      "Validation Loss for fold 7: 0.04567178711295128\n",
      "Validation Loss for fold 7: 0.04471290732423464\n",
      "Validation Loss for fold 7: 0.046019369115432106\n",
      "Validation Loss for fold 7: 0.043832264840602875\n",
      "Validation Loss for fold 7: 0.04420949767033259\n",
      "Validation Loss for fold 7: 0.04582409809033076\n",
      "Validation Loss for fold 7: 0.046716113885243736\n",
      "Validation Loss for fold 7: 0.04271118839581808\n",
      "Validation Loss for fold 7: 0.04673318689068159\n",
      "Validation Loss for fold 7: 0.045731596648693085\n",
      "Validation Loss for fold 7: 0.04321923231085142\n",
      "Validation Loss for fold 7: 0.05162874981760979\n",
      "Validation Loss for fold 7: 0.04828208933273951\n",
      "Validation Loss for fold 7: 0.04773841674129168\n",
      "Validation Loss for fold 7: 0.04711549977461497\n",
      "Validation Loss for fold 7: 0.04187173520525297\n",
      "Validation Loss for fold 7: 0.04405682782332102\n",
      "Validation Loss for fold 7: 0.04654857764641444\n",
      "Validation Loss for fold 7: 0.048050512870152794\n",
      "Validation Loss for fold 7: 0.05143299077947935\n",
      "Validation Loss for fold 7: 0.04747596879800161\n",
      "--------------------------------\n",
      "FOLD 8\n",
      "--------------------------------\n",
      "Validation Loss for fold 8: 0.3623252014319102\n",
      "Validation Loss for fold 8: 0.3049717843532562\n",
      "Validation Loss for fold 8: 0.2528214355309804\n",
      "Validation Loss for fold 8: 0.2344642480214437\n",
      "Validation Loss for fold 8: 0.17671528458595276\n",
      "Validation Loss for fold 8: 0.17108420530954996\n",
      "Validation Loss for fold 8: 0.17142589638630548\n",
      "Validation Loss for fold 8: 0.13282560805479685\n",
      "Validation Loss for fold 8: 0.14457967877388\n",
      "Validation Loss for fold 8: 0.13580178717772165\n",
      "Validation Loss for fold 8: 0.12045127153396606\n",
      "Validation Loss for fold 8: 0.11983653157949448\n",
      "Validation Loss for fold 8: 0.11889607459306717\n",
      "Validation Loss for fold 8: 0.10902304202318192\n",
      "Validation Loss for fold 8: 0.10697388648986816\n",
      "Validation Loss for fold 8: 0.10900097340345383\n",
      "Validation Loss for fold 8: 0.09822861850261688\n",
      "Validation Loss for fold 8: 0.09327999502420425\n",
      "Validation Loss for fold 8: 0.099270132680734\n",
      "Validation Loss for fold 8: 0.09234686195850372\n",
      "Validation Loss for fold 8: 0.08978669097026189\n",
      "Validation Loss for fold 8: 0.09125995139280955\n",
      "Validation Loss for fold 8: 0.09039977689584096\n",
      "Validation Loss for fold 8: 0.08931966125965118\n",
      "Validation Loss for fold 8: 0.09198640535275142\n",
      "Validation Loss for fold 8: 0.08610544850428899\n",
      "Validation Loss for fold 8: 0.08462497591972351\n",
      "Validation Loss for fold 8: 0.08380395919084549\n",
      "Validation Loss for fold 8: 0.091809776922067\n",
      "Validation Loss for fold 8: 0.08224869519472122\n",
      "Validation Loss for fold 8: 0.0913052757581075\n",
      "Validation Loss for fold 8: 0.08597381909688313\n",
      "Validation Loss for fold 8: 0.0859503224492073\n",
      "Validation Loss for fold 8: 0.08053772648175557\n",
      "Validation Loss for fold 8: 0.07809509585301082\n",
      "Validation Loss for fold 8: 0.08002392947673798\n",
      "Validation Loss for fold 8: 0.08777687698602676\n",
      "Validation Loss for fold 8: 0.08055232216914494\n",
      "Validation Loss for fold 8: 0.0881350686152776\n",
      "Validation Loss for fold 8: 0.08814271042744319\n",
      "Validation Loss for fold 8: 0.07586593925952911\n",
      "Validation Loss for fold 8: 0.08054982870817184\n",
      "Validation Loss for fold 8: 0.07160907611250877\n",
      "Validation Loss for fold 8: 0.06575917204221089\n",
      "Validation Loss for fold 8: 0.06984564041097958\n",
      "Validation Loss for fold 8: 0.07006086284915607\n",
      "Validation Loss for fold 8: 0.0787312959631284\n",
      "Validation Loss for fold 8: 0.07237732907136281\n",
      "Validation Loss for fold 8: 0.06975380207101504\n",
      "Validation Loss for fold 8: 0.07597331206003825\n",
      "Validation Loss for fold 8: 0.07432746390501659\n",
      "Validation Loss for fold 8: 0.07489675531784694\n",
      "Validation Loss for fold 8: 0.07575848450263341\n",
      "Validation Loss for fold 8: 0.06542611370484035\n",
      "Validation Loss for fold 8: 0.06757232050100963\n",
      "Validation Loss for fold 8: 0.0709972009062767\n",
      "Validation Loss for fold 8: 0.0698152557015419\n",
      "Validation Loss for fold 8: 0.07420184463262558\n",
      "Validation Loss for fold 8: 0.06464298193653424\n",
      "Validation Loss for fold 8: 0.07415790855884552\n",
      "Validation Loss for fold 8: 0.07284012685219447\n",
      "Validation Loss for fold 8: 0.07314028218388557\n",
      "Validation Loss for fold 8: 0.06644043450554211\n",
      "Validation Loss for fold 8: 0.06939574082692464\n",
      "Validation Loss for fold 8: 0.06566216796636581\n",
      "Validation Loss for fold 8: 0.07097224394480388\n",
      "Validation Loss for fold 8: 0.0692480926712354\n",
      "Validation Loss for fold 8: 0.0675607721010844\n",
      "Validation Loss for fold 8: 0.06595419968167941\n",
      "Validation Loss for fold 8: 0.0700012631714344\n",
      "Validation Loss for fold 8: 0.07327452798684438\n",
      "Validation Loss for fold 8: 0.06399061158299446\n",
      "Validation Loss for fold 8: 0.06335586309432983\n",
      "Validation Loss for fold 8: 0.06438341860969861\n",
      "Validation Loss for fold 8: 0.06711251040299733\n",
      "Validation Loss for fold 8: 0.07371607547005017\n",
      "Validation Loss for fold 8: 0.07184613992770512\n",
      "Validation Loss for fold 8: 0.06544100493192673\n",
      "Validation Loss for fold 8: 0.06573333715399106\n",
      "Validation Loss for fold 8: 0.06205935403704643\n",
      "Validation Loss for fold 8: 0.07120818768938382\n",
      "Validation Loss for fold 8: 0.06113438059886297\n",
      "Validation Loss for fold 8: 0.06639918809135754\n",
      "Validation Loss for fold 8: 0.06254966805378596\n",
      "Validation Loss for fold 8: 0.06690418347716331\n",
      "Validation Loss for fold 8: 0.06523760904868443\n",
      "Validation Loss for fold 8: 0.06370653832952182\n",
      "Validation Loss for fold 8: 0.06616070866584778\n",
      "Validation Loss for fold 8: 0.06801851838827133\n",
      "Validation Loss for fold 8: 0.0638784368832906\n",
      "Validation Loss for fold 8: 0.06520446638266246\n",
      "Validation Loss for fold 8: 0.06145406017700831\n",
      "Validation Loss for fold 8: 0.06275584921240807\n",
      "Validation Loss for fold 8: 0.061857412258783974\n",
      "Validation Loss for fold 8: 0.06034574036796888\n",
      "Validation Loss for fold 8: 0.06027624011039734\n",
      "Validation Loss for fold 8: 0.06282061835130055\n",
      "Validation Loss for fold 8: 0.06058540567755699\n",
      "Validation Loss for fold 8: 0.05679576223095258\n",
      "Validation Loss for fold 8: 0.06277960787216823\n",
      "Validation Loss for fold 8: 0.06360120822985967\n",
      "Validation Loss for fold 8: 0.062002621591091156\n",
      "Validation Loss for fold 8: 0.06027521938085556\n",
      "Validation Loss for fold 8: 0.061134650061527886\n",
      "Validation Loss for fold 8: 0.059433540950218834\n",
      "Validation Loss for fold 8: 0.05887318029999733\n",
      "Validation Loss for fold 8: 0.05677195762594541\n",
      "Validation Loss for fold 8: 0.07042089849710464\n",
      "Validation Loss for fold 8: 0.05731973176201185\n",
      "Validation Loss for fold 8: 0.058331082264582314\n",
      "Validation Loss for fold 8: 0.06713184962670009\n",
      "Validation Loss for fold 8: 0.06643979623913765\n",
      "Validation Loss for fold 8: 0.06061272447307905\n",
      "Validation Loss for fold 8: 0.06286482140421867\n",
      "Validation Loss for fold 8: 0.05776377891500791\n",
      "Validation Loss for fold 8: 0.06179525454839071\n",
      "Validation Loss for fold 8: 0.056493405252695084\n",
      "Validation Loss for fold 8: 0.06372073789437611\n",
      "Validation Loss for fold 8: 0.059281195203463234\n",
      "Validation Loss for fold 8: 0.059704966843128204\n",
      "Validation Loss for fold 8: 0.054907090961933136\n",
      "Validation Loss for fold 8: 0.0592683677872022\n",
      "Validation Loss for fold 8: 0.0639135812719663\n",
      "Validation Loss for fold 8: 0.05842454979817072\n",
      "Validation Loss for fold 8: 0.061068388322989144\n",
      "Validation Loss for fold 8: 0.06056303530931473\n",
      "Validation Loss for fold 8: 0.055522672832012177\n",
      "Validation Loss for fold 8: 0.05311034992337227\n",
      "Validation Loss for fold 8: 0.0550950455168883\n",
      "Validation Loss for fold 8: 0.0611208143333594\n",
      "Validation Loss for fold 8: 0.05915999040007591\n",
      "Validation Loss for fold 8: 0.05293369789918264\n",
      "Validation Loss for fold 8: 0.05475138376156489\n",
      "Validation Loss for fold 8: 0.061386071145534515\n",
      "Validation Loss for fold 8: 0.05451073621710142\n",
      "Validation Loss for fold 8: 0.05229788149396578\n",
      "Validation Loss for fold 8: 0.06206954518953959\n",
      "Validation Loss for fold 8: 0.06120008106033007\n",
      "Validation Loss for fold 8: 0.05868918697039286\n",
      "Validation Loss for fold 8: 0.05573611209789912\n",
      "Validation Loss for fold 8: 0.05641082550088564\n",
      "Validation Loss for fold 8: 0.05453799292445183\n",
      "Validation Loss for fold 8: 0.05814081182082494\n",
      "Validation Loss for fold 8: 0.05752944822112719\n",
      "Validation Loss for fold 8: 0.054742551098267235\n",
      "Validation Loss for fold 8: 0.05122130364179611\n",
      "Validation Loss for fold 8: 0.05711409077048302\n",
      "Validation Loss for fold 8: 0.05992850661277771\n",
      "Validation Loss for fold 8: 0.054739305128653847\n",
      "Validation Loss for fold 8: 0.06092156718174616\n",
      "Validation Loss for fold 8: 0.05970300237337748\n",
      "Validation Loss for fold 8: 0.049240054562687874\n",
      "Validation Loss for fold 8: 0.06696606799960136\n",
      "Validation Loss for fold 8: 0.054034072905778885\n",
      "Validation Loss for fold 8: 0.058017381777366005\n",
      "Validation Loss for fold 8: 0.059672207882006965\n",
      "Validation Loss for fold 8: 0.056546264638503395\n",
      "Validation Loss for fold 8: 0.05667374779780706\n",
      "Validation Loss for fold 8: 0.059772820522387825\n",
      "Validation Loss for fold 8: 0.05627002442876498\n",
      "Validation Loss for fold 8: 0.05293178682525953\n",
      "Validation Loss for fold 8: 0.05486352865894636\n",
      "Validation Loss for fold 8: 0.05365685373544693\n",
      "Validation Loss for fold 8: 0.056899841874837875\n",
      "Validation Loss for fold 8: 0.054669711738824844\n",
      "Validation Loss for fold 8: 0.059560256699721016\n",
      "Validation Loss for fold 8: 0.05641531944274902\n",
      "Validation Loss for fold 8: 0.054713654021422066\n",
      "Validation Loss for fold 8: 0.052659367521603904\n",
      "Validation Loss for fold 8: 0.04912250923613707\n",
      "Validation Loss for fold 8: 0.052036198476950325\n",
      "Validation Loss for fold 8: 0.050774370630582176\n",
      "Validation Loss for fold 8: 0.056450389325618744\n",
      "Validation Loss for fold 8: 0.05941241482893626\n",
      "Validation Loss for fold 8: 0.0578899160027504\n",
      "Validation Loss for fold 8: 0.05707203348477682\n",
      "Validation Loss for fold 8: 0.057952514539162316\n",
      "Validation Loss for fold 8: 0.05374073858062426\n",
      "Validation Loss for fold 8: 0.05885538458824158\n",
      "Validation Loss for fold 8: 0.053869557877381645\n",
      "Validation Loss for fold 8: 0.05769739920894305\n",
      "Validation Loss for fold 8: 0.05580620840191841\n",
      "Validation Loss for fold 8: 0.05292818074425062\n",
      "Validation Loss for fold 8: 0.05443079521258672\n",
      "Validation Loss for fold 8: 0.05425174410144488\n",
      "Validation Loss for fold 8: 0.053588591516017914\n",
      "Validation Loss for fold 8: 0.05514036491513252\n",
      "Validation Loss for fold 8: 0.058295921732982\n",
      "Validation Loss for fold 8: 0.05948835611343384\n",
      "Validation Loss for fold 8: 0.05931405598918597\n",
      "Validation Loss for fold 8: 0.05341327687104543\n",
      "Validation Loss for fold 8: 0.05781852329770724\n",
      "Validation Loss for fold 8: 0.054816645880540214\n",
      "Validation Loss for fold 8: 0.05674125254154205\n",
      "Validation Loss for fold 8: 0.051455434411764145\n",
      "Validation Loss for fold 8: 0.05539502203464508\n",
      "Validation Loss for fold 8: 0.04944493621587753\n",
      "Validation Loss for fold 8: 0.056717404474814735\n",
      "Validation Loss for fold 8: 0.05648460860053698\n",
      "Validation Loss for fold 8: 0.05524007355173429\n",
      "Validation Loss for fold 8: 0.0539256843427817\n",
      "Validation Loss for fold 8: 0.05047334233919779\n",
      "Validation Loss for fold 8: 0.050910974542299904\n",
      "Validation Loss for fold 8: 0.055157143622636795\n",
      "Validation Loss for fold 8: 0.047368411595622696\n",
      "Validation Loss for fold 8: 0.05365309367577235\n",
      "Validation Loss for fold 8: 0.05021738260984421\n",
      "Validation Loss for fold 8: 0.05630358929435412\n",
      "Validation Loss for fold 8: 0.05195380002260208\n",
      "Validation Loss for fold 8: 0.051140165577332176\n",
      "Validation Loss for fold 8: 0.05044863745570183\n",
      "Validation Loss for fold 8: 0.054619072626034416\n",
      "Validation Loss for fold 8: 0.053730472922325134\n",
      "Validation Loss for fold 8: 0.06035629411538442\n",
      "Validation Loss for fold 8: 0.05669134110212326\n",
      "Validation Loss for fold 8: 0.05391846224665642\n",
      "Validation Loss for fold 8: 0.050647431363662086\n",
      "Validation Loss for fold 8: 0.05787246053417524\n",
      "Validation Loss for fold 8: 0.05433576429883639\n",
      "Validation Loss for fold 8: 0.054949173082908\n",
      "Validation Loss for fold 8: 0.055189959704875946\n",
      "Validation Loss for fold 8: 0.0495353601872921\n",
      "Validation Loss for fold 8: 0.052534569054841995\n",
      "Validation Loss for fold 8: 0.05528666451573372\n",
      "Validation Loss for fold 8: 0.05923829972743988\n",
      "Validation Loss for fold 8: 0.05789532512426376\n",
      "Validation Loss for fold 8: 0.05610323200623194\n",
      "Validation Loss for fold 8: 0.05279844750960668\n",
      "Validation Loss for fold 8: 0.04936525598168373\n",
      "Validation Loss for fold 8: 0.05510730048020681\n",
      "Validation Loss for fold 8: 0.05063949649532636\n",
      "Validation Loss for fold 8: 0.051576909919579826\n",
      "Validation Loss for fold 8: 0.05797794088721275\n",
      "Validation Loss for fold 8: 0.05454062670469284\n",
      "Validation Loss for fold 8: 0.052446783830722175\n",
      "Validation Loss for fold 8: 0.05759378522634506\n",
      "Validation Loss for fold 8: 0.05824365963538488\n",
      "Validation Loss for fold 8: 0.0479480375846227\n",
      "Validation Loss for fold 8: 0.051703685273726784\n",
      "Validation Loss for fold 8: 0.05523183320959409\n",
      "Validation Loss for fold 8: 0.053646800418694816\n",
      "Validation Loss for fold 8: 0.048876101771990456\n",
      "Validation Loss for fold 8: 0.051436432947715126\n",
      "Validation Loss for fold 8: 0.062195632606744766\n",
      "Validation Loss for fold 8: 0.05318377912044525\n",
      "Validation Loss for fold 8: 0.05027496566375097\n",
      "Validation Loss for fold 8: 0.05417973672350248\n",
      "Validation Loss for fold 8: 0.048086932549873986\n",
      "Validation Loss for fold 8: 0.054170274486144386\n",
      "Validation Loss for fold 8: 0.051095958799123764\n",
      "Validation Loss for fold 8: 0.05541036153833071\n",
      "Validation Loss for fold 8: 0.050597877552111946\n",
      "Validation Loss for fold 8: 0.04991225401560465\n",
      "Validation Loss for fold 8: 0.0481051467359066\n",
      "Validation Loss for fold 8: 0.05183295408884684\n",
      "Validation Loss for fold 8: 0.046065330505371094\n",
      "Validation Loss for fold 8: 0.0481015108525753\n",
      "Validation Loss for fold 8: 0.056970544159412384\n",
      "Validation Loss for fold 8: 0.05215233067671458\n",
      "Validation Loss for fold 8: 0.050078963239987694\n",
      "Validation Loss for fold 8: 0.04796590656042099\n",
      "Validation Loss for fold 8: 0.049499151607354484\n",
      "Validation Loss for fold 8: 0.05425971125562986\n",
      "Validation Loss for fold 8: 0.050423835714658104\n",
      "Validation Loss for fold 8: 0.048669187972942986\n",
      "Validation Loss for fold 8: 0.05707374960184097\n",
      "Validation Loss for fold 8: 0.04835008581479391\n",
      "Validation Loss for fold 8: 0.04628632341821989\n",
      "Validation Loss for fold 8: 0.05428333828846613\n",
      "Validation Loss for fold 8: 0.053149369855721794\n",
      "Validation Loss for fold 8: 0.05073971549669901\n",
      "Validation Loss for fold 8: 0.05280874545375506\n",
      "Validation Loss for fold 8: 0.04959230124950409\n",
      "Validation Loss for fold 8: 0.04948063318928083\n",
      "Validation Loss for fold 8: 0.05031798283259074\n",
      "Validation Loss for fold 8: 0.05219611277182897\n",
      "Validation Loss for fold 8: 0.05117105692625046\n",
      "Validation Loss for fold 8: 0.05019190162420273\n",
      "Validation Loss for fold 8: 0.05147411674261093\n",
      "Validation Loss for fold 8: 0.052834540605545044\n",
      "Validation Loss for fold 8: 0.04996280868848165\n",
      "Validation Loss for fold 8: 0.04616337517897288\n",
      "Validation Loss for fold 8: 0.05758687357107798\n",
      "Validation Loss for fold 8: 0.050394680351018906\n",
      "Validation Loss for fold 8: 0.05185239762067795\n",
      "Validation Loss for fold 8: 0.05289974436163902\n",
      "Validation Loss for fold 8: 0.0539923831820488\n",
      "Validation Loss for fold 8: 0.04999456057945887\n",
      "Validation Loss for fold 8: 0.05053721865018209\n",
      "Validation Loss for fold 8: 0.045672198136647545\n",
      "Validation Loss for fold 8: 0.056993952641884484\n",
      "Validation Loss for fold 8: 0.05369423578182856\n",
      "Validation Loss for fold 8: 0.05643546084562937\n",
      "Validation Loss for fold 8: 0.054966945201158524\n",
      "Validation Loss for fold 8: 0.049460003773371376\n",
      "Validation Loss for fold 8: 0.05294650668899218\n",
      "Validation Loss for fold 8: 0.05366839716831843\n",
      "Validation Loss for fold 8: 0.04907786473631859\n",
      "Validation Loss for fold 8: 0.0522928312420845\n",
      "Validation Loss for fold 8: 0.05260648081700007\n",
      "--------------------------------\n",
      "FOLD 9\n",
      "--------------------------------\n",
      "Validation Loss for fold 9: 0.46946753064791363\n",
      "Validation Loss for fold 9: 0.3672219117482503\n",
      "Validation Loss for fold 9: 0.2727731267611186\n",
      "Validation Loss for fold 9: 0.23581408460934958\n",
      "Validation Loss for fold 9: 0.20881314078966776\n",
      "Validation Loss for fold 9: 0.17367817958196005\n",
      "Validation Loss for fold 9: 0.17621601621309915\n",
      "Validation Loss for fold 9: 0.15317795674006143\n",
      "Validation Loss for fold 9: 0.15288337568442026\n",
      "Validation Loss for fold 9: 0.13482323785622916\n",
      "Validation Loss for fold 9: 0.12449879447619121\n",
      "Validation Loss for fold 9: 0.12175821264584859\n",
      "Validation Loss for fold 9: 0.1353620613614718\n",
      "Validation Loss for fold 9: 0.12261141836643219\n",
      "Validation Loss for fold 9: 0.113527516523997\n",
      "Validation Loss for fold 9: 0.12489733099937439\n",
      "Validation Loss for fold 9: 0.11316188176472981\n",
      "Validation Loss for fold 9: 0.12621955573558807\n",
      "Validation Loss for fold 9: 0.1196008746822675\n",
      "Validation Loss for fold 9: 0.11261431872844696\n",
      "Validation Loss for fold 9: 0.11132584512233734\n",
      "Validation Loss for fold 9: 0.10782468318939209\n",
      "Validation Loss for fold 9: 0.10179155816634496\n",
      "Validation Loss for fold 9: 0.1069331889351209\n",
      "Validation Loss for fold 9: 0.11836834500233333\n",
      "Validation Loss for fold 9: 0.10280592242876689\n",
      "Validation Loss for fold 9: 0.11428836733102798\n",
      "Validation Loss for fold 9: 0.11294777939716975\n",
      "Validation Loss for fold 9: 0.0961959461371104\n",
      "Validation Loss for fold 9: 0.09830558051665624\n",
      "Validation Loss for fold 9: 0.10939681281646092\n",
      "Validation Loss for fold 9: 0.10276226699352264\n",
      "Validation Loss for fold 9: 0.09578997641801834\n",
      "Validation Loss for fold 9: 0.10768906027078629\n",
      "Validation Loss for fold 9: 0.107682965695858\n",
      "Validation Loss for fold 9: 0.10004661480585735\n",
      "Validation Loss for fold 9: 0.10557497044404347\n",
      "Validation Loss for fold 9: 0.0915562870601813\n",
      "Validation Loss for fold 9: 0.0972343310713768\n",
      "Validation Loss for fold 9: 0.0951089213291804\n",
      "Validation Loss for fold 9: 0.09462488691012065\n",
      "Validation Loss for fold 9: 0.09447994579871495\n",
      "Validation Loss for fold 9: 0.08836770554383595\n",
      "Validation Loss for fold 9: 0.0896904394030571\n",
      "Validation Loss for fold 9: 0.09345689167579015\n",
      "Validation Loss for fold 9: 0.09108621875445048\n",
      "Validation Loss for fold 9: 0.08540766934553783\n",
      "Validation Loss for fold 9: 0.09734066824118297\n",
      "Validation Loss for fold 9: 0.09222437938054402\n",
      "Validation Loss for fold 9: 0.08880089223384857\n",
      "Validation Loss for fold 9: 0.08242171506086986\n",
      "Validation Loss for fold 9: 0.093687338133653\n",
      "Validation Loss for fold 9: 0.09164803723494212\n",
      "Validation Loss for fold 9: 0.08930828422307968\n",
      "Validation Loss for fold 9: 0.09221872438987096\n",
      "Validation Loss for fold 9: 0.08744702239831288\n",
      "Validation Loss for fold 9: 0.08560888717571895\n",
      "Validation Loss for fold 9: 0.08631277581055959\n",
      "Validation Loss for fold 9: 0.08933538695176442\n",
      "Validation Loss for fold 9: 0.09068029870589574\n",
      "Validation Loss for fold 9: 0.08377133061488469\n",
      "Validation Loss for fold 9: 0.07963627701004346\n",
      "Validation Loss for fold 9: 0.09007568905750911\n",
      "Validation Loss for fold 9: 0.08837807675202687\n",
      "Validation Loss for fold 9: 0.09341316173473994\n",
      "Validation Loss for fold 9: 0.07860159501433372\n",
      "Validation Loss for fold 9: 0.08281360069910686\n",
      "Validation Loss for fold 9: 0.08248447130123775\n",
      "Validation Loss for fold 9: 0.08736670513947804\n",
      "Validation Loss for fold 9: 0.07917231569687526\n",
      "Validation Loss for fold 9: 0.08860935270786285\n",
      "Validation Loss for fold 9: 0.08500889192024867\n",
      "Validation Loss for fold 9: 0.08148009826739629\n",
      "Validation Loss for fold 9: 0.08223120123147964\n",
      "Validation Loss for fold 9: 0.07714466005563736\n",
      "Validation Loss for fold 9: 0.0768814707795779\n",
      "Validation Loss for fold 9: 0.0831402267018954\n",
      "Validation Loss for fold 9: 0.07942480593919754\n",
      "Validation Loss for fold 9: 0.08012995620568593\n",
      "Validation Loss for fold 9: 0.08244730283816655\n",
      "Validation Loss for fold 9: 0.08299672355254491\n",
      "Validation Loss for fold 9: 0.07639761020739873\n",
      "Validation Loss for fold 9: 0.08552748461564381\n",
      "Validation Loss for fold 9: 0.07537208621700604\n",
      "Validation Loss for fold 9: 0.07884158690770467\n",
      "Validation Loss for fold 9: 0.07802881300449371\n",
      "Validation Loss for fold 9: 0.07925249139467876\n",
      "Validation Loss for fold 9: 0.07986553261677425\n",
      "Validation Loss for fold 9: 0.07898038377364476\n",
      "Validation Loss for fold 9: 0.08042662839094798\n",
      "Validation Loss for fold 9: 0.07395798464616139\n",
      "Validation Loss for fold 9: 0.06920201952258746\n",
      "Validation Loss for fold 9: 0.07321690519650777\n",
      "Validation Loss for fold 9: 0.07466185837984085\n",
      "Validation Loss for fold 9: 0.07160659258564313\n",
      "Validation Loss for fold 9: 0.07343348115682602\n",
      "Validation Loss for fold 9: 0.07304842273394267\n",
      "Validation Loss for fold 9: 0.07533620794614156\n",
      "Validation Loss for fold 9: 0.07077551260590553\n",
      "Validation Loss for fold 9: 0.08033562699953715\n",
      "Validation Loss for fold 9: 0.07502187540133794\n",
      "Validation Loss for fold 9: 0.0789661134282748\n",
      "Validation Loss for fold 9: 0.07783034443855286\n",
      "Validation Loss for fold 9: 0.07021660233537357\n",
      "Validation Loss for fold 9: 0.07169992104172707\n",
      "Validation Loss for fold 9: 0.08068600917855899\n",
      "Validation Loss for fold 9: 0.07750351230303447\n",
      "Validation Loss for fold 9: 0.07557958861192067\n",
      "Validation Loss for fold 9: 0.06537546341617902\n",
      "Validation Loss for fold 9: 0.0694732094804446\n",
      "Validation Loss for fold 9: 0.07462375362714131\n",
      "Validation Loss for fold 9: 0.07805137584606807\n",
      "Validation Loss for fold 9: 0.06978928297758102\n",
      "Validation Loss for fold 9: 0.076902541021506\n",
      "Validation Loss for fold 9: 0.0649843302865823\n",
      "Validation Loss for fold 9: 0.07216138641039531\n",
      "Validation Loss for fold 9: 0.07577368865410487\n",
      "Validation Loss for fold 9: 0.07212022691965103\n",
      "Validation Loss for fold 9: 0.07029771308104198\n",
      "Validation Loss for fold 9: 0.06715876360734303\n",
      "Validation Loss for fold 9: 0.07206396510203679\n",
      "Validation Loss for fold 9: 0.07622621456782024\n",
      "Validation Loss for fold 9: 0.06837831934293111\n",
      "Validation Loss for fold 9: 0.06636531154314677\n",
      "Validation Loss for fold 9: 0.06941222647825877\n",
      "Validation Loss for fold 9: 0.06561426197489102\n",
      "Validation Loss for fold 9: 0.07408343503872554\n",
      "Validation Loss for fold 9: 0.0755615159869194\n",
      "Validation Loss for fold 9: 0.07122913996378581\n",
      "Validation Loss for fold 9: 0.0746430071691672\n",
      "Validation Loss for fold 9: 0.0662725418806076\n",
      "Validation Loss for fold 9: 0.06405773758888245\n",
      "Validation Loss for fold 9: 0.06455875063935916\n",
      "Validation Loss for fold 9: 0.06310484185814857\n",
      "Validation Loss for fold 9: 0.0672902228931586\n",
      "Validation Loss for fold 9: 0.06513261049985886\n",
      "Validation Loss for fold 9: 0.06864139934380849\n",
      "Validation Loss for fold 9: 0.07399945457776387\n",
      "Validation Loss for fold 9: 0.0766993264357249\n",
      "Validation Loss for fold 9: 0.06452277799447377\n",
      "Validation Loss for fold 9: 0.06696988518039386\n",
      "Validation Loss for fold 9: 0.06370190903544426\n",
      "Validation Loss for fold 9: 0.06640342622995377\n",
      "Validation Loss for fold 9: 0.06798330694437027\n",
      "Validation Loss for fold 9: 0.07007751613855362\n",
      "Validation Loss for fold 9: 0.07186431686083476\n",
      "Validation Loss for fold 9: 0.07402507836620013\n",
      "Validation Loss for fold 9: 0.07000446816285451\n",
      "Validation Loss for fold 9: 0.07380411277214687\n",
      "Validation Loss for fold 9: 0.06692374746004741\n",
      "Validation Loss for fold 9: 0.0741586871445179\n",
      "Validation Loss for fold 9: 0.06347169975439708\n",
      "Validation Loss for fold 9: 0.0640336126089096\n",
      "Validation Loss for fold 9: 0.06980791439612706\n",
      "Validation Loss for fold 9: 0.07661922151843707\n",
      "Validation Loss for fold 9: 0.06548140943050385\n",
      "Validation Loss for fold 9: 0.0650392696261406\n",
      "Validation Loss for fold 9: 0.06740644077459972\n",
      "Validation Loss for fold 9: 0.07659295325477918\n",
      "Validation Loss for fold 9: 0.06249980380137762\n",
      "Validation Loss for fold 9: 0.06645022705197334\n",
      "Validation Loss for fold 9: 0.07314944639801979\n",
      "Validation Loss for fold 9: 0.061848449210325875\n",
      "Validation Loss for fold 9: 0.06256972377498944\n",
      "Validation Loss for fold 9: 0.0652164841691653\n",
      "Validation Loss for fold 9: 0.06736144920190175\n",
      "Validation Loss for fold 9: 0.06126087655623754\n",
      "Validation Loss for fold 9: 0.056721355145176254\n",
      "Validation Loss for fold 9: 0.06638366853197415\n",
      "Validation Loss for fold 9: 0.0683816410601139\n",
      "Validation Loss for fold 9: 0.07269033417105675\n",
      "Validation Loss for fold 9: 0.06453200926383336\n",
      "Validation Loss for fold 9: 0.06396400928497314\n",
      "Validation Loss for fold 9: 0.061447971810897194\n",
      "Validation Loss for fold 9: 0.06463266164064407\n",
      "Validation Loss for fold 9: 0.06508195772767067\n",
      "Validation Loss for fold 9: 0.059341022123893104\n",
      "Validation Loss for fold 9: 0.0653664047519366\n",
      "Validation Loss for fold 9: 0.05755876128872236\n",
      "Validation Loss for fold 9: 0.0600564070045948\n",
      "Validation Loss for fold 9: 0.06336005901296933\n",
      "Validation Loss for fold 9: 0.06193193420767784\n",
      "Validation Loss for fold 9: 0.062003470957279205\n",
      "Validation Loss for fold 9: 0.05822678282856941\n",
      "Validation Loss for fold 9: 0.06528150041898091\n",
      "Validation Loss for fold 9: 0.060635762910048165\n",
      "Validation Loss for fold 9: 0.06667236611247063\n",
      "Validation Loss for fold 9: 0.06540886436899503\n",
      "Validation Loss for fold 9: 0.06295890112717946\n",
      "Validation Loss for fold 9: 0.06441014756759007\n",
      "Validation Loss for fold 9: 0.06692439317703247\n",
      "Validation Loss for fold 9: 0.06342885394891103\n",
      "Validation Loss for fold 9: 0.06323195993900299\n",
      "Validation Loss for fold 9: 0.0672031541665395\n",
      "Validation Loss for fold 9: 0.057757639636596046\n",
      "Validation Loss for fold 9: 0.062206863115231194\n",
      "Validation Loss for fold 9: 0.06665928040941556\n",
      "Validation Loss for fold 9: 0.059531072775522866\n",
      "Validation Loss for fold 9: 0.06415055443843205\n",
      "Validation Loss for fold 9: 0.06475923955440521\n",
      "Validation Loss for fold 9: 0.06264905134836833\n",
      "Validation Loss for fold 9: 0.06979231908917427\n",
      "Validation Loss for fold 9: 0.06000124290585518\n",
      "Validation Loss for fold 9: 0.05705342814326286\n",
      "Validation Loss for fold 9: 0.05967691416541735\n",
      "Validation Loss for fold 9: 0.06217991063992182\n",
      "Validation Loss for fold 9: 0.05844118197758993\n",
      "Validation Loss for fold 9: 0.06643812482555707\n",
      "Validation Loss for fold 9: 0.05476980283856392\n",
      "Validation Loss for fold 9: 0.05923695613940557\n",
      "Validation Loss for fold 9: 0.06702748189369838\n",
      "Validation Loss for fold 9: 0.05710171659787496\n",
      "Validation Loss for fold 9: 0.06248657653729121\n",
      "Validation Loss for fold 9: 0.05324565495053927\n",
      "Validation Loss for fold 9: 0.06322429204980533\n",
      "Validation Loss for fold 9: 0.06833554307619731\n",
      "Validation Loss for fold 9: 0.06028290092945099\n",
      "Validation Loss for fold 9: 0.0593126080930233\n",
      "Validation Loss for fold 9: 0.06679279853900273\n",
      "Validation Loss for fold 9: 0.06014986957112948\n",
      "Validation Loss for fold 9: 0.0565789428850015\n",
      "Validation Loss for fold 9: 0.061084521313508354\n",
      "Validation Loss for fold 9: 0.05964077264070511\n",
      "Validation Loss for fold 9: 0.05761331816514333\n",
      "Validation Loss for fold 9: 0.06714129075407982\n",
      "Validation Loss for fold 9: 0.06186444312334061\n",
      "Validation Loss for fold 9: 0.058978681763013206\n",
      "Validation Loss for fold 9: 0.06049806624650955\n",
      "Validation Loss for fold 9: 0.05713294446468353\n",
      "Validation Loss for fold 9: 0.05116947367787361\n",
      "Validation Loss for fold 9: 0.05791310469309489\n",
      "Validation Loss for fold 9: 0.055029891431331635\n",
      "Validation Loss for fold 9: 0.06166018545627594\n",
      "Validation Loss for fold 9: 0.05774317185084025\n",
      "Validation Loss for fold 9: 0.06306447709600131\n",
      "Validation Loss for fold 9: 0.060424054662386574\n",
      "Validation Loss for fold 9: 0.06120022013783455\n",
      "Validation Loss for fold 9: 0.05922641729315122\n",
      "Validation Loss for fold 9: 0.06395441169540088\n",
      "Validation Loss for fold 9: 0.05761033544937769\n",
      "Validation Loss for fold 9: 0.05906210963924726\n",
      "Validation Loss for fold 9: 0.061652050664027534\n",
      "Validation Loss for fold 9: 0.052595059076944985\n",
      "Validation Loss for fold 9: 0.06002314140399297\n",
      "Validation Loss for fold 9: 0.057185849795738854\n",
      "Validation Loss for fold 9: 0.057035624980926514\n",
      "Validation Loss for fold 9: 0.06286409373084705\n",
      "Validation Loss for fold 9: 0.061207917829354606\n",
      "Validation Loss for fold 9: 0.06340094904104869\n",
      "Validation Loss for fold 9: 0.056583575904369354\n",
      "Validation Loss for fold 9: 0.059656694531440735\n",
      "Validation Loss for fold 9: 0.059938132762908936\n",
      "Validation Loss for fold 9: 0.056175472835699715\n",
      "Validation Loss for fold 9: 0.05832295119762421\n",
      "Validation Loss for fold 9: 0.054975138356288276\n",
      "Validation Loss for fold 9: 0.0590760683019956\n",
      "Validation Loss for fold 9: 0.05677307521303495\n",
      "Validation Loss for fold 9: 0.05961619565884272\n",
      "Validation Loss for fold 9: 0.05918112273017565\n",
      "Validation Loss for fold 9: 0.056570115188757576\n",
      "Validation Loss for fold 9: 0.058372363448143005\n",
      "Validation Loss for fold 9: 0.056292906403541565\n",
      "Validation Loss for fold 9: 0.05737800647815069\n",
      "Validation Loss for fold 9: 0.06186430032054583\n",
      "Validation Loss for fold 9: 0.05720872680346171\n",
      "Validation Loss for fold 9: 0.06613628938794136\n",
      "Validation Loss for fold 9: 0.058172749976317085\n",
      "Validation Loss for fold 9: 0.05847089737653732\n",
      "Validation Loss for fold 9: 0.05131117689112822\n",
      "Validation Loss for fold 9: 0.05436735972762108\n",
      "Validation Loss for fold 9: 0.055446198831001915\n",
      "Validation Loss for fold 9: 0.05939607694745064\n",
      "Validation Loss for fold 9: 0.06535052135586739\n",
      "Validation Loss for fold 9: 0.05826134110490481\n",
      "Validation Loss for fold 9: 0.058245702336231865\n",
      "Validation Loss for fold 9: 0.05288982639710108\n",
      "Validation Loss for fold 9: 0.054414406418800354\n",
      "Validation Loss for fold 9: 0.05585964769124985\n",
      "Validation Loss for fold 9: 0.05571925019224485\n",
      "Validation Loss for fold 9: 0.05305622021357218\n",
      "Validation Loss for fold 9: 0.06360391279061635\n",
      "Validation Loss for fold 9: 0.05577839662631353\n",
      "Validation Loss for fold 9: 0.05696231871843338\n",
      "Validation Loss for fold 9: 0.05438502381245295\n",
      "Validation Loss for fold 9: 0.0584830567240715\n",
      "Validation Loss for fold 9: 0.058277443051338196\n",
      "Validation Loss for fold 9: 0.054582775880893074\n",
      "Validation Loss for fold 9: 0.05761683980623881\n",
      "Validation Loss for fold 9: 0.05305977538228035\n",
      "Validation Loss for fold 9: 0.059761026253302894\n",
      "Validation Loss for fold 9: 0.05246741076310476\n",
      "Validation Loss for fold 9: 0.055114069332679115\n",
      "Validation Loss for fold 9: 0.059100682536760964\n",
      "Validation Loss for fold 9: 0.05588257188598315\n",
      "Validation Loss for fold 9: 0.057967757185300194\n",
      "Validation Loss for fold 9: 0.05914084861675898\n",
      "Validation Loss for fold 9: 0.05616427088777224\n",
      "Validation Loss for fold 9: 0.052758834014336266\n",
      "Validation Loss for fold 9: 0.05878614137570063\n",
      "Validation Loss for fold 9: 0.05928893635670344\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Avg Validation Loss</td><td>▄▃▂▂▄▂▂▂█▂▁▁▃▂▂▂▅▂▁▁▄▂▂▂▃▂▂▁▁▁▁▁▃▂▂▁▄▂▂▁</td></tr><tr><td>Avg. Training Loss</td><td>▄▂▁▁▄▂▁▁█▂▁▁▃▂▁▁▅▂▁▁▃▂▁▁▄▂▁▁▃▂▁▁▂▁▁▁▄▁▁▁</td></tr><tr><td>Epoch</td><td>▂▃▅▇▁▃▅▇▁▄▅▇▂▃▆▇▁▄▅▇▂▃▅█▁▃▆▇▂▄▅█▂▃▆▇▁▄▅█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Avg Validation Loss</td><td>0.05929</td></tr><tr><td>Avg. Training Loss</td><td>0.0378</td></tr><tr><td>Epoch</td><td>299</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">upbeat-fire-38</strong> at: <a href='https://wandb.ai/jcrich/collaborative%20filter%20model/runs/cq244pkg' target=\"_blank\">https://wandb.ai/jcrich/collaborative%20filter%20model/runs/cq244pkg</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240312_102644-cq244pkg/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Correct variable names and logic\n",
    "# Set the best validation loss to infinity at the start\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for fold, (train_ids, val_ids) in enumerate(kfold.split(train_ds)):\n",
    "    print(f'FOLD {fold}')\n",
    "    print('--------------------------------')\n",
    "\n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "    val_subsampler = torch.utils.data.SubsetRandomSampler(val_ids)\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=train_subsampler)\n",
    "    valloader = torch.utils.data.DataLoader(validation_ds, batch_size=BATCH_SIZE, sampler=val_subsampler)\n",
    "\n",
    "    model = MLPCollaborativeFilter(num_users + 1, num_movies + 1, embedding_dim=FEATURES)\n",
    "    optimiser = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE,weight_decay=L2_REGULARIZATION)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for user_indices, item_indices, ratings in trainloader:\n",
    "            optimiser.zero_grad()\n",
    "            outputs = model(user_indices, item_indices).squeeze()\n",
    "            loss = criterion(outputs, ratings)\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Average training loss over all batches\n",
    "        train_loss /= len(trainloader)\n",
    "        # Log training loss for the current epoch\n",
    "        wandb.log({\"Avg. Training Loss\": train_loss, \"Epoch\": epoch})\n",
    "        mlflow.log_metric(\"Avg. Training Loss\", train_loss, step=epoch)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for user_indices, item_indices, ratings in valloader:\n",
    "                outputs = model(user_indices, item_indices).squeeze()\n",
    "                loss = criterion(outputs, ratings)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        avg_validation_loss = val_loss / len(valloader)\n",
    "        print(f'Validation Loss for fold {fold}: {avg_validation_loss}')\n",
    "        \n",
    "        if avg_validation_loss < best_val_loss:\n",
    "            best_val_loss = avg_validation_loss\n",
    "            mlflow.log_metric(\"Best Validation Loss\", best_val_loss, step=epoch)\n",
    "            mlflow.pytorch.log_model(model, \"model\")\n",
    "            # Save model state\n",
    "            torch.save(model.state_dict(), 'best_model_state.pth')\n",
    "            # If you also want to save the optimizer state along with the model:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimiser_state_dict': optimiser.state_dict(),\n",
    "                'loss': avg_validation_loss,\n",
    "            }, 'best_col_model_checkpoint.pth')\n",
    "            # Log the model checkpoint as an artifact\n",
    "            mlflow.log_artifact('best_col_model_checkpoint.pth')\n",
    "        \n",
    "        # Log validation loss for the current epoch\n",
    "        wandb.log({\"Avg Validation Loss\": avg_validation_loss, \"Epoch\": epoch})\n",
    "        mlflow.log_metric(\"Avg Validation Loss\", avg_validation_loss, step=epoch)\n",
    "    \n",
    "    print('--------------------------------')\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9ede2c71-6995-4fd5-a657-e15996945cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05917179976638995\n"
     ]
    }
   ],
   "source": [
    "#test data\n",
    "model.eval()  # Switch to evaluation mode\n",
    "test_loss = 0\n",
    "correct_predictions = 0  # Fixed variable name for clarity\n",
    "total_contexts = 0\n",
    "\n",
    "with torch.no_grad():  # Disable gradient computation\n",
    "    for user_ids, movie_ids, ratings in test_data_loader:\n",
    "        predictions = model(user_ids,movie_ids)  # Generate predictions\n",
    "        # Calculate and accumulate loss\n",
    "        loss = criterion(predictions, ratings)\n",
    "        test_loss += loss.item()\n",
    "        \n",
    "        # # Reshape predictions to match [batch_size, context_size, vocab_size]\n",
    "        # predictions = predictions.view(-1, context_size, VOCAB_SIZE)\n",
    "        \n",
    "        # # Get top prediction for each context position\n",
    "        # top_predictions = predictions.argmax(dim=2)\n",
    "        \n",
    "        # # Calculate correct predictions\n",
    "        # correct_preds = (top_predictions == context).float().sum()\n",
    "        # correct_predictions += correct_preds.item()  # Accumulate correct predictions\n",
    "        \n",
    "        # total_contexts += context.numel()  # Total number of context word positions evaluated\n",
    "\n",
    "# Calculate final metrics\n",
    "test_loss /= len(test_data_loader)  # Average loss per batch\n",
    "# print('correct predictions = ',correct_predictions)\n",
    "# print('out of  = ',total_contexts)\n",
    "# accuracy = correct_predictions / total_contexts  # Compute accuracy\n",
    "\n",
    "# print(f'Test set: Average loss: {test_loss:.4f}, Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Evaluate the model\n",
    "# (This would involve using a separate validation set or performing cross-validation)\n",
    "print(test_loss)\n",
    "\n",
    "mlflow.log_metric('Post training test loss',test_loss)\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff24efc-8629-4242-8cce-6311cb1f9192",
   "metadata": {},
   "source": [
    "# Batch Training - on all trainable data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "08519bcf-2bf1-4d39-9f42-8b68b14135ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18391"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6dc2a29b-3b26-47ac-a881-2cf4ef13b777",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ds = ColFDataset(data,encoder)\n",
    "all_data_loader = DataLoader(train_ds,batch_size=BATCH_SIZE,shuffle=False,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0d1d344e-f48f-4ee2-ab94-b470f2159b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model =  MLPCollaborativeFilter(num_users + 1, num_movies + 1, embedding_dim=FEATURES)\n",
    "optimiser = optim.SGD(final_model.parameters(), lr=LEARNING_RATE,weight_decay=L2_REGULARIZATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "31c8ce29-174d-4687-9476-a996d5a9ccfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optional: Set MLflow experiment\n",
    "mlflow.set_experiment(\"Finetuned Collaborative Filter\")\n",
    "# Log model parameters (example)\n",
    "mlflow.start_run()\n",
    "mlflow.log_param(\"epochs\", EPOCHS)\n",
    "mlflow.log_param(\"optimizer\", type(optimiser).__name__)\n",
    "mlflow.log_param(\"learning rate\", LEARNING_RATE)\n",
    "mlflow.log_param(\"batch size\", BATCH_SIZE)\n",
    "mlflow.log_param(\"L2 regularization\", L2_REGULARIZATION)\n",
    "mlflow.log_param(\"KFOLDS\", num_folds)\n",
    "mlflow.log_param(\"drop out\", 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b8e36fba-65b3-40c4-ad25-0e6b7b004449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.16.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/james/Desktop/torchex/movie-users/wandb/run-20240312_103659-ooaye7lj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jcrich/collaborative%20filter%20model/runs/ooaye7lj' target=\"_blank\">denim-smoke-39</a></strong> to <a href='https://wandb.ai/jcrich/collaborative%20filter%20model' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jcrich/collaborative%20filter%20model' target=\"_blank\">https://wandb.ai/jcrich/collaborative%20filter%20model</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jcrich/collaborative%20filter%20model/runs/ooaye7lj' target=\"_blank\">https://wandb.ai/jcrich/collaborative%20filter%20model/runs/ooaye7lj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/jcrich/collaborative%20filter%20model/runs/ooaye7lj?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f6de18fa800>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    entity=\"jcrich\",\n",
    "    project=\"collaborative filter model\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"architecture\": \"collaborative filter\",\n",
    "    \"dataset\": \"letterboxd\",\n",
    "    \"epochs\": EPOCHS,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "19e1b550-c0e4-41e8-b28a-e56c79d42e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 133.20it/s]\n",
      "Epoch 1: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 183.06it/s]\n",
      "Epoch 2: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 168.00it/s]\n",
      "Epoch 3: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 180.18it/s]\n",
      "Epoch 4: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 179.47it/s]\n",
      "Epoch 5: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 173.65it/s]\n",
      "Epoch 6: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 160.60it/s]\n",
      "Epoch 7: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 167.01it/s]\n",
      "Epoch 8: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 186.68it/s]\n",
      "Epoch 9: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 197.09it/s]\n",
      "Epoch 10: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 202.80it/s]\n",
      "Epoch 11: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 224.32it/s]\n",
      "Epoch 12: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 216.09it/s]\n",
      "Epoch 13: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 230.57it/s]\n",
      "Epoch 14: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 262.69it/s]\n",
      "Epoch 15: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 261.78it/s]\n",
      "Epoch 16: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 251.15it/s]\n",
      "Epoch 17: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 244.05it/s]\n",
      "Epoch 18: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 262.32it/s]\n",
      "Epoch 19: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 257.68it/s]\n",
      "Epoch 20: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 264.16it/s]\n",
      "Epoch 21: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 269.31it/s]\n",
      "Epoch 22: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 277.15it/s]\n",
      "Epoch 23: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 260.44it/s]\n",
      "Epoch 24: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 235.03it/s]\n",
      "Epoch 25: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 257.30it/s]\n",
      "Epoch 26: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 261.94it/s]\n",
      "Epoch 27: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 273.20it/s]\n",
      "Epoch 28: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 253.00it/s]\n",
      "Epoch 29: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 257.07it/s]\n",
      "Epoch 30: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 287.03it/s]\n",
      "Epoch 31: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 274.69it/s]\n",
      "Epoch 32: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 238.33it/s]\n",
      "Epoch 33: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 237.43it/s]\n",
      "Epoch 34: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 252.21it/s]\n",
      "Epoch 35: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 241.70it/s]\n",
      "Epoch 36: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 281.17it/s]\n",
      "Epoch 37: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 260.20it/s]\n",
      "Epoch 38: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 248.87it/s]\n",
      "Epoch 39: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 248.91it/s]\n",
      "Epoch 40: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 250.57it/s]\n",
      "Epoch 41: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 250.88it/s]\n",
      "Epoch 42: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 267.23it/s]\n",
      "Epoch 43: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 264.35it/s]\n",
      "Epoch 44: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 250.79it/s]\n",
      "Epoch 45: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 267.29it/s]\n",
      "Epoch 46: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 279.17it/s]\n",
      "Epoch 47: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 245.52it/s]\n",
      "Epoch 48: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 293.66it/s]\n",
      "Epoch 49: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 259.94it/s]\n",
      "Epoch 50: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 243.19it/s]\n",
      "Epoch 51: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 264.52it/s]\n",
      "Epoch 52: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 263.23it/s]\n",
      "Epoch 53: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 274.09it/s]\n",
      "Epoch 54: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 284.80it/s]\n",
      "Epoch 55: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 274.08it/s]\n",
      "Epoch 56: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 275.32it/s]\n",
      "Epoch 57: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 290.32it/s]\n",
      "Epoch 58: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 295.78it/s]\n",
      "Epoch 59: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 275.82it/s]\n",
      "Epoch 60: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 292.98it/s]\n",
      "Epoch 61: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 252.08it/s]\n",
      "Epoch 62: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 235.55it/s]\n",
      "Epoch 63: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 225.88it/s]\n",
      "Epoch 64: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 247.19it/s]\n",
      "Epoch 65: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 256.64it/s]\n",
      "Epoch 66: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 250.84it/s]\n",
      "Epoch 67: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 247.65it/s]\n",
      "Epoch 68: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 213.54it/s]\n",
      "Epoch 69: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 219.42it/s]\n",
      "Epoch 70: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 272.17it/s]\n",
      "Epoch 71: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 281.86it/s]\n",
      "Epoch 72: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 256.01it/s]\n",
      "Epoch 73: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 246.90it/s]\n",
      "Epoch 74: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 236.59it/s]\n",
      "Epoch 75: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 253.26it/s]\n",
      "Epoch 76: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 272.87it/s]\n",
      "Epoch 77: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 272.69it/s]\n",
      "Epoch 78: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 283.88it/s]\n",
      "Epoch 79: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 249.85it/s]\n",
      "Epoch 80: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 233.17it/s]\n",
      "Epoch 81: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 275.59it/s]\n",
      "Epoch 82: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 285.96it/s]\n",
      "Epoch 83: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 275.84it/s]\n",
      "Epoch 84: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 239.26it/s]\n",
      "Epoch 85: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 259.06it/s]\n",
      "Epoch 86: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 259.86it/s]\n",
      "Epoch 87: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 267.94it/s]\n",
      "Epoch 88: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 265.49it/s]\n",
      "Epoch 89: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 258.73it/s]\n",
      "Epoch 90: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 264.35it/s]\n",
      "Epoch 91: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 265.66it/s]\n",
      "Epoch 92: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 237.08it/s]\n",
      "Epoch 93: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 232.82it/s]\n",
      "Epoch 94: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 273.71it/s]\n",
      "Epoch 95: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 258.21it/s]\n",
      "Epoch 96: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 267.13it/s]\n",
      "Epoch 97: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 253.73it/s]\n",
      "Epoch 98: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 245.82it/s]\n",
      "Epoch 99: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 274.66it/s]\n",
      "Epoch 100: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 279.29it/s]\n",
      "Epoch 101: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 291.62it/s]\n",
      "Epoch 102: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 269.93it/s]\n",
      "Epoch 103: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 248.56it/s]\n",
      "Epoch 104: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 253.48it/s]\n",
      "Epoch 105: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 254.45it/s]\n",
      "Epoch 106: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 273.91it/s]\n",
      "Epoch 107: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 265.32it/s]\n",
      "Epoch 108: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 257.71it/s]\n",
      "Epoch 109: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 244.32it/s]\n",
      "Epoch 110: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 270.94it/s]\n",
      "Epoch 111: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 278.77it/s]\n",
      "Epoch 112: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 278.10it/s]\n",
      "Epoch 113: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 266.57it/s]\n",
      "Epoch 114: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 243.47it/s]\n",
      "Epoch 115: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 247.08it/s]\n",
      "Epoch 116: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 239.87it/s]\n",
      "Epoch 117: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 231.34it/s]\n",
      "Epoch 118: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 245.80it/s]\n",
      "Epoch 119: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 247.92it/s]\n",
      "Epoch 120: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 247.02it/s]\n",
      "Epoch 121: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 260.64it/s]\n",
      "Epoch 122: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 263.66it/s]\n",
      "Epoch 123: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 279.77it/s]\n",
      "Epoch 124: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 281.47it/s]\n",
      "Epoch 125: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 252.42it/s]\n",
      "Epoch 126: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 254.91it/s]\n",
      "Epoch 127: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 256.78it/s]\n",
      "Epoch 128: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 271.94it/s]\n",
      "Epoch 129: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 259.29it/s]\n",
      "Epoch 130: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 277.79it/s]\n",
      "Epoch 131: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 277.55it/s]\n",
      "Epoch 132: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 263.73it/s]\n",
      "Epoch 133: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 260.21it/s]\n",
      "Epoch 134: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 234.89it/s]\n",
      "Epoch 135: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 264.91it/s]\n",
      "Epoch 136: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 268.49it/s]\n",
      "Epoch 137: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 260.34it/s]\n",
      "Epoch 138: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 263.17it/s]\n",
      "Epoch 139: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 256.00it/s]\n",
      "Epoch 140: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 252.19it/s]\n",
      "Epoch 141: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 229.91it/s]\n",
      "Epoch 142: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 253.97it/s]\n",
      "Epoch 143: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 265.45it/s]\n",
      "Epoch 144: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 271.91it/s]\n",
      "Epoch 145: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 268.04it/s]\n",
      "Epoch 146: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 265.60it/s]\n",
      "Epoch 147: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 279.73it/s]\n",
      "Epoch 148: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 279.82it/s]\n",
      "Epoch 149: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 265.60it/s]\n",
      "Epoch 150: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 250.66it/s]\n",
      "Epoch 151: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 263.75it/s]\n",
      "Epoch 152: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 244.40it/s]\n",
      "Epoch 153: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 268.80it/s]\n",
      "Epoch 154: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 267.51it/s]\n",
      "Epoch 155: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 245.43it/s]\n",
      "Epoch 156: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 271.31it/s]\n",
      "Epoch 157: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 276.80it/s]\n",
      "Epoch 158: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 268.95it/s]\n",
      "Epoch 159: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 252.63it/s]\n",
      "Epoch 160: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 242.47it/s]\n",
      "Epoch 161: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 203.74it/s]\n",
      "Epoch 162: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 243.39it/s]\n",
      "Epoch 163: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 248.13it/s]\n",
      "Epoch 164: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 267.58it/s]\n",
      "Epoch 165: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 220.54it/s]\n",
      "Epoch 166: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 242.27it/s]\n",
      "Epoch 167: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 251.92it/s]\n",
      "Epoch 168: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 250.01it/s]\n",
      "Epoch 169: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 251.56it/s]\n",
      "Epoch 170: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 266.33it/s]\n",
      "Epoch 171: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 264.68it/s]\n",
      "Epoch 172: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 259.19it/s]\n",
      "Epoch 173: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 256.58it/s]\n",
      "Epoch 174: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 253.38it/s]\n",
      "Epoch 175: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 262.70it/s]\n",
      "Epoch 176: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 263.56it/s]\n",
      "Epoch 177: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 258.87it/s]\n",
      "Epoch 178: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 275.61it/s]\n",
      "Epoch 179: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 274.34it/s]\n",
      "Epoch 180: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 282.04it/s]\n",
      "Epoch 181: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 274.25it/s]\n",
      "Epoch 182: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 262.39it/s]\n",
      "Epoch 183: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 254.66it/s]\n",
      "Epoch 184: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 263.03it/s]\n",
      "Epoch 185: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 263.54it/s]\n",
      "Epoch 186: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 253.56it/s]\n",
      "Epoch 187: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 258.80it/s]\n",
      "Epoch 188: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 259.60it/s]\n",
      "Epoch 189: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 264.76it/s]\n",
      "Epoch 190: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 241.78it/s]\n",
      "Epoch 191: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 242.20it/s]\n",
      "Epoch 192: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 272.19it/s]\n",
      "Epoch 193: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 267.26it/s]\n",
      "Epoch 194: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 266.58it/s]\n",
      "Epoch 195: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 243.77it/s]\n",
      "Epoch 196: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 260.50it/s]\n",
      "Epoch 197: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 254.17it/s]\n",
      "Epoch 198: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 263.42it/s]\n",
      "Epoch 199: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 267.73it/s]\n",
      "Epoch 200: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 250.09it/s]\n",
      "Epoch 201: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 263.98it/s]\n",
      "Epoch 202: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 258.52it/s]\n",
      "Epoch 203: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 261.07it/s]\n",
      "Epoch 204: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 246.48it/s]\n",
      "Epoch 205: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 251.71it/s]\n",
      "Epoch 206: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 253.15it/s]\n",
      "Epoch 207: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 258.09it/s]\n",
      "Epoch 208: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 247.08it/s]\n",
      "Epoch 209: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 246.72it/s]\n",
      "Epoch 210: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 285.73it/s]\n",
      "Epoch 211: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 254.81it/s]\n",
      "Epoch 212: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 240.60it/s]\n",
      "Epoch 213: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 236.85it/s]\n",
      "Epoch 214: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 233.25it/s]\n",
      "Epoch 215: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 249.75it/s]\n",
      "Epoch 216: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 250.67it/s]\n",
      "Epoch 217: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 258.03it/s]\n",
      "Epoch 218: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 269.53it/s]\n",
      "Epoch 219: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 238.90it/s]\n",
      "Epoch 220: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 249.35it/s]\n",
      "Epoch 221: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 242.15it/s]\n",
      "Epoch 222: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 227.74it/s]\n",
      "Epoch 223: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 244.52it/s]\n",
      "Epoch 224: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 246.87it/s]\n",
      "Epoch 225: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 240.84it/s]\n",
      "Epoch 226: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 238.63it/s]\n",
      "Epoch 227: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 239.37it/s]\n",
      "Epoch 228: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 233.03it/s]\n",
      "Epoch 229: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 236.08it/s]\n",
      "Epoch 230: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 232.55it/s]\n",
      "Epoch 231: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 224.19it/s]\n",
      "Epoch 232: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 238.28it/s]\n",
      "Epoch 233: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 271.90it/s]\n",
      "Epoch 234: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 259.78it/s]\n",
      "Epoch 235: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 253.37it/s]\n",
      "Epoch 236: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 253.16it/s]\n",
      "Epoch 237: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 234.82it/s]\n",
      "Epoch 238: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 244.38it/s]\n",
      "Epoch 239: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 263.62it/s]\n",
      "Epoch 240: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 261.96it/s]\n",
      "Epoch 241: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 257.63it/s]\n",
      "Epoch 242: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 245.75it/s]\n",
      "Epoch 243: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 248.44it/s]\n",
      "Epoch 244: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 254.97it/s]\n",
      "Epoch 245: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 256.04it/s]\n",
      "Epoch 246: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 275.19it/s]\n",
      "Epoch 247: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 263.30it/s]\n",
      "Epoch 248: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 253.17it/s]\n",
      "Epoch 249: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 249.65it/s]\n",
      "Epoch 250: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 236.98it/s]\n",
      "Epoch 251: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 250.96it/s]\n",
      "Epoch 252: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 263.92it/s]\n",
      "Epoch 253: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 273.56it/s]\n",
      "Epoch 254: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 270.55it/s]\n",
      "Epoch 255: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 285.18it/s]\n",
      "Epoch 256: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 271.61it/s]\n",
      "Epoch 257: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 271.83it/s]\n",
      "Epoch 258: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 274.25it/s]\n",
      "Epoch 259: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 293.16it/s]\n",
      "Epoch 260: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 268.87it/s]\n",
      "Epoch 261: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 241.47it/s]\n",
      "Epoch 262: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 245.33it/s]\n",
      "Epoch 263: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 259.89it/s]\n",
      "Epoch 264: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 293.66it/s]\n",
      "Epoch 265: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 246.52it/s]\n",
      "Epoch 266: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 277.11it/s]\n",
      "Epoch 267: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 268.60it/s]\n",
      "Epoch 268: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 273.49it/s]\n",
      "Epoch 269: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 289.25it/s]\n",
      "Epoch 270: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 298.35it/s]\n",
      "Epoch 271: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 294.56it/s]\n",
      "Epoch 272: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 281.88it/s]\n",
      "Epoch 273: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 261.96it/s]\n",
      "Epoch 274: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 253.22it/s]\n",
      "Epoch 275: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 264.27it/s]\n",
      "Epoch 276: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 274.76it/s]\n",
      "Epoch 277: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 277.31it/s]\n",
      "Epoch 278: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 271.02it/s]\n",
      "Epoch 279: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 275.56it/s]\n",
      "Epoch 280: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 270.86it/s]\n",
      "Epoch 281: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 265.18it/s]\n",
      "Epoch 282: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 260.45it/s]\n",
      "Epoch 283: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 261.56it/s]\n",
      "Epoch 284: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 261.31it/s]\n",
      "Epoch 285: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 255.79it/s]\n",
      "Epoch 286: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 243.91it/s]\n",
      "Epoch 287: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 245.90it/s]\n",
      "Epoch 288: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 273.37it/s]\n",
      "Epoch 289: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 266.44it/s]\n",
      "Epoch 290: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 276.57it/s]\n",
      "Epoch 291: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 270.42it/s]\n",
      "Epoch 292: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 241.01it/s]\n",
      "Epoch 293: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 242.77it/s]\n",
      "Epoch 294: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 253.37it/s]\n",
      "Epoch 295: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 260.15it/s]\n",
      "Epoch 296: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 250.24it/s]\n",
      "Epoch 297: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 259.75it/s]\n",
      "Epoch 298: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 253.29it/s]\n",
      "Epoch 299: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 260.17it/s]\n",
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Avg. Training Loss</td><td>█▆▄▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Avg. Training Loss</td><td>0.0347</td></tr><tr><td>Epoch</td><td>299</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">denim-smoke-39</strong> at: <a href='https://wandb.ai/jcrich/collaborative%20filter%20model/runs/ooaye7lj' target=\"_blank\">https://wandb.ai/jcrich/collaborative%20filter%20model/runs/ooaye7lj</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240312_103659-ooaye7lj/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize lists to keep track of losses and epochs\n",
    "# Initialize MLflow run\n",
    "# Initialize MLflow run\n",
    "# Initialize MLflow run\n",
    "\n",
    "# Ensure this is the model you intend to train\n",
    "model = final_model\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()  # Correct model reference for training mode\n",
    "    total_epoch_loss = 0\n",
    "    for batch_idx, (user_ids, movie_ids, ratings) in tqdm(enumerate(all_data_loader), total=len(all_data_loader), desc=f'Epoch {epoch}'):\n",
    "\n",
    "        optimiser.zero_grad()\n",
    "        predictions = model(user_ids, movie_ids)  # Ensure consistent model reference\n",
    "        loss = criterion(predictions, ratings)\n",
    "\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        \n",
    "        batch_loss = loss.item()\n",
    "        total_epoch_loss += batch_loss\n",
    "        \n",
    "    avg_epoch_loss = total_epoch_loss / len(all_data_loader)\n",
    "    \n",
    "    # Logging\n",
    "    wandb.log({\"Avg. Training Loss\": avg_epoch_loss, \"Epoch\": epoch})\n",
    "    mlflow.log_metric(\"Avg. Training Loss\", avg_epoch_loss, step=epoch)\n",
    "\n",
    "# Finish the Weights & Biases run\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "25d87936-8980-4124-b32c-cc1a4813102b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18391"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7dfed358-d01f-4c10-a71a-04d55bbb4989",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': final_model.state_dict(),\n",
    "    'optimiser_state_dict': optimiser.state_dict(),\n",
    "    'Avg. training loss': avg_epoch_loss,\n",
    "}, 'final_col_model_checkpoint.pth')\n",
    "mlflow.log_artifact('final_col_model_checkpoint.pth')\n",
    "mlflow.pytorch.log_model(final_model, \"model\")\n",
    "# Save model state\n",
    "torch.save(final_model.state_dict(), 'best_model_state.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "63a5e18c-653c-4d10-9bac-b3669a38c95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05759985182891812\n"
     ]
    }
   ],
   "source": [
    "#test data\n",
    "final_model.eval()  # Switch to evaluation mode\n",
    "test_loss = 0\n",
    "correct_predictions = 0  # Fixed variable name for clarity\n",
    "total_contexts = 0\n",
    "\n",
    "with torch.no_grad():  # Disable gradient computation\n",
    "    for user_ids, movie_ids, ratings in test_data_loader:\n",
    "        predictions = final_model(user_ids,movie_ids)  # Generate predictions\n",
    "        # Calculate and accumulate loss\n",
    "        loss = criterion(predictions, ratings)\n",
    "        test_loss += loss.item()\n",
    "        \n",
    "        # # Reshape predictions to match [batch_size, context_size, vocab_size]\n",
    "        # predictions = predictions.view(-1, context_size, VOCAB_SIZE)\n",
    "        \n",
    "        # # Get top prediction for each context position\n",
    "        # top_predictions = predictions.argmax(dim=2)\n",
    "        \n",
    "        # # Calculate correct predictions\n",
    "        # correct_preds = (top_predictions == context).float().sum()\n",
    "        # correct_predictions += correct_preds.item()  # Accumulate correct predictions\n",
    "        \n",
    "        # total_contexts += context.numel()  # Total number of context word positions evaluated\n",
    "\n",
    "# Calculate final metrics\n",
    "test_loss /= len(test_data_loader)  # Average loss per batch\n",
    "# print('correct predictions = ',correct_predictions)\n",
    "# print('out of  = ',total_contexts)\n",
    "# accuracy = correct_predictions / total_contexts  # Compute accuracy\n",
    "\n",
    "# print(f'Test set: Average loss: {test_loss:.4f}, Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Evaluate the model\n",
    "# (This would involve using a separate validation set or performing cross-validation)\n",
    "print(test_loss)\n",
    "\n",
    "mlflow.log_metric('Post training test loss',test_loss)\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10373ba-5557-46bf-aa37-8f4234b43def",
   "metadata": {},
   "source": [
    "## Preference ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377c97ca-a1c8-4a79-ad7f-f844c2b6ab59",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_user_id = 140440102666832"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb9de61-95a2-4c4d-bae9-78f17254013c",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_uid = encoder.encode(target_user_id,encoder.user_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb44d161-b86c-48e5-a2a8-228ee34d61fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_features(uid):\n",
    "\n",
    "    for i in range(0,len(all_ds)):\n",
    "\n",
    "        u,m,r = all_ds[i]\n",
    "        # print(u)\n",
    "        if u == uid:\n",
    "            yield [m,r]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca61297-1afc-439e-9a3a-81bff33d384f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ratings(e_uid,e_mid,model):\n",
    "    e_uid_tensor = torch.tensor(e_uid, dtype=torch.int64).unsqueeze(0)\n",
    "    movie_eid_tensor = torch.tensor(e_mid,dtype=torch.int64).unsqueeze(0)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        user_rating = model(e_uid_tensor,movie_eid_tensor)\n",
    "        return movie_eid_tensor,user_rating\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3780e9b-1c32-4f2b-a45e-14bdb9d35b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get user and features\n",
    "seen_features = set(tuple(fe) for fe in get_user_features(e_uid)) # Get user features if needed\n",
    "seen_movies = set(sf[0].item() for sf in seen_features)\n",
    "all_movies = set([num for num in range(30)])\n",
    "unseen_movies = all_movies - seen_movies\n",
    "unseen_features = {tuple(predict_ratings(e_uid,um,model)) for um in unseen_movies}\n",
    "all_features = seen_features | unseen_features\n",
    "\n",
    "sorted_set = sorted(all_features, key=lambda x: x[1])\n",
    "top_n = 10\n",
    "recommendations = list(filter(lambda x: x[0].item() in unseen_movies,sorted_set))\n",
    "n_recommendations = list(map(lambda x: x[0],recommendations[-1:top_n*-1:-1]))\n",
    "n_recommendations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3470ac7f-df21-40aa-94aa-32aa93606c9a",
   "metadata": {},
   "source": [
    "# Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9289eb5-57c3-42bd-ba7f-7c519590d8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all users, with all movie ratings.\n",
    "def generate_all_ratings(model,num_users,num_movies):\n",
    "    #build matrix\n",
    "\n",
    "    #data already encoded....\n",
    "    all_data = [['dummy',mnm,unm,'dummy','0/10'] for unm in range(0,num_users) for mnm in range(0,num_movies)]\n",
    "    # print(all_data)\n",
    "    # print([em for em in all_data])\n",
    "    fake_encoder = Encoder([did[2] for did in all_data],[did[1] for did in all_data])\n",
    "    all_ds = CFDataset(all_data,fake_encoder)\n",
    "    \n",
    "    data_loader = DataLoader(all_ds, batch_size=1)\n",
    "    for uid,mid,ra in data_loader:\n",
    "        # print(uid,mid)\n",
    "        prediction = model(uid,mid)\n",
    "        yield (uid.item(),mid.item(),prediction.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788b3ed2-c265-4b06-a5b9-53fa0c63b904",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ratings_tensor(data, num_users, num_movies):\n",
    "    # Initialize an empty tensor to hold the ratings\n",
    "    ratings_tensor = torch.zeros(num_users, num_movies)\n",
    "    \n",
    "    # Iterate over the data and fill the tensor\n",
    "    for entry in data:\n",
    "        user_id, movie_id, rating = entry\n",
    "        # Convert rating to float\n",
    "        rating = float(rating)\n",
    "        ratings_tensor[user_id, movie_id] = rating\n",
    "    \n",
    "    return ratings_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd12391-a846-475e-be55-b2516a0ef7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Generate Predictions\n",
    "predictions = [ra for ra in generate_all_ratings(model,num_users,num_movies)]\n",
    "# print(predictions)\n",
    "ratings_tensor = create_ratings_tensor(predictions,num_users,num_movies)\n",
    "print(ratings_tensor)\n",
    "# predictions[0]\n",
    "\n",
    "# ratings_tensor.shape\n",
    "\n",
    "# ratings_tensor[0]\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Assuming predicted_ratings is your tensor of predicted ratings\n",
    "# predicted_ratings.shape should be (num_users, num_movies)\n",
    "\n",
    "# # Calculate cosine similarity\n",
    "# user_similarities = F.cosine_similarity(ratings_tensor, ratings_tensor, dim=1)\n",
    "\n",
    "# user_similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d0c474-3481-489e-b494-047c64b14925",
   "metadata": {},
   "source": [
    "## COSINE SIMILARITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cae1de-bc62-460e-b2db-f046961f4493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Normalize predicted ratings\n",
    "normalized_ratings = F.normalize(ratings_tensor, p=2, dim=1)\n",
    "\n",
    "# Step 2: Calculate cosine similarity\n",
    "user_similarities = torch.matmul(normalized_ratings, normalized_ratings.T)\n",
    "\n",
    "# Set diagonal elements to a large negative value to exclude self-similarity\n",
    "user_similarities.fill_diagonal_(-float('inf'))\n",
    "\n",
    "# You can optionally convert the similarities tensor to a numpy array for easier manipulation\n",
    "user_similarities_np = user_similarities.numpy()\n",
    "\n",
    "# Print or use the user similarities tensor\n",
    "print(user_similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb89e2b-202f-405a-a732-26cf09959925",
   "metadata": {},
   "source": [
    "### Matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e402f0-b39d-4599-9617-5fcf23d00a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_matches = {}\n",
    "for i in range(len(user_similarities)):\n",
    "    # Sort similarities for the current user i\n",
    "    ranked_users = torch.argsort(torch.tensor(user_similarities[i]), descending=True)\n",
    "    # Exclude self from top matches (optional)\n",
    "    top_matches[i] = ranked_users[ranked_users != i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d560016-14da-4170-be81-43e27b094043",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebc2399-8831-4bef-b722-b9405c776b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.decode(71,encoder.idx_to_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cb1dfe-e30f-407f-accb-367b5bce66a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#decoded\n",
    "\n",
    "decoded_matches = {encoder.decode(k,encoder.idx_to_user):[encoder.decode(ve,encoder.idx_to_user) for ve in v.tolist()] for k,v in top_matches.items()}\n",
    "decoded_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73632e60-b177-479e-8e78-c6eea82ea863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Select Top Matches\n",
    "N = 5  # Number of top matches to select\n",
    "for user, similar_users in decoded_matches.items():\n",
    "    decoded_matches[user] = similar_users[:N]\n",
    "\n",
    "decoded_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7f3715-96e0-44a4-9a49-eee4e94bfaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#decode...."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c7c0fa-b24f-4418-b5ca-e1713254be27",
   "metadata": {},
   "source": [
    "## FINALLY FIND THE MATCHES...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd884b67-4541-453a-8bdf-87adf9d4ca39",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_user = 140440102666832\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0929f29-14e7-4cfa-acc9-9a741b7a5187",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_matches[target_user]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ed138f-43a0-4e9c-a66d-c7030de6e662",
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbour = 140440115331424"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a6a7ef-f019-4273-bc7c-c63dae3a104d",
   "metadata": {},
   "source": [
    "## Now see how they relate to the database..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b89fe2d-944d-43bc-b2a2-81af64bd6ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = db.get_table_values('Reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df7b55f-d93f-4a8d-b4c3-f0b1ea9ddf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2b3f3b-a7f8-48a6-b772-23c522f5ab85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reviews_info(user_id,reviews):\n",
    "    # print(db.get_table_values('Users'))\n",
    "    info = {}\n",
    "    info[user_id] = list(filter(lambda x :x[0] == user_id,db.get_table_values('Users')))[0][1]\n",
    "\n",
    "    \n",
    "    #TODO finish work...\n",
    "    mov_tab = db.get_table_values('Movies')\n",
    "    \n",
    "    for review in reviews:\n",
    "        mov_id = review[1]\n",
    "        info[mov_id] = {'title':list(filter(lambda x :x[0] == mov_id,mov_tab))[0][1],'rating':review[4]}\n",
    "        \n",
    "    return info\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef46e616-1299-4659-8e09-f6d3fdf44b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89593c73-41e3-4a6e-8e9c-61fd41b372dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_reviews = list(filter(lambda rv: rv[2] == target_user,reviews))\n",
    "neighbour_reviews = list(filter(lambda rv: rv[2] == neighbour,reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fd7bda-4f06-41de-89d2-7e84ce851863",
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_info = reviews_info(target_user,target_reviews)\n",
    "neigh_info = reviews_info(neighbour,neighbour_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cf7940-a7bf-4443-87b6-abd8d7d47bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78c8ccb-97d3-439a-ac8d-28ebe5e8ba91",
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6864fda6-f897-4c0a-8cf8-405e8cadd208",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae878490-881c-4345-9040-934a38615e40",
   "metadata": {},
   "source": [
    "# SAVE_MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef1a3d5-a607-4a92-82a3-237a07f15ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653f0e2d-f90d-4bc4-b72c-2dfda1b2f542",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "menv",
   "language": "python",
   "name": "menv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
