{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a573191-c50c-46a7-be39-8d9e9ad5ae8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from db.ipynb\n",
      "best_contentfiltermodel_checkpoint.pth\tmenv\n",
      "best_model_checkpoint.pth\t\tmlruns\n",
      "best_model_state.pth\t\t\tmodels\n",
      "cf_model.py\t\t\t\tmovie.py\n",
      "config.py\t\t\t\tpath_to_your_database.db\n",
      "dataset.py\t\t\t\t__pycache__\n",
      "db.ipynb\t\t\t\tq.py\n",
      "Dockerfile\t\t\t\tREADME.md\n",
      "done.txt\t\t\t\trequirements.txt\n",
      "fdstests.ipynb\t\t\t\tserver\n",
      "filemanager.py\t\t\t\ttest.db\n",
      "holocenemodels\t\t\t\ttestsources.ipynb\n",
      "indie_letterboxd.db\t\t\ttodo.txt\n",
      "indie_letterboxd_v2.db\t\t\ttrain_colfilter.ipynb\n",
      "letterboxd\t\t\t\ttrain_confilter.ipynb\n",
      "letterboxd.db\t\t\t\tuser.py\n",
      "main.ipynb\t\t\t\twandb\n",
      "main.py\t\t\t\t\tweb_spider.py\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from cf_model import MLPCollaborativeFilter\n",
    "from dataset import ColFDataset, Encoder, split_data\n",
    "import import_ipynb\n",
    "import db\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4967b732-0b9e-4747-a84f-07e3e8f4dd71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users: 2400\n",
      "\n",
      "Movies: 771\n",
      "\n",
      "Reviews: 18504\n"
     ]
    }
   ],
   "source": [
    "DB_NAME = 'indie_letterboxd_v2'\n",
    "db.display_size(DB_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28be6c6f-cf28-486e-9aa5-78b8ff780a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(DB_NAME+'.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# SQL query to fetch user name and ID, movie name and ID, and rating from reviews\n",
    "query = '''\n",
    "    SELECT \n",
    "        u.user_id,\n",
    "        u.name AS user_name, \n",
    "        m.movie_id,\n",
    "        m.title AS movie_title, \n",
    "        r.rating\n",
    "    FROM \n",
    "        Reviews r\n",
    "    JOIN Users u ON r.user_id = u.user_id\n",
    "    JOIN Movies m ON r.movie_id = m.movie_id\n",
    "    '''\n",
    "\n",
    "try:\n",
    "    cursor.execute(query)\n",
    "    reviews = cursor.fetchall()\n",
    "    \n",
    "    # for review in reviews:\n",
    "    #     print(\"User ID:\", review[0], \"| User Name:\", review[1], \"| Movie ID:\", review[2], \"| Movie Title:\", review[3], \"| Rating:\", review[4])\n",
    "except sqlite3.Error as e:\n",
    "    print(\"An error occurred:\", e)\n",
    "finally:\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8802ab01-9d57-4fdf-8341-215f3e0212ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Callum Kaur', 'So Much Tenderness', 0.76), ('Leila Petrauskas', 'Terrorizers', 0.8), ('Leah Foster', 'The Rye Horn', 0.7), ('مهدیس روحانی', 'Dance First', 0.5), ('السيدة ربى عليان', 'A Hero', 0.9), ('श्रीमती सजना गुप्\\u200dता', 'Zwigato', 0.72), ('Jessica Roman', 'Mountain Cat', 0.7), ('Daniela Urbanová', 'Saint Omer', 0.7), ('Kavaliauskas, Perla', 'Here We Are', 0.7), ('Natalia Agustina, S.IP', 'The Goldfinch', 0.4)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(db.display_size())\n",
    "#TODO change shuffle and not shuffle\n",
    "\n",
    "# Assuming 'reviews' is a list of tuples and you've already created 'data'\n",
    "data = [tuple([did[1], did[3], did[-1]]) for did in reviews]\n",
    "\n",
    "# Shuffle 'data' in place with random.shuffle()\n",
    "random.shuffle(data)\n",
    "\n",
    "# Now 'data' is shuffled, and you can work with it\n",
    "print(data[:10])\n",
    "#found invalid values earlier.\n",
    "\n",
    "data = [pre for pre in data if pre[-1] > 0]\n",
    "data.count(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a8d4986-83f4-4fbc-8bba-429cda6a865e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test data is holdout for Kfolds cross data validation evaluation.\n",
    "train_data, test_data = split_data(data)\n",
    "train_data, validation_data = split_data(train_data,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2eccd133-b107-4dfa-9b16-b07bc58b375b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13241"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2438744-302d-4f6b-9a63-d49b2eadbd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_users_data = [ud[0] for ud in train_data]\n",
    "train_movies_data = [md[1] for md in train_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1b2649-c1fb-48bd-ab55-030b640199e7",
   "metadata": {},
   "source": [
    "## after splitting, only use the training data --- for encoding!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfbc8886-5861-4526-9488-ed53e502a2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(users=train_users_data,movies=train_movies_data)###<---- important\n",
    "import pickle\n",
    "\n",
    "# Assuming 'encoder' is your encoder object\n",
    "with open('encoder.pkl', 'wb') as file:\n",
    "    pickle.dump(encoder, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "319e8a9c-44a3-44d4-99c7-ad3b042220e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ColFDataset(train_data,encoder)\n",
    "test_ds = ColFDataset(test_data,encoder)\n",
    "validation_ds = ColFDataset(validation_data,encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af64661d-4749-45e3-a3a9-dcfdb27b43ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1471"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds.movie_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a97f9cb-f0f1-45db-b175-4293529fa52d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.movie_ids.count(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae3212ab-ac1f-4efa-b29a-a509247f08d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13241"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validation_ds.movie_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75750116-45b4-4af9-be6e-d3b828a05311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6989"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_ds.user_ids.count(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2a65e0d-f096-4312-b02e-2ccbbb1a38af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1911"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_ds.movie_ids.count(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cbc125dc-8fca-445a-8f90-ea504b004900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3679"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_ds.movie_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83a82ced-863d-4bd4-b00d-a99a24047c89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds.movie_ids.count(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75616c10-0c39-460b-a33d-9507b9043ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "train_data_loader = DataLoader(train_ds,batch_size=BATCH_SIZE,shuffle=False,drop_last=True)\n",
    "validation_data_loader = DataLoader(validation_ds,batch_size=BATCH_SIZE,shuffle=False,drop_last=True)\n",
    "test_data_loader = DataLoader(test_ds,batch_size=BATCH_SIZE,shuffle=False,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36edabff-0087-49fd-9cbf-c22e06c27b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users = len(encoder.vocab_to_idx['users'])\n",
    "num_movies = len(encoder.vocab_to_idx['movies'])\n",
    "FEATURES=700\n",
    "model = MLPCollaborativeFilter(num_users + 1, num_movies + 1, embedding_dim=FEATURES)\n",
    "\n",
    "\n",
    "LEARNING_RATE = 0.001\n",
    "criterion = nn.MSELoss()\n",
    "#weight decay L2 regularization\n",
    "# optimiser = optim.SGD(model.parameters(), lr=0.001,weight_decay=1e-5)\\\n",
    "L2_REGULARIZATION=0.1\n",
    "optimiser = optim.SGD(model.parameters(), lr=LEARNING_RATE,weight_decay=L2_REGULARIZATION)\n",
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "157cd204-76fe-473a-9e57-a7adda48b185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18391\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1471"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(data))\n",
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0aea2d-2597-4a50-9b6d-d65231ed49eb",
   "metadata": {},
   "source": [
    "# EDA "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadfc0f4-b4df-4835-85b1-ebae07f11741",
   "metadata": {},
   "source": [
    "    Examine the data for patterns, anomalies, or characteristics.\n",
    "    Check for data quality issues such as missing values, outliers, or incorrect data types. <--- either predropped or imputed.\n",
    "    Get a sense of the distributions of your variables and the relationships between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1228065-7071-4b93-b460-a4204e6fbbda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               user               movie  ratings\n",
      "0       Callum Kaur  So Much Tenderness     0.76\n",
      "1  Leila Petrauskas         Terrorizers     0.80\n",
      "2       Leah Foster        The Rye Horn     0.70\n",
      "3      مهدیس روحانی         Dance First     0.50\n",
      "4  السيدة ربى عليان              A Hero     0.90\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 18391 entries, 0 to 18390\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   user     18391 non-null  object \n",
      " 1   movie    18391 non-null  object \n",
      " 2   ratings  18391 non-null  float64\n",
      "dtypes: float64(1), object(2)\n",
      "memory usage: 431.2+ KB\n",
      "None\n",
      "            ratings\n",
      "count  18391.000000\n",
      "mean       0.674751\n",
      "std        0.210540\n",
      "min        0.020000\n",
      "25%        0.540000\n",
      "50%        0.700000\n",
      "75%        0.800000\n",
      "max        1.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "column_names = ['user', 'movie','ratings']\n",
    "\n",
    "df = pd.DataFrame(data, columns=column_names)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(df.head())\n",
    "\n",
    "# Get a summary of the dataset\n",
    "print(df.info())\n",
    "\n",
    "# Generate descriptive statistics\n",
    "print(df.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c02bbd90-8966-44b8-9ddd-59d31ce3d82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user       0\n",
      "movie      0\n",
      "ratings    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "51da5313-6bec-42a9-835e-2c535197d382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x1500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 250x250 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Histograms for numeric features\n",
    "df.hist(bins=50, figsize=(20,15))\n",
    "plt.show()\n",
    "\n",
    "# Box plots for distributions and spotting outliers\n",
    "sns.boxplot(data=df)\n",
    "plt.show()\n",
    "\n",
    "# Pairplot to visualize pairwise relationships between features\n",
    "sns.pairplot(df)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6b05fd44-31ed-4c75-b0b2-5558d525bd0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>movie</th>\n",
       "      <th>ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [user, movie, ratings]\n",
       "Index: []"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['ratings'] == -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5fe39638-e29b-49ca-bf75-4d8d44ace1c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    18\n",
       "1    11\n",
       "2    12\n",
       "3    11\n",
       "4     6\n",
       "Name: title_length, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['title_length'] = df['movie'].str.len()\n",
    "df['title_length'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6c4fe575-c792-46de-9fac-c4e280e4889f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "df['title_length']\n",
    "# Correlation matrix\n",
    "\n",
    "corr_matrix = df[['title_length','ratings']].corr()\n",
    "\n",
    "# Heatmap of the correlation matrix\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\")\n",
    "plt.show()\n",
    "\n",
    "# Specifically look at correlations with the target variable if defined\n",
    "# target_corr = corr_matrix[\"YourTargetVariable\"].sort_values(ascending=False)\n",
    "# print(target_corr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dc364699-eaa9-4bd5-9e46-359b19c0271f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26\n",
      "                              user                        movie  ratings  \\\n",
      "18                Կիրա Կոշկակարյան         Bigger on the Inside     0.10   \n",
      "38              Claudia Dumitrescu               Simple Passion     0.10   \n",
      "73     Олимпий Гертрудович Кулаков                        Joker     0.10   \n",
      "112                 Fredrik Nilsen                       Zalava     0.10   \n",
      "151                   Stephen Hill                      Liberte     0.10   \n",
      "...                            ...                          ...      ...   \n",
      "18225                 Gijs de Smit  David Foster Off the Record     0.10   \n",
      "18231                 Itzel Amador                        Joker     0.10   \n",
      "18240             Christian Berger                       Memory     0.10   \n",
      "18276         Βαλάντης Χαραλάμπους                   Quickening     0.10   \n",
      "18305            Войнка Пенджакова                   Saint Omer     0.06   \n",
      "\n",
      "       title_length  \n",
      "18               20  \n",
      "38               14  \n",
      "73                5  \n",
      "112               6  \n",
      "151               7  \n",
      "...             ...  \n",
      "18225            27  \n",
      "18231             5  \n",
      "18240             6  \n",
      "18276            10  \n",
      "18305            10  \n",
      "\n",
      "[300 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Example of finding outliers for a 'feature_name'\n",
    "Q1 = df['ratings'].quantile(0.25)\n",
    "Q3 = df['ratings'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "print(IQR)\n",
    "outlier_condition = ((df['ratings'] < (Q1 - 1.5 * IQR)) | (df['ratings'] > (Q3 + 1.5 * IQR)))\n",
    "outliers = df[outlier_condition]\n",
    "print(outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5dbea2-ea83-4f1a-b206-60a993873cfe",
   "metadata": {},
   "source": [
    "## MLFLOw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ae300159-0298-450a-b512-ddf65ad3d207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///home/james/Desktop/torchex/movie-users/mlruns/827152334394031969', creation_time=1710099404696, experiment_id='827152334394031969', last_update_time=1710099404696, lifecycle_stage='active', name='Finetuned Collaborative Filter', tags={}>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# Optional: Set MLflow experiment\n",
    "mlflow.set_experiment(\"Finetuned Collaborative Filter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5bebaeb2-02a1-47d8-8f20-28d488b69201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Log model parameters (example)\n",
    "mlflow.start_run()\n",
    "mlflow.log_param(\"epochs\", EPOCHS)\n",
    "mlflow.log_param(\"optimizer\", type(optimiser).__name__)\n",
    "mlflow.log_param(\"learning rate\", LEARNING_RATE)\n",
    "mlflow.log_param(\"batch size\", BATCH_SIZE)\n",
    "mlflow.log_param(\"L2 regularization\", L2_REGULARIZATION)\n",
    "mlflow.log_param(\"drop out\", 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623dc331-6d00-41db-b670-fe7c76a9d10b",
   "metadata": {},
   "source": [
    "# PRETEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1f13953d-a5bc-4fd2-8d48-3c323be56cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43195568678671853\n"
     ]
    }
   ],
   "source": [
    "#test data\n",
    "model.eval()  # Switch to evaluation mode\n",
    "test_loss = 0\n",
    "correct_predictions = 0  # Fixed variable name for clarity\n",
    "total_contexts = 0\n",
    "\n",
    "with torch.no_grad():  # Disable gradient computation\n",
    "    for user_ids, movie_ids, ratings in test_data_loader:\n",
    "        predictions = model(user_ids,movie_ids)  # Generate predictions\n",
    "        # Calculate and accumulate loss\n",
    "        loss = criterion(predictions, ratings)\n",
    "        test_loss += loss.item()\n",
    "        \n",
    "\n",
    "test_loss /= len(test_data_loader)  # Average loss per batch\n",
    "\n",
    "print(test_loss)\n",
    "\n",
    "mlflow.log_metric('pretraining test loss',test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39e0f67-b1d4-4a01-826a-4e37d9fb5127",
   "metadata": {},
   "source": [
    "## wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "000ffd1c-94ac-4276-bede-a4b7f186794f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjcrich\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/james/Desktop/torchex/movie-users/wandb/run-20240311_215042-6dqticeg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jcrich/collaborative%20filter%20model/runs/6dqticeg' target=\"_blank\">skilled-firebrand-30</a></strong> to <a href='https://wandb.ai/jcrich/collaborative%20filter%20model' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jcrich/collaborative%20filter%20model' target=\"_blank\">https://wandb.ai/jcrich/collaborative%20filter%20model</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jcrich/collaborative%20filter%20model/runs/6dqticeg' target=\"_blank\">https://wandb.ai/jcrich/collaborative%20filter%20model/runs/6dqticeg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/jcrich/collaborative%20filter%20model/runs/6dqticeg?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f554980b580>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "import random\n",
    "\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    entity=\"jcrich\",\n",
    "    project=\"collaborative filter model\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"architecture\": \"collaborative filter\",\n",
    "    \"dataset\": \"imdb\",\n",
    "    \"epochs\": EPOCHS,\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4ef218b7-74b1-4769-9175-cfc7a9fa2f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 215.83it/s]\n",
      "Epoch 1: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 329.05it/s]\n",
      "Epoch 2: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 328.11it/s]\n",
      "Epoch 3: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 285.57it/s]\n",
      "Epoch 4: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 322.70it/s]\n",
      "Epoch 5: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 254.67it/s]\n",
      "Epoch 6: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 317.15it/s]\n",
      "Epoch 7: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 316.53it/s]\n",
      "Epoch 8: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 345.00it/s]\n",
      "Epoch 9: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 335.80it/s]\n",
      "Epoch 10: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 359.60it/s]\n",
      "Epoch 11: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 344.35it/s]\n",
      "Epoch 12: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 380.39it/s]\n",
      "Epoch 13: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 368.31it/s]\n",
      "Epoch 14: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 372.24it/s]\n",
      "Epoch 15: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 384.36it/s]\n",
      "Epoch 16: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 392.24it/s]\n",
      "Epoch 17: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 284.40it/s]\n",
      "Epoch 18: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 374.75it/s]\n",
      "Epoch 19: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 376.38it/s]\n",
      "Epoch 20: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 382.03it/s]\n",
      "Epoch 21: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 369.92it/s]\n",
      "Epoch 22: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 335.61it/s]\n",
      "Epoch 23: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 350.17it/s]\n",
      "Epoch 24: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 377.15it/s]\n",
      "Epoch 25: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 335.53it/s]\n",
      "Epoch 26: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 375.37it/s]\n",
      "Epoch 27: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 333.22it/s]\n",
      "Epoch 28: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 386.50it/s]\n",
      "Epoch 29: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 359.77it/s]\n",
      "Epoch 30: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 352.66it/s]\n",
      "Epoch 31: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 348.13it/s]\n",
      "Epoch 32: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 374.73it/s]\n",
      "Epoch 33: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 373.95it/s]\n",
      "Epoch 34: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 373.06it/s]\n",
      "Epoch 35: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 374.85it/s]\n",
      "Epoch 36: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 383.27it/s]\n",
      "Epoch 37: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 371.07it/s]\n",
      "Epoch 38: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 317.47it/s]\n",
      "Epoch 39: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 352.93it/s]\n",
      "Epoch 40: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 366.31it/s]\n",
      "Epoch 41: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 388.34it/s]\n",
      "Epoch 42: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 377.08it/s]\n",
      "Epoch 43: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 377.26it/s]\n",
      "Epoch 44: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 363.80it/s]\n",
      "Epoch 45: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 375.88it/s]\n",
      "Epoch 46: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 363.67it/s]\n",
      "Epoch 47: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 357.10it/s]\n",
      "Epoch 48: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 332.33it/s]\n",
      "Epoch 49: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 363.47it/s]\n",
      "Epoch 50: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 369.40it/s]\n",
      "Epoch 51: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 387.86it/s]\n",
      "Epoch 52: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 369.57it/s]\n",
      "Epoch 53: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 338.51it/s]\n",
      "Epoch 54: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 354.02it/s]\n",
      "Epoch 55: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 357.36it/s]\n",
      "Epoch 56: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 336.21it/s]\n",
      "Epoch 57: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 343.37it/s]\n",
      "Epoch 58: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 387.51it/s]\n",
      "Epoch 59: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 363.29it/s]\n",
      "Epoch 60: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 381.51it/s]\n",
      "Epoch 61: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 326.93it/s]\n",
      "Epoch 62: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 327.52it/s]\n",
      "Epoch 63: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 345.35it/s]\n",
      "Epoch 64: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 344.27it/s]\n",
      "Epoch 65: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 373.66it/s]\n",
      "Epoch 66: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 361.68it/s]\n",
      "Epoch 67: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 376.17it/s]\n",
      "Epoch 68: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 371.36it/s]\n",
      "Epoch 69: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 343.75it/s]\n",
      "Epoch 70: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 347.98it/s]\n",
      "Epoch 71: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 312.92it/s]\n",
      "Epoch 72: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 340.21it/s]\n",
      "Epoch 73: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 334.69it/s]\n",
      "Epoch 74: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 339.08it/s]\n",
      "Epoch 75: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 361.99it/s]\n",
      "Epoch 76: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 364.01it/s]\n",
      "Epoch 77: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 392.33it/s]\n",
      "Epoch 78: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 369.17it/s]\n",
      "Epoch 79: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 380.57it/s]\n",
      "Epoch 80: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 341.85it/s]\n",
      "Epoch 81: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 365.24it/s]\n",
      "Epoch 82: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 372.40it/s]\n",
      "Epoch 83: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 368.96it/s]\n",
      "Epoch 84: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 352.97it/s]\n",
      "Epoch 85: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 338.65it/s]\n",
      "Epoch 86: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 358.12it/s]\n",
      "Epoch 87: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 334.71it/s]\n",
      "Epoch 88: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 367.25it/s]\n",
      "Epoch 89: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 361.11it/s]\n",
      "Epoch 90: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 357.59it/s]\n",
      "Epoch 91: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 371.40it/s]\n",
      "Epoch 92: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 353.79it/s]\n",
      "Epoch 93: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 307.17it/s]\n",
      "Epoch 94: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 318.70it/s]\n",
      "Epoch 95: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 350.06it/s]\n",
      "Epoch 96: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 372.89it/s]\n",
      "Epoch 97: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 339.28it/s]\n",
      "Epoch 98: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 355.23it/s]\n",
      "Epoch 99: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 346.08it/s]\n",
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>Training Loss</td><td>█▅▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Validation Loss</td><td>█▅▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>99</td></tr><tr><td>Training Loss</td><td>0.05465</td></tr><tr><td>Validation Loss</td><td>0.07352</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">skilled-firebrand-30</strong> at: <a href='https://wandb.ai/jcrich/collaborative%20filter%20model/runs/6dqticeg' target=\"_blank\">https://wandb.ai/jcrich/collaborative%20filter%20model/runs/6dqticeg</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240311_215042-6dqticeg/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize lists to keep track of losses and epochs\n",
    "# Initialize MLflow run\n",
    "# Initialize MLflow run\n",
    "# Initialize MLflow run\n",
    "\n",
    "train_losses = []\n",
    "validation_losses = []\n",
    "epochs = []\n",
    "\n",
    "# Set the best validation loss to infinity at the start\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_epoch_loss = 0\n",
    "    for batch_idx, (user_ids, movie_ids, ratings) in tqdm(enumerate(train_data_loader), total=len(train_data_loader), desc=f'Epoch {epoch}'):\n",
    "\n",
    "        optimiser.zero_grad()\n",
    "        predictions = model(user_ids, movie_ids)\n",
    "        loss = criterion(predictions, ratings)\n",
    "\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        \n",
    "        batch_loss = loss.item()\n",
    "        total_epoch_loss += batch_loss\n",
    "        \n",
    "    # After all batches, calculate average loss for the epoch\n",
    "    avg_epoch_loss = total_epoch_loss / len(train_data_loader)\n",
    "    train_losses.append(avg_epoch_loss)\n",
    "    epochs.append(epoch)\n",
    "    \n",
    "    # Log training loss for the current epoch\n",
    "    wandb.log({\"Training Loss\": avg_epoch_loss, \"Epoch\": epoch})\n",
    "    mlflow.log_metric(\"Training Loss\", avg_epoch_loss, step=epoch)\n",
    "\n",
    "    # Start validation phase\n",
    "    model.eval()\n",
    "    total_validation_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (user_ids, movie_ids, ratings) in enumerate(validation_data_loader):\n",
    "            predictions = model(user_ids, movie_ids)\n",
    "            loss = criterion(predictions, ratings)\n",
    "            total_validation_loss += loss.item()\n",
    "    \n",
    "    # Calculate and log validation loss after the epoch\n",
    "    avg_validation_loss = total_validation_loss / len(validation_data_loader)\n",
    "    validation_losses.append(avg_validation_loss)\n",
    "\n",
    "    # Save the model if this epoch has the best validation loss so far\n",
    "    if avg_validation_loss < best_val_loss:\n",
    "        best_val_loss = avg_validation_loss\n",
    "        mlflow.log_metric(\"Best Validation Loss\", best_val_loss, step=epoch)\n",
    "        mlflow.pytorch.log_model(model, \"model\")\n",
    "        # Save model state\n",
    "        torch.save(model.state_dict(), 'best_model_state.pth')\n",
    "        # If you also want to save the optimizer state along with the model:\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimiser_state_dict': optimiser.state_dict(),\n",
    "            'loss': avg_validation_loss,\n",
    "        }, 'best_col_model_checkpoint.pth')\n",
    "        # Log the model checkpoint as an artifact\n",
    "        mlflow.log_artifact('best_col_model_checkpoint.pth')\n",
    "    \n",
    "    # Log validation loss for the current epoch\n",
    "    wandb.log({\"Validation Loss\": avg_validation_loss, \"Epoch\": epoch})\n",
    "    mlflow.log_metric(\"Validation Loss\", avg_validation_loss, step=epoch)\n",
    "    \n",
    "# Finish the Weights & Biases run\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9ede2c71-6995-4fd5-a657-e15996945cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07306494217431336\n"
     ]
    }
   ],
   "source": [
    "#test data\n",
    "model.eval()  # Switch to evaluation mode\n",
    "test_loss = 0\n",
    "correct_predictions = 0  # Fixed variable name for clarity\n",
    "total_contexts = 0\n",
    "\n",
    "with torch.no_grad():  # Disable gradient computation\n",
    "    for user_ids, movie_ids, ratings in test_data_loader:\n",
    "        predictions = model(user_ids,movie_ids)  # Generate predictions\n",
    "        # Calculate and accumulate loss\n",
    "        loss = criterion(predictions, ratings)\n",
    "        test_loss += loss.item()\n",
    "        \n",
    "        # # Reshape predictions to match [batch_size, context_size, vocab_size]\n",
    "        # predictions = predictions.view(-1, context_size, VOCAB_SIZE)\n",
    "        \n",
    "        # # Get top prediction for each context position\n",
    "        # top_predictions = predictions.argmax(dim=2)\n",
    "        \n",
    "        # # Calculate correct predictions\n",
    "        # correct_preds = (top_predictions == context).float().sum()\n",
    "        # correct_predictions += correct_preds.item()  # Accumulate correct predictions\n",
    "        \n",
    "        # total_contexts += context.numel()  # Total number of context word positions evaluated\n",
    "\n",
    "# Calculate final metrics\n",
    "test_loss /= len(test_data_loader)  # Average loss per batch\n",
    "# print('correct predictions = ',correct_predictions)\n",
    "# print('out of  = ',total_contexts)\n",
    "# accuracy = correct_predictions / total_contexts  # Compute accuracy\n",
    "\n",
    "# print(f'Test set: Average loss: {test_loss:.4f}, Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Evaluate the model\n",
    "# (This would involve using a separate validation set or performing cross-validation)\n",
    "print(test_loss)\n",
    "\n",
    "mlflow.log_metric('Post training test loss',test_loss)\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff24efc-8629-4242-8cce-6311cb1f9192",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8631385b-7cfe-427b-b2db-427241171649",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPLit\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Configuration\n",
    "num_folds = 10\n",
    "data_size = len(test_ds)  # Assuming 'dataset' is a PyTorch Dataset\n",
    "\n",
    "# Define the K-fold Cross Validator\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "31c8ce29-174d-4687-9476-a996d5a9ccfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optional: Set MLflow experiment\n",
    "mlflow.set_experiment(\"Finetuned Collaborative Filter\")\n",
    "# Log model parameters (example)\n",
    "mlflow.start_run()\n",
    "mlflow.log_param(\"epochs\", EPOCHS)\n",
    "mlflow.log_param(\"optimizer\", type(optimiser).__name__)\n",
    "mlflow.log_param(\"learning rate\", LEARNING_RATE)\n",
    "mlflow.log_param(\"batch size\", BATCH_SIZE)\n",
    "mlflow.log_param(\"L2 regularization\", L2_REGULARIZATION)\n",
    "mlflow.log_param(\"KFOLDS\", num_folds)\n",
    "mlflow.log_param(\"drop out\", 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b8e36fba-65b3-40c4-ad25-0e6b7b004449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.16.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/james/Desktop/torchex/movie-users/wandb/run-20240311_215552-dxwil5oh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jcrich/collaborative%20filter%20model/runs/dxwil5oh' target=\"_blank\">stellar-salad-32</a></strong> to <a href='https://wandb.ai/jcrich/collaborative%20filter%20model' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jcrich/collaborative%20filter%20model' target=\"_blank\">https://wandb.ai/jcrich/collaborative%20filter%20model</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jcrich/collaborative%20filter%20model/runs/dxwil5oh' target=\"_blank\">https://wandb.ai/jcrich/collaborative%20filter%20model/runs/dxwil5oh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/jcrich/collaborative%20filter%20model/runs/dxwil5oh?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f559b7bc0a0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    entity=\"jcrich\",\n",
    "    project=\"collaborative filter model\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"architecture\": \"collaborative filter\",\n",
    "    \"dataset\": \"letterboxd\",\n",
    "    \"epochs\": EPOCHS,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8b0830cd-43f1-454c-8e81-72e5d73549fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0\n",
      "--------------------------------\n",
      "Validation Loss for fold 0: 0.5213800668716431\n",
      "Validation Loss for fold 0: 0.4675179322560628\n",
      "Validation Loss for fold 0: 0.37011366089185077\n",
      "Validation Loss for fold 0: 0.3169592320919037\n",
      "Validation Loss for fold 0: 0.26690048972765607\n",
      "Validation Loss for fold 0: 0.24119565884272257\n",
      "Validation Loss for fold 0: 0.21109308302402496\n",
      "Validation Loss for fold 0: 0.19659075140953064\n",
      "Validation Loss for fold 0: 0.19230835636456808\n",
      "Validation Loss for fold 0: 0.16834469139575958\n",
      "Validation Loss for fold 0: 0.1724931796391805\n",
      "Validation Loss for fold 0: 0.16517843306064606\n",
      "Validation Loss for fold 0: 0.17479813595612845\n",
      "Validation Loss for fold 0: 0.14418483773867288\n",
      "Validation Loss for fold 0: 0.15777303278446198\n",
      "Validation Loss for fold 0: 0.146331657965978\n",
      "Validation Loss for fold 0: 0.14786007503668466\n",
      "Validation Loss for fold 0: 0.15986826519171396\n",
      "Validation Loss for fold 0: 0.150053933262825\n",
      "Validation Loss for fold 0: 0.13514054814974466\n",
      "Validation Loss for fold 0: 0.1446077525615692\n",
      "Validation Loss for fold 0: 0.14801408350467682\n",
      "Validation Loss for fold 0: 0.12419719000657399\n",
      "Validation Loss for fold 0: 0.12600277612606683\n",
      "Validation Loss for fold 0: 0.1276884526014328\n",
      "Validation Loss for fold 0: 0.12450616310040157\n",
      "Validation Loss for fold 0: 0.13923203448454538\n",
      "Validation Loss for fold 0: 0.1292270744840304\n",
      "Validation Loss for fold 0: 0.12951502203941345\n",
      "Validation Loss for fold 0: 0.11723395933707555\n",
      "Validation Loss for fold 0: 0.1229058454434077\n",
      "Validation Loss for fold 0: 0.13276821374893188\n",
      "Validation Loss for fold 0: 0.1293587883313497\n",
      "Validation Loss for fold 0: 0.11283668130636215\n",
      "Validation Loss for fold 0: 0.12954801072676977\n",
      "Validation Loss for fold 0: 0.11485708753267924\n",
      "Validation Loss for fold 0: 0.10860019425551097\n",
      "Validation Loss for fold 0: 0.12164401262998581\n",
      "Validation Loss for fold 0: 0.12137475858132045\n",
      "Validation Loss for fold 0: 0.11511697868506114\n",
      "Validation Loss for fold 0: 0.10924699157476425\n",
      "Validation Loss for fold 0: 0.11217287182807922\n",
      "Validation Loss for fold 0: 0.10816834618647893\n",
      "Validation Loss for fold 0: 0.10780945420265198\n",
      "Validation Loss for fold 0: 0.11139906446139018\n",
      "Validation Loss for fold 0: 0.10817369570334752\n",
      "Validation Loss for fold 0: 0.10761507352193196\n",
      "Validation Loss for fold 0: 0.11507527033487956\n",
      "Validation Loss for fold 0: 0.1041930615901947\n",
      "Validation Loss for fold 0: 0.103139894704024\n",
      "Validation Loss for fold 0: 0.10966009646654129\n",
      "Validation Loss for fold 0: 0.10136967897415161\n",
      "Validation Loss for fold 0: 0.10080363601446152\n",
      "Validation Loss for fold 0: 0.10028073191642761\n",
      "Validation Loss for fold 0: 0.10364391654729843\n",
      "Validation Loss for fold 0: 0.10489841302235921\n",
      "Validation Loss for fold 0: 0.10744945704936981\n",
      "Validation Loss for fold 0: 0.10681495318810146\n",
      "Validation Loss for fold 0: 0.09757833431164424\n",
      "Validation Loss for fold 0: 0.09739875545104344\n",
      "Validation Loss for fold 0: 0.10370908677577972\n",
      "Validation Loss for fold 0: 0.09290247162183125\n",
      "Validation Loss for fold 0: 0.09960840394099553\n",
      "Validation Loss for fold 0: 0.09624611834685008\n",
      "Validation Loss for fold 0: 0.10184718171755473\n",
      "Validation Loss for fold 0: 0.09180172781149547\n",
      "Validation Loss for fold 0: 0.09613331655661266\n",
      "Validation Loss for fold 0: 0.09724961469570796\n",
      "Validation Loss for fold 0: 0.09263387819131215\n",
      "Validation Loss for fold 0: 0.0878206913669904\n",
      "Validation Loss for fold 0: 0.0952990750471751\n",
      "Validation Loss for fold 0: 0.09285923341910045\n",
      "Validation Loss for fold 0: 0.1047772467136383\n",
      "Validation Loss for fold 0: 0.09747126946846645\n",
      "Validation Loss for fold 0: 0.08624114096164703\n",
      "Validation Loss for fold 0: 0.09507471074660619\n",
      "Validation Loss for fold 0: 0.09228148559729259\n",
      "Validation Loss for fold 0: 0.09630446135997772\n",
      "Validation Loss for fold 0: 0.08746928224960963\n",
      "Validation Loss for fold 0: 0.07923795282840729\n",
      "Validation Loss for fold 0: 0.07882921894391377\n",
      "Validation Loss for fold 0: 0.09246888260046641\n",
      "Validation Loss for fold 0: 0.08035243550936381\n",
      "Validation Loss for fold 0: 0.08618666231632233\n",
      "Validation Loss for fold 0: 0.08944770693778992\n",
      "Validation Loss for fold 0: 0.08900623768568039\n",
      "Validation Loss for fold 0: 0.08623017867406209\n",
      "Validation Loss for fold 0: 0.08439675966898601\n",
      "Validation Loss for fold 0: 0.09219972292582194\n",
      "Validation Loss for fold 0: 0.08744975179433823\n",
      "Validation Loss for fold 0: 0.0908707653482755\n",
      "Validation Loss for fold 0: 0.08843136578798294\n",
      "Validation Loss for fold 0: 0.08280747880538304\n",
      "Validation Loss for fold 0: 0.07749975348512332\n",
      "Validation Loss for fold 0: 0.08224983761707942\n",
      "Validation Loss for fold 0: 0.08916143576304118\n",
      "Validation Loss for fold 0: 0.08288090179363887\n",
      "Validation Loss for fold 0: 0.08288513123989105\n",
      "Validation Loss for fold 0: 0.08150252948204677\n",
      "Validation Loss for fold 0: 0.08564711858828862\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Validation Loss for fold 1: 0.6349525054295858\n",
      "Validation Loss for fold 1: 0.5302000840504965\n",
      "Validation Loss for fold 1: 0.40380791823069256\n",
      "Validation Loss for fold 1: 0.39253101746241253\n",
      "Validation Loss for fold 1: 0.3278656204541524\n",
      "Validation Loss for fold 1: 0.3128144343694051\n",
      "Validation Loss for fold 1: 0.27077092230319977\n",
      "Validation Loss for fold 1: 0.2480529099702835\n",
      "Validation Loss for fold 1: 0.23521644870440164\n",
      "Validation Loss for fold 1: 0.24063599109649658\n",
      "Validation Loss for fold 1: 0.21106833219528198\n",
      "Validation Loss for fold 1: 0.2241154064734777\n",
      "Validation Loss for fold 1: 0.20720873276392618\n",
      "Validation Loss for fold 1: 0.18670966724554697\n",
      "Validation Loss for fold 1: 0.18053391575813293\n",
      "Validation Loss for fold 1: 0.19363234440485635\n",
      "Validation Loss for fold 1: 0.18324123322963715\n",
      "Validation Loss for fold 1: 0.18019569416840872\n",
      "Validation Loss for fold 1: 0.18314999838670096\n",
      "Validation Loss for fold 1: 0.17025850216547647\n",
      "Validation Loss for fold 1: 0.1933544228474299\n",
      "Validation Loss for fold 1: 0.18734661241372427\n",
      "Validation Loss for fold 1: 0.17719190319379172\n",
      "Validation Loss for fold 1: 0.16159701844056448\n",
      "Validation Loss for fold 1: 0.17974399526913962\n",
      "Validation Loss for fold 1: 0.16914446403582892\n",
      "Validation Loss for fold 1: 0.1577936808268229\n",
      "Validation Loss for fold 1: 0.16239158312479654\n",
      "Validation Loss for fold 1: 0.14969618866840997\n",
      "Validation Loss for fold 1: 0.14618026713530222\n",
      "Validation Loss for fold 1: 0.14257370680570602\n",
      "Validation Loss for fold 1: 0.15331233541170755\n",
      "Validation Loss for fold 1: 0.14745339254538217\n",
      "Validation Loss for fold 1: 0.13890776534875235\n",
      "Validation Loss for fold 1: 0.13323536763588587\n",
      "Validation Loss for fold 1: 0.137886310617129\n",
      "Validation Loss for fold 1: 0.15034843981266022\n",
      "Validation Loss for fold 1: 0.13436579455931982\n",
      "Validation Loss for fold 1: 0.12464092671871185\n",
      "Validation Loss for fold 1: 0.1443707620104154\n",
      "Validation Loss for fold 1: 0.1310139795144399\n",
      "Validation Loss for fold 1: 0.13357380032539368\n",
      "Validation Loss for fold 1: 0.14015259593725204\n",
      "Validation Loss for fold 1: 0.1292731687426567\n",
      "Validation Loss for fold 1: 0.1329399198293686\n",
      "Validation Loss for fold 1: 0.12739958117405573\n",
      "Validation Loss for fold 1: 0.13323461016019186\n",
      "Validation Loss for fold 1: 0.12752374509970346\n",
      "Validation Loss for fold 1: 0.12695533533891043\n",
      "Validation Loss for fold 1: 0.1252836436033249\n",
      "Validation Loss for fold 1: 0.1308756743868192\n",
      "Validation Loss for fold 1: 0.11887496461470921\n",
      "Validation Loss for fold 1: 0.11728541553020477\n",
      "Validation Loss for fold 1: 0.11908361564079921\n",
      "Validation Loss for fold 1: 0.12352048854033153\n",
      "Validation Loss for fold 1: 0.11764766027530034\n",
      "Validation Loss for fold 1: 0.10605828712383907\n",
      "Validation Loss for fold 1: 0.129506287475427\n",
      "Validation Loss for fold 1: 0.11975966393947601\n",
      "Validation Loss for fold 1: 0.10831209272146225\n",
      "Validation Loss for fold 1: 0.10953384637832642\n",
      "Validation Loss for fold 1: 0.11495263129472733\n",
      "Validation Loss for fold 1: 0.11221358925104141\n",
      "Validation Loss for fold 1: 0.11344853043556213\n",
      "Validation Loss for fold 1: 0.11133500188589096\n",
      "Validation Loss for fold 1: 0.12039911498626073\n",
      "Validation Loss for fold 1: 0.116721677283446\n",
      "Validation Loss for fold 1: 0.10815618435541789\n",
      "Validation Loss for fold 1: 0.09845763196547826\n",
      "Validation Loss for fold 1: 0.10278322050968806\n",
      "Validation Loss for fold 1: 0.10756189376115799\n",
      "Validation Loss for fold 1: 0.10133359332879384\n",
      "Validation Loss for fold 1: 0.09731889516115189\n",
      "Validation Loss for fold 1: 0.10696804523468018\n",
      "Validation Loss for fold 1: 0.09984791527191798\n",
      "Validation Loss for fold 1: 0.10722244779268901\n",
      "Validation Loss for fold 1: 0.09173523510495822\n",
      "Validation Loss for fold 1: 0.09549142916997273\n",
      "Validation Loss for fold 1: 0.1014690895875295\n",
      "Validation Loss for fold 1: 0.09742823739846547\n",
      "Validation Loss for fold 1: 0.08363177751501401\n",
      "Validation Loss for fold 1: 0.10284109165271123\n",
      "Validation Loss for fold 1: 0.09936338414748509\n",
      "Validation Loss for fold 1: 0.09453246245781581\n",
      "Validation Loss for fold 1: 0.09254481146732967\n",
      "Validation Loss for fold 1: 0.0949735517303149\n",
      "Validation Loss for fold 1: 0.1017929141720136\n",
      "Validation Loss for fold 1: 0.09778656313816707\n",
      "Validation Loss for fold 1: 0.08838157852490743\n",
      "Validation Loss for fold 1: 0.09158298373222351\n",
      "Validation Loss for fold 1: 0.09023540715376537\n",
      "Validation Loss for fold 1: 0.09155325839916865\n",
      "Validation Loss for fold 1: 0.08723322798808415\n",
      "Validation Loss for fold 1: 0.09112310409545898\n",
      "Validation Loss for fold 1: 0.0903315320611\n",
      "Validation Loss for fold 1: 0.08760589609543483\n",
      "Validation Loss for fold 1: 0.08982283125321071\n",
      "Validation Loss for fold 1: 0.08646508554617564\n",
      "Validation Loss for fold 1: 0.0909343163172404\n",
      "Validation Loss for fold 1: 0.09000834325949351\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Validation Loss for fold 2: 0.5785630544026693\n",
      "Validation Loss for fold 2: 0.4814274807771047\n",
      "Validation Loss for fold 2: 0.323639174302419\n",
      "Validation Loss for fold 2: 0.23270549376805624\n",
      "Validation Loss for fold 2: 0.17171606421470642\n",
      "Validation Loss for fold 2: 0.15606010456879935\n",
      "Validation Loss for fold 2: 0.1325927128394445\n",
      "Validation Loss for fold 2: 0.11090006430943807\n",
      "Validation Loss for fold 2: 0.11028987417618434\n",
      "Validation Loss for fold 2: 0.10307922959327698\n",
      "Validation Loss for fold 2: 0.1016871730486552\n",
      "Validation Loss for fold 2: 0.09692087272802989\n",
      "Validation Loss for fold 2: 0.09372841070095699\n",
      "Validation Loss for fold 2: 0.08763740956783295\n",
      "Validation Loss for fold 2: 0.09005053341388702\n",
      "Validation Loss for fold 2: 0.08260759462912877\n",
      "Validation Loss for fold 2: 0.08724111318588257\n",
      "Validation Loss for fold 2: 0.08778462807337443\n",
      "Validation Loss for fold 2: 0.08224524060885112\n",
      "Validation Loss for fold 2: 0.07929938783248265\n",
      "Validation Loss for fold 2: 0.07925997177759807\n",
      "Validation Loss for fold 2: 0.07325318455696106\n",
      "Validation Loss for fold 2: 0.08243748048941295\n",
      "Validation Loss for fold 2: 0.08425141125917435\n",
      "Validation Loss for fold 2: 0.07961913694938023\n",
      "Validation Loss for fold 2: 0.07333802804350853\n",
      "Validation Loss for fold 2: 0.07401390373706818\n",
      "Validation Loss for fold 2: 0.0758697936932246\n",
      "Validation Loss for fold 2: 0.071428498874108\n",
      "Validation Loss for fold 2: 0.08133011807998021\n",
      "Validation Loss for fold 2: 0.07383636633555095\n",
      "Validation Loss for fold 2: 0.07658981531858444\n",
      "Validation Loss for fold 2: 0.07556227346261342\n",
      "Validation Loss for fold 2: 0.08222402632236481\n",
      "Validation Loss for fold 2: 0.0809066245953242\n",
      "Validation Loss for fold 2: 0.070644856741031\n",
      "Validation Loss for fold 2: 0.07848544791340828\n",
      "Validation Loss for fold 2: 0.07501311351855595\n",
      "Validation Loss for fold 2: 0.08173361917336781\n",
      "Validation Loss for fold 2: 0.07462222998340924\n",
      "Validation Loss for fold 2: 0.07562856872876485\n",
      "Validation Loss for fold 2: 0.08235978831847508\n",
      "Validation Loss for fold 2: 0.06677808860937755\n",
      "Validation Loss for fold 2: 0.0710463710129261\n",
      "Validation Loss for fold 2: 0.0724283258120219\n",
      "Validation Loss for fold 2: 0.07311562945445378\n",
      "Validation Loss for fold 2: 0.07045719027519226\n",
      "Validation Loss for fold 2: 0.07742638885974884\n",
      "Validation Loss for fold 2: 0.06894321118791898\n",
      "Validation Loss for fold 2: 0.0700811098019282\n",
      "Validation Loss for fold 2: 0.07180090869466464\n",
      "Validation Loss for fold 2: 0.079103438804547\n",
      "Validation Loss for fold 2: 0.06929745525121689\n",
      "Validation Loss for fold 2: 0.06283601373434067\n",
      "Validation Loss for fold 2: 0.06660943975051244\n",
      "Validation Loss for fold 2: 0.06468898802995682\n",
      "Validation Loss for fold 2: 0.06393657500545184\n",
      "Validation Loss for fold 2: 0.06729737669229507\n",
      "Validation Loss for fold 2: 0.07066669315099716\n",
      "Validation Loss for fold 2: 0.0770792265733083\n",
      "Validation Loss for fold 2: 0.06940176337957382\n",
      "Validation Loss for fold 2: 0.06574809675415356\n",
      "Validation Loss for fold 2: 0.06690722207228343\n",
      "Validation Loss for fold 2: 0.07497460395097733\n",
      "Validation Loss for fold 2: 0.067076592395703\n",
      "Validation Loss for fold 2: 0.06524977584679921\n",
      "Validation Loss for fold 2: 0.06641101216276486\n",
      "Validation Loss for fold 2: 0.061539621402819954\n",
      "Validation Loss for fold 2: 0.07011297345161438\n",
      "Validation Loss for fold 2: 0.0668046623468399\n",
      "Validation Loss for fold 2: 0.0653336172302564\n",
      "Validation Loss for fold 2: 0.06817290683587392\n",
      "Validation Loss for fold 2: 0.0646125003695488\n",
      "Validation Loss for fold 2: 0.06226621940732002\n",
      "Validation Loss for fold 2: 0.06495300556222598\n",
      "Validation Loss for fold 2: 0.0637276458243529\n",
      "Validation Loss for fold 2: 0.0653052752216657\n",
      "Validation Loss for fold 2: 0.06856697052717209\n",
      "Validation Loss for fold 2: 0.06744036699334781\n",
      "Validation Loss for fold 2: 0.061968435843785606\n",
      "Validation Loss for fold 2: 0.06302429735660553\n",
      "Validation Loss for fold 2: 0.06454852471748988\n",
      "Validation Loss for fold 2: 0.06078176945447922\n",
      "Validation Loss for fold 2: 0.0638788752257824\n",
      "Validation Loss for fold 2: 0.062201693654060364\n",
      "Validation Loss for fold 2: 0.060540515929460526\n",
      "Validation Loss for fold 2: 0.06526631489396095\n",
      "Validation Loss for fold 2: 0.06507224837938945\n",
      "Validation Loss for fold 2: 0.06397521868348122\n",
      "Validation Loss for fold 2: 0.06606382379929225\n",
      "Validation Loss for fold 2: 0.07044367492198944\n",
      "Validation Loss for fold 2: 0.05820689102013906\n",
      "Validation Loss for fold 2: 0.06406451513369878\n",
      "Validation Loss for fold 2: 0.06252361958225568\n",
      "Validation Loss for fold 2: 0.06828142081697781\n",
      "Validation Loss for fold 2: 0.06437755872805913\n",
      "Validation Loss for fold 2: 0.06053638085722923\n",
      "Validation Loss for fold 2: 0.06128105397025744\n",
      "Validation Loss for fold 2: 0.058063232650359474\n",
      "Validation Loss for fold 2: 0.06735843668381374\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "Validation Loss for fold 3: 0.3689846992492676\n",
      "Validation Loss for fold 3: 0.28724117080370587\n",
      "Validation Loss for fold 3: 0.24819066127141318\n",
      "Validation Loss for fold 3: 0.22775998711585999\n",
      "Validation Loss for fold 3: 0.2141487201054891\n",
      "Validation Loss for fold 3: 0.2015830228726069\n",
      "Validation Loss for fold 3: 0.20275718967119852\n",
      "Validation Loss for fold 3: 0.20508069296677908\n",
      "Validation Loss for fold 3: 0.17174817125002542\n",
      "Validation Loss for fold 3: 0.1800945301850637\n",
      "Validation Loss for fold 3: 0.1705592026313146\n",
      "Validation Loss for fold 3: 0.17570415139198303\n",
      "Validation Loss for fold 3: 0.18203765153884888\n",
      "Validation Loss for fold 3: 0.16170720010995865\n",
      "Validation Loss for fold 3: 0.1640536238749822\n",
      "Validation Loss for fold 3: 0.16280828913052878\n",
      "Validation Loss for fold 3: 0.15894611676534018\n",
      "Validation Loss for fold 3: 0.15631043910980225\n",
      "Validation Loss for fold 3: 0.16736500461896262\n",
      "Validation Loss for fold 3: 0.16474576791127524\n",
      "Validation Loss for fold 3: 0.1584643373886744\n",
      "Validation Loss for fold 3: 0.17005308965841928\n",
      "Validation Loss for fold 3: 0.16519503792126974\n",
      "Validation Loss for fold 3: 0.15280559162298837\n",
      "Validation Loss for fold 3: 0.15691640476385751\n",
      "Validation Loss for fold 3: 0.15308880309263864\n",
      "Validation Loss for fold 3: 0.1602449044585228\n",
      "Validation Loss for fold 3: 0.17749347786108652\n",
      "Validation Loss for fold 3: 0.15831952293713888\n",
      "Validation Loss for fold 3: 0.14069103449583054\n",
      "Validation Loss for fold 3: 0.14370172222455344\n",
      "Validation Loss for fold 3: 0.15412785112857819\n",
      "Validation Loss for fold 3: 0.15070231755574545\n",
      "Validation Loss for fold 3: 0.15374061465263367\n",
      "Validation Loss for fold 3: 0.13659821699062982\n",
      "Validation Loss for fold 3: 0.14764364063739777\n",
      "Validation Loss for fold 3: 0.14389159282048544\n",
      "Validation Loss for fold 3: 0.14838364720344543\n",
      "Validation Loss for fold 3: 0.14055187503496805\n",
      "Validation Loss for fold 3: 0.15063009659449259\n",
      "Validation Loss for fold 3: 0.14555971324443817\n",
      "Validation Loss for fold 3: 0.13745635002851486\n",
      "Validation Loss for fold 3: 0.1427383522192637\n",
      "Validation Loss for fold 3: 0.14956373473008475\n",
      "Validation Loss for fold 3: 0.14044826726118723\n",
      "Validation Loss for fold 3: 0.14949561655521393\n",
      "Validation Loss for fold 3: 0.14797983566919962\n",
      "Validation Loss for fold 3: 0.13466649750868478\n",
      "Validation Loss for fold 3: 0.135979692141215\n",
      "Validation Loss for fold 3: 0.13633142908414206\n",
      "Validation Loss for fold 3: 0.12985657155513763\n",
      "Validation Loss for fold 3: 0.1420163537065188\n",
      "Validation Loss for fold 3: 0.13188333809375763\n",
      "Validation Loss for fold 3: 0.12589221447706223\n",
      "Validation Loss for fold 3: 0.13302219907442728\n",
      "Validation Loss for fold 3: 0.134973814090093\n",
      "Validation Loss for fold 3: 0.12955463180939356\n",
      "Validation Loss for fold 3: 0.13524321218331656\n",
      "Validation Loss for fold 3: 0.1311461180448532\n",
      "Validation Loss for fold 3: 0.13586037357648215\n",
      "Validation Loss for fold 3: 0.121456245581309\n",
      "Validation Loss for fold 3: 0.11864351232846577\n",
      "Validation Loss for fold 3: 0.12500223765770593\n",
      "Validation Loss for fold 3: 0.12796177715063095\n",
      "Validation Loss for fold 3: 0.11992038538058598\n",
      "Validation Loss for fold 3: 0.12301496664683025\n",
      "Validation Loss for fold 3: 0.11767270416021347\n",
      "Validation Loss for fold 3: 0.12674557665983835\n",
      "Validation Loss for fold 3: 0.12324989835421245\n",
      "Validation Loss for fold 3: 0.1284684787193934\n",
      "Validation Loss for fold 3: 0.12250509361426036\n",
      "Validation Loss for fold 3: 0.11773386349280675\n",
      "Validation Loss for fold 3: 0.11813439180453618\n",
      "Validation Loss for fold 3: 0.1259393518169721\n",
      "Validation Loss for fold 3: 0.12948089589675268\n",
      "Validation Loss for fold 3: 0.11711842070023219\n",
      "Validation Loss for fold 3: 0.11704848955074947\n",
      "Validation Loss for fold 3: 0.11525492618481319\n",
      "Validation Loss for fold 3: 0.11952885240316391\n",
      "Validation Loss for fold 3: 0.11286547283331554\n",
      "Validation Loss for fold 3: 0.11678274224201839\n",
      "Validation Loss for fold 3: 0.1201847493648529\n",
      "Validation Loss for fold 3: 0.11266545454661052\n",
      "Validation Loss for fold 3: 0.11688913404941559\n",
      "Validation Loss for fold 3: 0.11229873200257619\n",
      "Validation Loss for fold 3: 0.10928468157847722\n",
      "Validation Loss for fold 3: 0.10990474869807561\n",
      "Validation Loss for fold 3: 0.10350009550650914\n",
      "Validation Loss for fold 3: 0.10514934609333675\n",
      "Validation Loss for fold 3: 0.1150876631339391\n",
      "Validation Loss for fold 3: 0.1068476860721906\n",
      "Validation Loss for fold 3: 0.10744345684846242\n",
      "Validation Loss for fold 3: 0.10979134341080983\n",
      "Validation Loss for fold 3: 0.10491220901409785\n",
      "Validation Loss for fold 3: 0.10333702216545741\n",
      "Validation Loss for fold 3: 0.10506848245859146\n",
      "Validation Loss for fold 3: 0.1078715870777766\n",
      "Validation Loss for fold 3: 0.09704835464557011\n",
      "Validation Loss for fold 3: 0.10220427066087723\n",
      "Validation Loss for fold 3: 0.10861923048893611\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "Validation Loss for fold 4: 0.37374081214269\n",
      "Validation Loss for fold 4: 0.2756885091463725\n",
      "Validation Loss for fold 4: 0.20396540562311807\n",
      "Validation Loss for fold 4: 0.17175969978173575\n",
      "Validation Loss for fold 4: 0.16062040627002716\n",
      "Validation Loss for fold 4: 0.12851431717475256\n",
      "Validation Loss for fold 4: 0.11438838144143422\n",
      "Validation Loss for fold 4: 0.10419520487387975\n",
      "Validation Loss for fold 4: 0.11704179396231969\n",
      "Validation Loss for fold 4: 0.11065687984228134\n",
      "Validation Loss for fold 4: 0.10590135057767232\n",
      "Validation Loss for fold 4: 0.09082397321859996\n",
      "Validation Loss for fold 4: 0.09033560752868652\n",
      "Validation Loss for fold 4: 0.09662536283334096\n",
      "Validation Loss for fold 4: 0.09586970259745915\n",
      "Validation Loss for fold 4: 0.08563138296206792\n",
      "Validation Loss for fold 4: 0.09955839564402898\n",
      "Validation Loss for fold 4: 0.09447292238473892\n",
      "Validation Loss for fold 4: 0.08232359091440837\n",
      "Validation Loss for fold 4: 0.08950050920248032\n",
      "Validation Loss for fold 4: 0.07880567262570064\n",
      "Validation Loss for fold 4: 0.07842118044694264\n",
      "Validation Loss for fold 4: 0.073016290863355\n",
      "Validation Loss for fold 4: 0.08289873103300731\n",
      "Validation Loss for fold 4: 0.08339084933201472\n",
      "Validation Loss for fold 4: 0.07669371366500854\n",
      "Validation Loss for fold 4: 0.07761211196581523\n",
      "Validation Loss for fold 4: 0.07759047672152519\n",
      "Validation Loss for fold 4: 0.07636447250843048\n",
      "Validation Loss for fold 4: 0.07740325232346852\n",
      "Validation Loss for fold 4: 0.08757323771715164\n",
      "Validation Loss for fold 4: 0.07583645979563396\n",
      "Validation Loss for fold 4: 0.07513235757748286\n",
      "Validation Loss for fold 4: 0.0743442823489507\n",
      "Validation Loss for fold 4: 0.07911216964324315\n",
      "Validation Loss for fold 4: 0.07326685885588329\n",
      "Validation Loss for fold 4: 0.07242247710625331\n",
      "Validation Loss for fold 4: 0.07173939049243927\n",
      "Validation Loss for fold 4: 0.06822814295689265\n",
      "Validation Loss for fold 4: 0.06613762304186821\n",
      "Validation Loss for fold 4: 0.0693824477493763\n",
      "Validation Loss for fold 4: 0.07590098927418391\n",
      "Validation Loss for fold 4: 0.07011671364307404\n",
      "Validation Loss for fold 4: 0.07070495188236237\n",
      "Validation Loss for fold 4: 0.06925209984183311\n",
      "Validation Loss for fold 4: 0.07551078001658122\n",
      "Validation Loss for fold 4: 0.07884458700815837\n",
      "Validation Loss for fold 4: 0.07333132872978847\n",
      "Validation Loss for fold 4: 0.06162702292203903\n",
      "Validation Loss for fold 4: 0.065800242125988\n",
      "Validation Loss for fold 4: 0.06550081943472226\n",
      "Validation Loss for fold 4: 0.06554623569051425\n",
      "Validation Loss for fold 4: 0.0719497303167979\n",
      "Validation Loss for fold 4: 0.06419422353307407\n",
      "Validation Loss for fold 4: 0.06400147080421448\n",
      "Validation Loss for fold 4: 0.0629703626036644\n",
      "Validation Loss for fold 4: 0.0703742578625679\n",
      "Validation Loss for fold 4: 0.0629555272559325\n",
      "Validation Loss for fold 4: 0.07079028089841206\n",
      "Validation Loss for fold 4: 0.06303058192133904\n",
      "Validation Loss for fold 4: 0.06806877752145131\n",
      "Validation Loss for fold 4: 0.06413145735859871\n",
      "Validation Loss for fold 4: 0.06820197651783626\n",
      "Validation Loss for fold 4: 0.07009721919894218\n",
      "Validation Loss for fold 4: 0.0638518122335275\n",
      "Validation Loss for fold 4: 0.06936549892028172\n",
      "Validation Loss for fold 4: 0.06347803523143132\n",
      "Validation Loss for fold 4: 0.07189966986576717\n",
      "Validation Loss for fold 4: 0.06250684584180514\n",
      "Validation Loss for fold 4: 0.06538466612497966\n",
      "Validation Loss for fold 4: 0.06287725518147151\n",
      "Validation Loss for fold 4: 0.0599738284945488\n",
      "Validation Loss for fold 4: 0.06978944689035416\n",
      "Validation Loss for fold 4: 0.06379653761784236\n",
      "Validation Loss for fold 4: 0.062313374131917953\n",
      "Validation Loss for fold 4: 0.0602863480647405\n",
      "Validation Loss for fold 4: 0.060655868301788964\n",
      "Validation Loss for fold 4: 0.05828374748428663\n",
      "Validation Loss for fold 4: 0.05694896851976713\n",
      "Validation Loss for fold 4: 0.06732439373930295\n",
      "Validation Loss for fold 4: 0.06451304629445076\n",
      "Validation Loss for fold 4: 0.06276396537820499\n",
      "Validation Loss for fold 4: 0.0618739128112793\n",
      "Validation Loss for fold 4: 0.06569894154866536\n",
      "Validation Loss for fold 4: 0.06310922776659329\n",
      "Validation Loss for fold 4: 0.061913122733434044\n",
      "Validation Loss for fold 4: 0.06347885107000668\n",
      "Validation Loss for fold 4: 0.06835394973556201\n",
      "Validation Loss for fold 4: 0.0658186028401057\n",
      "Validation Loss for fold 4: 0.06287539005279541\n",
      "Validation Loss for fold 4: 0.05693558479348818\n",
      "Validation Loss for fold 4: 0.0667218379676342\n",
      "Validation Loss for fold 4: 0.0690311739842097\n",
      "Validation Loss for fold 4: 0.060678157955408096\n",
      "Validation Loss for fold 4: 0.05883511280020078\n",
      "Validation Loss for fold 4: 0.0632200725376606\n",
      "Validation Loss for fold 4: 0.05966860552628835\n",
      "Validation Loss for fold 4: 0.056789866338173546\n",
      "Validation Loss for fold 4: 0.06077392399311066\n",
      "Validation Loss for fold 4: 0.0570266917347908\n",
      "--------------------------------\n",
      "FOLD 5\n",
      "--------------------------------\n",
      "Validation Loss for fold 5: 0.569132924079895\n",
      "Validation Loss for fold 5: 0.4160998264948527\n",
      "Validation Loss for fold 5: 0.32848499218622845\n",
      "Validation Loss for fold 5: 0.2650837649901708\n",
      "Validation Loss for fold 5: 0.2017571578423182\n",
      "Validation Loss for fold 5: 0.1900151421626409\n",
      "Validation Loss for fold 5: 0.15095698088407516\n",
      "Validation Loss for fold 5: 0.1469336301088333\n",
      "Validation Loss for fold 5: 0.14781532684961954\n",
      "Validation Loss for fold 5: 0.14427601794401804\n",
      "Validation Loss for fold 5: 0.12962445120016733\n",
      "Validation Loss for fold 5: 0.11868694672981898\n",
      "Validation Loss for fold 5: 0.11553305387496948\n",
      "Validation Loss for fold 5: 0.12271214028199513\n",
      "Validation Loss for fold 5: 0.12326140701770782\n",
      "Validation Loss for fold 5: 0.10820003350575765\n",
      "Validation Loss for fold 5: 0.10431557645400365\n",
      "Validation Loss for fold 5: 0.10593271255493164\n",
      "Validation Loss for fold 5: 0.12885363896687826\n",
      "Validation Loss for fold 5: 0.09926429390907288\n",
      "Validation Loss for fold 5: 0.10416181633869807\n",
      "Validation Loss for fold 5: 0.1037735790014267\n",
      "Validation Loss for fold 5: 0.10322801768779755\n",
      "Validation Loss for fold 5: 0.09641348073879878\n",
      "Validation Loss for fold 5: 0.10057220607995987\n",
      "Validation Loss for fold 5: 0.09643694758415222\n",
      "Validation Loss for fold 5: 0.09376918276151021\n",
      "Validation Loss for fold 5: 0.1003422886133194\n",
      "Validation Loss for fold 5: 0.10277705142895381\n",
      "Validation Loss for fold 5: 0.09014048924048741\n",
      "Validation Loss for fold 5: 0.09030347317457199\n",
      "Validation Loss for fold 5: 0.09425323208173116\n",
      "Validation Loss for fold 5: 0.09249205887317657\n",
      "Validation Loss for fold 5: 0.10368701318899791\n",
      "Validation Loss for fold 5: 0.09699082374572754\n",
      "Validation Loss for fold 5: 0.08953952044248581\n",
      "Validation Loss for fold 5: 0.09137601653734843\n",
      "Validation Loss for fold 5: 0.09161483744780223\n",
      "Validation Loss for fold 5: 0.08758029341697693\n",
      "Validation Loss for fold 5: 0.08149899418155353\n",
      "Validation Loss for fold 5: 0.0930281604329745\n",
      "Validation Loss for fold 5: 0.09151306251684825\n",
      "Validation Loss for fold 5: 0.09136846164862315\n",
      "Validation Loss for fold 5: 0.0890261506040891\n",
      "Validation Loss for fold 5: 0.09064417084058125\n",
      "Validation Loss for fold 5: 0.08710494140783946\n",
      "Validation Loss for fold 5: 0.08627231419086456\n",
      "Validation Loss for fold 5: 0.0917389343182246\n",
      "Validation Loss for fold 5: 0.0861680159966151\n",
      "Validation Loss for fold 5: 0.08424840122461319\n",
      "Validation Loss for fold 5: 0.08228107541799545\n",
      "Validation Loss for fold 5: 0.08835729211568832\n",
      "Validation Loss for fold 5: 0.0836773340900739\n",
      "Validation Loss for fold 5: 0.08634076019128163\n",
      "Validation Loss for fold 5: 0.08394719660282135\n",
      "Validation Loss for fold 5: 0.09040798246860504\n",
      "Validation Loss for fold 5: 0.08358115454514821\n",
      "Validation Loss for fold 5: 0.09165049344301224\n",
      "Validation Loss for fold 5: 0.09197794397672017\n",
      "Validation Loss for fold 5: 0.07454707597692807\n",
      "Validation Loss for fold 5: 0.07815000663201015\n",
      "Validation Loss for fold 5: 0.084597148001194\n",
      "Validation Loss for fold 5: 0.077042605727911\n",
      "Validation Loss for fold 5: 0.08769984791676204\n",
      "Validation Loss for fold 5: 0.08121931552886963\n",
      "Validation Loss for fold 5: 0.08866277585426967\n",
      "Validation Loss for fold 5: 0.08378521725535393\n",
      "Validation Loss for fold 5: 0.07811244825522105\n",
      "Validation Loss for fold 5: 0.08389020959536235\n",
      "Validation Loss for fold 5: 0.0815093566974004\n",
      "Validation Loss for fold 5: 0.07618215183417003\n",
      "Validation Loss for fold 5: 0.07815931489070256\n",
      "Validation Loss for fold 5: 0.08730629086494446\n",
      "Validation Loss for fold 5: 0.0736519197622935\n",
      "Validation Loss for fold 5: 0.07117365921537082\n",
      "Validation Loss for fold 5: 0.08198600510756175\n",
      "Validation Loss for fold 5: 0.07591167092323303\n",
      "Validation Loss for fold 5: 0.07109479109446208\n",
      "Validation Loss for fold 5: 0.07886865486701329\n",
      "Validation Loss for fold 5: 0.07608786722024281\n",
      "Validation Loss for fold 5: 0.07747477293014526\n",
      "Validation Loss for fold 5: 0.07701460520426433\n",
      "Validation Loss for fold 5: 0.07381357625126839\n",
      "Validation Loss for fold 5: 0.07454754660526912\n",
      "Validation Loss for fold 5: 0.07721579819917679\n",
      "Validation Loss for fold 5: 0.07021137823661168\n",
      "Validation Loss for fold 5: 0.0809944396217664\n",
      "Validation Loss for fold 5: 0.07111661384503047\n",
      "Validation Loss for fold 5: 0.0787246401111285\n",
      "Validation Loss for fold 5: 0.07517019659280777\n",
      "Validation Loss for fold 5: 0.08194230124354362\n",
      "Validation Loss for fold 5: 0.06715847179293633\n",
      "Validation Loss for fold 5: 0.07440646986166637\n",
      "Validation Loss for fold 5: 0.07456195602814357\n",
      "Validation Loss for fold 5: 0.07224736362695694\n",
      "Validation Loss for fold 5: 0.07601402948300044\n",
      "Validation Loss for fold 5: 0.07353205482165019\n",
      "Validation Loss for fold 5: 0.06809002657731374\n",
      "Validation Loss for fold 5: 0.07205870499213536\n",
      "Validation Loss for fold 5: 0.06789312635858853\n",
      "--------------------------------\n",
      "FOLD 6\n",
      "--------------------------------\n",
      "Validation Loss for fold 6: 0.4061740239461263\n",
      "Validation Loss for fold 6: 0.3248184521993001\n",
      "Validation Loss for fold 6: 0.24821915725866953\n",
      "Validation Loss for fold 6: 0.21351441740989685\n",
      "Validation Loss for fold 6: 0.18196756144364676\n",
      "Validation Loss for fold 6: 0.18088891605536142\n",
      "Validation Loss for fold 6: 0.1575113038221995\n",
      "Validation Loss for fold 6: 0.15090110898017883\n",
      "Validation Loss for fold 6: 0.1288099686304728\n",
      "Validation Loss for fold 6: 0.11174813409646352\n",
      "Validation Loss for fold 6: 0.1124633252620697\n",
      "Validation Loss for fold 6: 0.11965099225441615\n",
      "Validation Loss for fold 6: 0.1149045079946518\n",
      "Validation Loss for fold 6: 0.12257642795642217\n",
      "Validation Loss for fold 6: 0.11066697537899017\n",
      "Validation Loss for fold 6: 0.10659666359424591\n",
      "Validation Loss for fold 6: 0.09712763130664825\n",
      "Validation Loss for fold 6: 0.09378304084142049\n",
      "Validation Loss for fold 6: 0.1052609955271085\n",
      "Validation Loss for fold 6: 0.08628353973229726\n",
      "Validation Loss for fold 6: 0.09091790268818538\n",
      "Validation Loss for fold 6: 0.09895701458056767\n",
      "Validation Loss for fold 6: 0.08576419949531555\n",
      "Validation Loss for fold 6: 0.0856352373957634\n",
      "Validation Loss for fold 6: 0.09003447741270065\n",
      "Validation Loss for fold 6: 0.08578716218471527\n",
      "Validation Loss for fold 6: 0.08343810091416042\n",
      "Validation Loss for fold 6: 0.08222755293051402\n",
      "Validation Loss for fold 6: 0.07499369233846664\n",
      "Validation Loss for fold 6: 0.07070719078183174\n",
      "Validation Loss for fold 6: 0.07592083265384038\n",
      "Validation Loss for fold 6: 0.07482948899269104\n",
      "Validation Loss for fold 6: 0.0789300004641215\n",
      "Validation Loss for fold 6: 0.07178469002246857\n",
      "Validation Loss for fold 6: 0.07296206802129745\n",
      "Validation Loss for fold 6: 0.07619752238194148\n",
      "Validation Loss for fold 6: 0.07265172650416692\n",
      "Validation Loss for fold 6: 0.06271508087714513\n",
      "Validation Loss for fold 6: 0.06119521955649058\n",
      "Validation Loss for fold 6: 0.07289529343446095\n",
      "Validation Loss for fold 6: 0.07120134433110555\n",
      "Validation Loss for fold 6: 0.07283990830183029\n",
      "Validation Loss for fold 6: 0.0745698648194472\n",
      "Validation Loss for fold 6: 0.06069887802004814\n",
      "Validation Loss for fold 6: 0.05966004108389219\n",
      "Validation Loss for fold 6: 0.06636012345552444\n",
      "Validation Loss for fold 6: 0.06415727982918422\n",
      "Validation Loss for fold 6: 0.06217743828892708\n",
      "Validation Loss for fold 6: 0.05992168063918749\n",
      "Validation Loss for fold 6: 0.060231237361828484\n",
      "Validation Loss for fold 6: 0.06352792059381802\n",
      "Validation Loss for fold 6: 0.0647618646423022\n",
      "Validation Loss for fold 6: 0.06028500944375992\n",
      "Validation Loss for fold 6: 0.06827798982461293\n",
      "Validation Loss for fold 6: 0.06312216073274612\n",
      "Validation Loss for fold 6: 0.06087827061613401\n",
      "Validation Loss for fold 6: 0.05922455837329229\n",
      "Validation Loss for fold 6: 0.05801778038342794\n",
      "Validation Loss for fold 6: 0.06001637503504753\n",
      "Validation Loss for fold 6: 0.058155037462711334\n",
      "Validation Loss for fold 6: 0.05765955771009127\n",
      "Validation Loss for fold 6: 0.06313368181387584\n",
      "Validation Loss for fold 6: 0.05423249676823616\n",
      "Validation Loss for fold 6: 0.05912699177861214\n",
      "Validation Loss for fold 6: 0.05613632624348005\n",
      "Validation Loss for fold 6: 0.053513605147600174\n",
      "Validation Loss for fold 6: 0.0525939054787159\n",
      "Validation Loss for fold 6: 0.06332013756036758\n",
      "Validation Loss for fold 6: 0.058757745971282326\n",
      "Validation Loss for fold 6: 0.057053469121456146\n",
      "Validation Loss for fold 6: 0.054124556481838226\n",
      "Validation Loss for fold 6: 0.056925926357507706\n",
      "Validation Loss for fold 6: 0.06278210133314133\n",
      "Validation Loss for fold 6: 0.05938412373264631\n",
      "Validation Loss for fold 6: 0.05314211050669352\n",
      "Validation Loss for fold 6: 0.05605878556768099\n",
      "Validation Loss for fold 6: 0.056916053096453346\n",
      "Validation Loss for fold 6: 0.05497753371795019\n",
      "Validation Loss for fold 6: 0.05425627529621124\n",
      "Validation Loss for fold 6: 0.054483977456887565\n",
      "Validation Loss for fold 6: 0.055412313590447106\n",
      "Validation Loss for fold 6: 0.058764119942982994\n",
      "Validation Loss for fold 6: 0.05301774044831594\n",
      "Validation Loss for fold 6: 0.052649397403001785\n",
      "Validation Loss for fold 6: 0.05654696375131607\n",
      "Validation Loss for fold 6: 0.05036642278234164\n",
      "Validation Loss for fold 6: 0.05414655183752378\n",
      "Validation Loss for fold 6: 0.055431349823872246\n",
      "Validation Loss for fold 6: 0.049189566324154534\n",
      "Validation Loss for fold 6: 0.05360866338014603\n",
      "Validation Loss for fold 6: 0.055047972748676934\n",
      "Validation Loss for fold 6: 0.05478762090206146\n",
      "Validation Loss for fold 6: 0.05063992117842039\n",
      "Validation Loss for fold 6: 0.05571253473560015\n",
      "Validation Loss for fold 6: 0.05227813248833021\n",
      "Validation Loss for fold 6: 0.053787052631378174\n",
      "Validation Loss for fold 6: 0.0550383614997069\n",
      "Validation Loss for fold 6: 0.05219907561937968\n",
      "Validation Loss for fold 6: 0.05177048593759537\n",
      "Validation Loss for fold 6: 0.05023333554466566\n",
      "--------------------------------\n",
      "FOLD 7\n",
      "--------------------------------\n",
      "Validation Loss for fold 7: 0.6395823359489441\n",
      "Validation Loss for fold 7: 0.5006837546825409\n",
      "Validation Loss for fold 7: 0.4168565571308136\n",
      "Validation Loss for fold 7: 0.3900926113128662\n",
      "Validation Loss for fold 7: 0.324550062417984\n",
      "Validation Loss for fold 7: 0.2811887462933858\n",
      "Validation Loss for fold 7: 0.29064703981081647\n",
      "Validation Loss for fold 7: 0.25020556151866913\n",
      "Validation Loss for fold 7: 0.2532975822687149\n",
      "Validation Loss for fold 7: 0.2092218150695165\n",
      "Validation Loss for fold 7: 0.21252300341924033\n",
      "Validation Loss for fold 7: 0.22811029851436615\n",
      "Validation Loss for fold 7: 0.20317880809307098\n",
      "Validation Loss for fold 7: 0.17852901419003805\n",
      "Validation Loss for fold 7: 0.20098691682020822\n",
      "Validation Loss for fold 7: 0.1997254639863968\n",
      "Validation Loss for fold 7: 0.19835547109444937\n",
      "Validation Loss for fold 7: 0.18572055300076803\n",
      "Validation Loss for fold 7: 0.19168884058793387\n",
      "Validation Loss for fold 7: 0.17555798093477884\n",
      "Validation Loss for fold 7: 0.18518005311489105\n",
      "Validation Loss for fold 7: 0.1772142251332601\n",
      "Validation Loss for fold 7: 0.16184217731157938\n",
      "Validation Loss for fold 7: 0.16428236166636148\n",
      "Validation Loss for fold 7: 0.1537120888630549\n",
      "Validation Loss for fold 7: 0.161484162012736\n",
      "Validation Loss for fold 7: 0.14926051100095114\n",
      "Validation Loss for fold 7: 0.15213322391112646\n",
      "Validation Loss for fold 7: 0.15975353121757507\n",
      "Validation Loss for fold 7: 0.14999694128831229\n",
      "Validation Loss for fold 7: 0.1515981157620748\n",
      "Validation Loss for fold 7: 0.15330549577871957\n",
      "Validation Loss for fold 7: 0.15391534566879272\n",
      "Validation Loss for fold 7: 0.16087295611699423\n",
      "Validation Loss for fold 7: 0.15147145092487335\n",
      "Validation Loss for fold 7: 0.1557995229959488\n",
      "Validation Loss for fold 7: 0.14271674553553262\n",
      "Validation Loss for fold 7: 0.13480868935585022\n",
      "Validation Loss for fold 7: 0.13972817858060202\n",
      "Validation Loss for fold 7: 0.1457185000181198\n",
      "Validation Loss for fold 7: 0.14438303808371225\n",
      "Validation Loss for fold 7: 0.13480592519044876\n",
      "Validation Loss for fold 7: 0.12796232849359512\n",
      "Validation Loss for fold 7: 0.14273602018753687\n",
      "Validation Loss for fold 7: 0.12482032428185146\n",
      "Validation Loss for fold 7: 0.14116347332795462\n",
      "Validation Loss for fold 7: 0.13677925616502762\n",
      "Validation Loss for fold 7: 0.13372073074181876\n",
      "Validation Loss for fold 7: 0.12937584519386292\n",
      "Validation Loss for fold 7: 0.13052680591742197\n",
      "Validation Loss for fold 7: 0.1338039735953013\n",
      "Validation Loss for fold 7: 0.13184823592503866\n",
      "Validation Loss for fold 7: 0.12997971226771673\n",
      "Validation Loss for fold 7: 0.12402704854806264\n",
      "Validation Loss for fold 7: 0.12410446753104527\n",
      "Validation Loss for fold 7: 0.12622676293055216\n",
      "Validation Loss for fold 7: 0.11781785388787587\n",
      "Validation Loss for fold 7: 0.11123812943696976\n",
      "Validation Loss for fold 7: 0.11859642714262009\n",
      "Validation Loss for fold 7: 0.11338654160499573\n",
      "Validation Loss for fold 7: 0.12214968850215276\n",
      "Validation Loss for fold 7: 0.11854639152685802\n",
      "Validation Loss for fold 7: 0.11793278406063716\n",
      "Validation Loss for fold 7: 0.10593983034292857\n",
      "Validation Loss for fold 7: 0.12440906961758931\n",
      "Validation Loss for fold 7: 0.11153598378101985\n",
      "Validation Loss for fold 7: 0.10694380352894466\n",
      "Validation Loss for fold 7: 0.10467738658189774\n",
      "Validation Loss for fold 7: 0.1129312738776207\n",
      "Validation Loss for fold 7: 0.10845398406187694\n",
      "Validation Loss for fold 7: 0.1065920020143191\n",
      "Validation Loss for fold 7: 0.11365406960248947\n",
      "Validation Loss for fold 7: 0.11073481788237889\n",
      "Validation Loss for fold 7: 0.1036775882045428\n",
      "Validation Loss for fold 7: 0.10275511691967647\n",
      "Validation Loss for fold 7: 0.1107716237505277\n",
      "Validation Loss for fold 7: 0.11194146672884624\n",
      "Validation Loss for fold 7: 0.10893810043732326\n",
      "Validation Loss for fold 7: 0.10355812559525172\n",
      "Validation Loss for fold 7: 0.10970945656299591\n",
      "Validation Loss for fold 7: 0.11051437010367711\n",
      "Validation Loss for fold 7: 0.10444616029659907\n",
      "Validation Loss for fold 7: 0.1010589748620987\n",
      "Validation Loss for fold 7: 0.10015087326367696\n",
      "Validation Loss for fold 7: 0.10576656212409337\n",
      "Validation Loss for fold 7: 0.09835940599441528\n",
      "Validation Loss for fold 7: 0.10214128841956456\n",
      "Validation Loss for fold 7: 0.09582209835449855\n",
      "Validation Loss for fold 7: 0.10551237811644872\n",
      "Validation Loss for fold 7: 0.10124252984921138\n",
      "Validation Loss for fold 7: 0.10350521902243297\n",
      "Validation Loss for fold 7: 0.10083797077337901\n",
      "Validation Loss for fold 7: 0.10692542046308517\n",
      "Validation Loss for fold 7: 0.09814481685558955\n",
      "Validation Loss for fold 7: 0.09175440172354381\n",
      "Validation Loss for fold 7: 0.09341877450545628\n",
      "Validation Loss for fold 7: 0.09772385905186336\n",
      "Validation Loss for fold 7: 0.09700331091880798\n",
      "Validation Loss for fold 7: 0.0947491725285848\n",
      "Validation Loss for fold 7: 0.08433358992139499\n",
      "--------------------------------\n",
      "FOLD 8\n",
      "--------------------------------\n",
      "Validation Loss for fold 8: 0.41136794288953143\n",
      "Validation Loss for fold 8: 0.35032852490743\n",
      "Validation Loss for fold 8: 0.2529586007197698\n",
      "Validation Loss for fold 8: 0.20806208749612173\n",
      "Validation Loss for fold 8: 0.17873594661553702\n",
      "Validation Loss for fold 8: 0.14123046894868216\n",
      "Validation Loss for fold 8: 0.14538500209649405\n",
      "Validation Loss for fold 8: 0.1279628723859787\n",
      "Validation Loss for fold 8: 0.12351387739181519\n",
      "Validation Loss for fold 8: 0.12189157555500667\n",
      "Validation Loss for fold 8: 0.10674331833918889\n",
      "Validation Loss for fold 8: 0.11002677430709203\n",
      "Validation Loss for fold 8: 0.10546932369470596\n",
      "Validation Loss for fold 8: 0.10030287752548854\n",
      "Validation Loss for fold 8: 0.09393570572137833\n",
      "Validation Loss for fold 8: 0.09774089604616165\n",
      "Validation Loss for fold 8: 0.09679270784060161\n",
      "Validation Loss for fold 8: 0.09203693767388661\n",
      "Validation Loss for fold 8: 0.09334224462509155\n",
      "Validation Loss for fold 8: 0.09446462740500768\n",
      "Validation Loss for fold 8: 0.09043201804161072\n",
      "Validation Loss for fold 8: 0.08081949253877004\n",
      "Validation Loss for fold 8: 0.07723557576537132\n",
      "Validation Loss for fold 8: 0.0892923300464948\n",
      "Validation Loss for fold 8: 0.08411962042252223\n",
      "Validation Loss for fold 8: 0.08138217280308406\n",
      "Validation Loss for fold 8: 0.07676307111978531\n",
      "Validation Loss for fold 8: 0.07755495607852936\n",
      "Validation Loss for fold 8: 0.07881029695272446\n",
      "Validation Loss for fold 8: 0.0772945483525594\n",
      "Validation Loss for fold 8: 0.07305016616980235\n",
      "Validation Loss for fold 8: 0.0762510746717453\n",
      "Validation Loss for fold 8: 0.07529716690381368\n",
      "Validation Loss for fold 8: 0.06679002195596695\n",
      "Validation Loss for fold 8: 0.069903913885355\n",
      "Validation Loss for fold 8: 0.07919370631376903\n",
      "Validation Loss for fold 8: 0.0753670185804367\n",
      "Validation Loss for fold 8: 0.074396513402462\n",
      "Validation Loss for fold 8: 0.06958947082360585\n",
      "Validation Loss for fold 8: 0.0770709899564584\n",
      "Validation Loss for fold 8: 0.06511769195397694\n",
      "Validation Loss for fold 8: 0.05913558478156725\n",
      "Validation Loss for fold 8: 0.06728346645832062\n",
      "Validation Loss for fold 8: 0.0674962227543195\n",
      "Validation Loss for fold 8: 0.06966882323225339\n",
      "Validation Loss for fold 8: 0.06852743153770764\n",
      "Validation Loss for fold 8: 0.061654181530078254\n",
      "Validation Loss for fold 8: 0.0602946604291598\n",
      "Validation Loss for fold 8: 0.06234487642844518\n",
      "Validation Loss for fold 8: 0.06098394840955734\n",
      "Validation Loss for fold 8: 0.06249000007907549\n",
      "Validation Loss for fold 8: 0.06568909933169682\n",
      "Validation Loss for fold 8: 0.0693963294227918\n",
      "Validation Loss for fold 8: 0.05689395094911257\n",
      "Validation Loss for fold 8: 0.0706736259162426\n",
      "Validation Loss for fold 8: 0.06451492259899776\n",
      "Validation Loss for fold 8: 0.06910006826122601\n",
      "Validation Loss for fold 8: 0.06755174820621808\n",
      "Validation Loss for fold 8: 0.0687973623474439\n",
      "Validation Loss for fold 8: 0.06259957204262416\n",
      "Validation Loss for fold 8: 0.05893710379799207\n",
      "Validation Loss for fold 8: 0.05789512147506078\n",
      "Validation Loss for fold 8: 0.0584913802643617\n",
      "Validation Loss for fold 8: 0.06692393744985263\n",
      "Validation Loss for fold 8: 0.060390960425138474\n",
      "Validation Loss for fold 8: 0.05937266101439794\n",
      "Validation Loss for fold 8: 0.06364788363377254\n",
      "Validation Loss for fold 8: 0.060278465350468956\n",
      "Validation Loss for fold 8: 0.0657093773285548\n",
      "Validation Loss for fold 8: 0.06248466297984123\n",
      "Validation Loss for fold 8: 0.05751132716735204\n",
      "Validation Loss for fold 8: 0.06196294476588567\n",
      "Validation Loss for fold 8: 0.06309640407562256\n",
      "Validation Loss for fold 8: 0.059000211457411446\n",
      "Validation Loss for fold 8: 0.0588068055609862\n",
      "Validation Loss for fold 8: 0.058257900178432465\n",
      "Validation Loss for fold 8: 0.055311691015958786\n",
      "Validation Loss for fold 8: 0.05672415345907211\n",
      "Validation Loss for fold 8: 0.05941067387660345\n",
      "Validation Loss for fold 8: 0.056764007856448494\n",
      "Validation Loss for fold 8: 0.05611161266764005\n",
      "Validation Loss for fold 8: 0.053461077312628426\n",
      "Validation Loss for fold 8: 0.06031313041845957\n",
      "Validation Loss for fold 8: 0.059391637643178306\n",
      "Validation Loss for fold 8: 0.055623079339663185\n",
      "Validation Loss for fold 8: 0.05784909054636955\n",
      "Validation Loss for fold 8: 0.06069488450884819\n",
      "Validation Loss for fold 8: 0.05413113782803217\n",
      "Validation Loss for fold 8: 0.05978702753782272\n",
      "Validation Loss for fold 8: 0.059113131215174995\n",
      "Validation Loss for fold 8: 0.05842958763241768\n",
      "Validation Loss for fold 8: 0.049152987698713936\n",
      "Validation Loss for fold 8: 0.059386940052111946\n",
      "Validation Loss for fold 8: 0.054397826393445335\n",
      "Validation Loss for fold 8: 0.0496862418949604\n",
      "Validation Loss for fold 8: 0.052121239403883614\n",
      "Validation Loss for fold 8: 0.05825114995241165\n",
      "Validation Loss for fold 8: 0.05092238883177439\n",
      "Validation Loss for fold 8: 0.051976703107357025\n",
      "Validation Loss for fold 8: 0.053054514030615486\n",
      "--------------------------------\n",
      "FOLD 9\n",
      "--------------------------------\n",
      "Validation Loss for fold 9: 0.2560283839702606\n",
      "Validation Loss for fold 9: 0.2108246237039566\n",
      "Validation Loss for fold 9: 0.1725963056087494\n",
      "Validation Loss for fold 9: 0.15496856967608133\n",
      "Validation Loss for fold 9: 0.1351082151134809\n",
      "Validation Loss for fold 9: 0.14654404173294702\n",
      "Validation Loss for fold 9: 0.12898782640695572\n",
      "Validation Loss for fold 9: 0.1174987182021141\n",
      "Validation Loss for fold 9: 0.11196283996105194\n",
      "Validation Loss for fold 9: 0.11508716642856598\n",
      "Validation Loss for fold 9: 0.10242006927728653\n",
      "Validation Loss for fold 9: 0.11426350722710292\n",
      "Validation Loss for fold 9: 0.10823147495587666\n",
      "Validation Loss for fold 9: 0.10301037381092708\n",
      "Validation Loss for fold 9: 0.10651498039563496\n",
      "Validation Loss for fold 9: 0.10108857850233714\n",
      "Validation Loss for fold 9: 0.08993740876515706\n",
      "Validation Loss for fold 9: 0.10741187383731206\n",
      "Validation Loss for fold 9: 0.08444331213831902\n",
      "Validation Loss for fold 9: 0.10028860221306483\n",
      "Validation Loss for fold 9: 0.08612448225418727\n",
      "Validation Loss for fold 9: 0.0930096631248792\n",
      "Validation Loss for fold 9: 0.09164242694775264\n",
      "Validation Loss for fold 9: 0.08639071385065715\n",
      "Validation Loss for fold 9: 0.0851927250623703\n",
      "Validation Loss for fold 9: 0.07898351550102234\n",
      "Validation Loss for fold 9: 0.08662105600039165\n",
      "Validation Loss for fold 9: 0.08683679501215617\n",
      "Validation Loss for fold 9: 0.0814705565571785\n",
      "Validation Loss for fold 9: 0.07182836905121803\n",
      "Validation Loss for fold 9: 0.0831056758761406\n",
      "Validation Loss for fold 9: 0.08351467549800873\n",
      "Validation Loss for fold 9: 0.07973578323920567\n",
      "Validation Loss for fold 9: 0.07554265856742859\n",
      "Validation Loss for fold 9: 0.07178378601868947\n",
      "Validation Loss for fold 9: 0.07004251703619957\n",
      "Validation Loss for fold 9: 0.07872186849514644\n",
      "Validation Loss for fold 9: 0.08215899020433426\n",
      "Validation Loss for fold 9: 0.07191919287045796\n",
      "Validation Loss for fold 9: 0.07171096404393514\n",
      "Validation Loss for fold 9: 0.06844119727611542\n",
      "Validation Loss for fold 9: 0.06855788578589757\n",
      "Validation Loss for fold 9: 0.07146839921673138\n",
      "Validation Loss for fold 9: 0.06978786488374074\n",
      "Validation Loss for fold 9: 0.0698368027806282\n",
      "Validation Loss for fold 9: 0.06303147412836552\n",
      "Validation Loss for fold 9: 0.06915022681156795\n",
      "Validation Loss for fold 9: 0.0733884001771609\n",
      "Validation Loss for fold 9: 0.06685881937543552\n",
      "Validation Loss for fold 9: 0.0698417102297147\n",
      "Validation Loss for fold 9: 0.07278956224521001\n",
      "Validation Loss for fold 9: 0.07279370973507564\n",
      "Validation Loss for fold 9: 0.07181731859842937\n",
      "Validation Loss for fold 9: 0.0702175498008728\n",
      "Validation Loss for fold 9: 0.06370606521765391\n",
      "Validation Loss for fold 9: 0.0682768573363622\n",
      "Validation Loss for fold 9: 0.06887007007996242\n",
      "Validation Loss for fold 9: 0.06527711575229962\n",
      "Validation Loss for fold 9: 0.06787953029076259\n",
      "Validation Loss for fold 9: 0.07409819215536118\n",
      "Validation Loss for fold 9: 0.06826345125834148\n",
      "Validation Loss for fold 9: 0.06615985309084256\n",
      "Validation Loss for fold 9: 0.07003812367717425\n",
      "Validation Loss for fold 9: 0.06568469355503719\n",
      "Validation Loss for fold 9: 0.05975779021779696\n",
      "Validation Loss for fold 9: 0.06911901384592056\n",
      "Validation Loss for fold 9: 0.06331238274772961\n",
      "Validation Loss for fold 9: 0.06229868407050768\n",
      "Validation Loss for fold 9: 0.06536860018968582\n",
      "Validation Loss for fold 9: 0.06141262004772822\n",
      "Validation Loss for fold 9: 0.06133731578787168\n",
      "Validation Loss for fold 9: 0.06344472865263621\n",
      "Validation Loss for fold 9: 0.0624597022930781\n",
      "Validation Loss for fold 9: 0.061499172200759254\n",
      "Validation Loss for fold 9: 0.05962569018205007\n",
      "Validation Loss for fold 9: 0.06505956997474034\n",
      "Validation Loss for fold 9: 0.05973748490214348\n",
      "Validation Loss for fold 9: 0.05952011669675509\n",
      "Validation Loss for fold 9: 0.06305735806624095\n",
      "Validation Loss for fold 9: 0.05866258839766184\n",
      "Validation Loss for fold 9: 0.06111918886502584\n",
      "Validation Loss for fold 9: 0.06392087539037068\n",
      "Validation Loss for fold 9: 0.06565618515014648\n",
      "Validation Loss for fold 9: 0.059399887919425964\n",
      "Validation Loss for fold 9: 0.060116744289795555\n",
      "Validation Loss for fold 9: 0.0562279149889946\n",
      "Validation Loss for fold 9: 0.0629723109304905\n",
      "Validation Loss for fold 9: 0.060298276444276176\n",
      "Validation Loss for fold 9: 0.06184130782882372\n",
      "Validation Loss for fold 9: 0.06503502031167348\n",
      "Validation Loss for fold 9: 0.06639101852973302\n",
      "Validation Loss for fold 9: 0.059036056200663246\n",
      "Validation Loss for fold 9: 0.05796545868118604\n",
      "Validation Loss for fold 9: 0.06129064535101255\n",
      "Validation Loss for fold 9: 0.06074936439593633\n",
      "Validation Loss for fold 9: 0.056076028694709144\n",
      "Validation Loss for fold 9: 0.05139335493246714\n",
      "Validation Loss for fold 9: 0.061223242431879044\n",
      "Validation Loss for fold 9: 0.057491314907868706\n",
      "Validation Loss for fold 9: 0.06195744127035141\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Avg Validation Loss</td><td>▄▃▂▂█▃▂▂▂▂▁▁▄▃▃▂▂▁▁▁▂▂▁▁▂▁▁▁▄▃▂▂▂▂▁▁▂▁▁▁</td></tr><tr><td>Epoch</td><td>▁▃▅▇▁▃▅▇▁▃▅▇▂▃▅▇▁▄▆▇▂▃▅█▂▄▆▇▂▄▆█▂▄▆█▂▄▆█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Avg Validation Loss</td><td>0.06196</td></tr><tr><td>Epoch</td><td>99</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">stellar-salad-32</strong> at: <a href='https://wandb.ai/jcrich/collaborative%20filter%20model/runs/dxwil5oh' target=\"_blank\">https://wandb.ai/jcrich/collaborative%20filter%20model/runs/dxwil5oh</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240311_215552-dxwil5oh/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Correct variable names and logic\n",
    "for fold, (train_ids, val_ids) in enumerate(kfold.split(train_ds)):\n",
    "    print(f'FOLD {fold}')\n",
    "    print('--------------------------------')\n",
    "\n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "    val_subsampler = torch.utils.data.SubsetRandomSampler(val_ids)\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=train_subsampler)\n",
    "    valloader = torch.utils.data.DataLoader(validation_ds, batch_size=BATCH_SIZE, sampler=val_subsampler)\n",
    "\n",
    "    model = MLPCollaborativeFilter(num_users + 1, num_movies + 1, embedding_dim=FEATURES)\n",
    "    optimiser = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        for user_indices, item_indices, ratings in trainloader:\n",
    "            optimiser.zero_grad()\n",
    "            outputs = model(user_indices, item_indices).squeeze()\n",
    "            loss = criterion(outputs, ratings)\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Average training loss over all batches\n",
    "        train_loss /= len(trainloader)\n",
    "        # Log training loss for the current epoch\n",
    "        wandb.log({\"Avg. Training Loss\": train_loss, \"Epoch\": epoch})\n",
    "        mlflow.log_metric(\"Avg. Training Loss\", train_loss, step=epoch)\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for user_indices, item_indices, ratings in valloader:\n",
    "                outputs = model(user_indices, item_indices).squeeze()\n",
    "                loss = criterion(outputs, ratings)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        avg_validation_loss = val_loss / len(valloader)\n",
    "        print(f'Validation Loss for fold {fold}: {avg_validation_loss}')\n",
    "        \n",
    "        if avg_validation_loss < best_val_loss:\n",
    "            best_val_loss = avg_validation_loss\n",
    "            mlflow.log_metric(\"Best Validation Loss\", best_val_loss, step=epoch)\n",
    "            mlflow.pytorch.log_model(model, \"model\")\n",
    "            # Save model state\n",
    "            torch.save(model.state_dict(), 'best_model_state.pth')\n",
    "            # If you also want to save the optimizer state along with the model:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimiser_state_dict': optimiser.state_dict(),\n",
    "                'loss': avg_validation_loss,\n",
    "            }, 'best_col_model_checkpoint.pth')\n",
    "            # Log the model checkpoint as an artifact\n",
    "            mlflow.log_artifact('best_col_model_checkpoint.pth')\n",
    "        \n",
    "        # Log validation loss for the current epoch\n",
    "        wandb.log({\"Avg Validation Loss\": avg_validation_loss, \"Epoch\": epoch})\n",
    "        mlflow.log_metric(\"Avg Validation Loss\", avg_validation_loss, step=epoch)\n",
    "    \n",
    "    print('--------------------------------')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "63a5e18c-653c-4d10-9bac-b3669a38c95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06580647492879316\n"
     ]
    }
   ],
   "source": [
    "#test data\n",
    "model.eval()  # Switch to evaluation mode\n",
    "test_loss = 0\n",
    "correct_predictions = 0  # Fixed variable name for clarity\n",
    "total_contexts = 0\n",
    "\n",
    "with torch.no_grad():  # Disable gradient computation\n",
    "    for user_ids, movie_ids, ratings in test_data_loader:\n",
    "        predictions = model(user_ids,movie_ids)  # Generate predictions\n",
    "        # Calculate and accumulate loss\n",
    "        loss = criterion(predictions, ratings)\n",
    "        test_loss += loss.item()\n",
    "        \n",
    "        # # Reshape predictions to match [batch_size, context_size, vocab_size]\n",
    "        # predictions = predictions.view(-1, context_size, VOCAB_SIZE)\n",
    "        \n",
    "        # # Get top prediction for each context position\n",
    "        # top_predictions = predictions.argmax(dim=2)\n",
    "        \n",
    "        # # Calculate correct predictions\n",
    "        # correct_preds = (top_predictions == context).float().sum()\n",
    "        # correct_predictions += correct_preds.item()  # Accumulate correct predictions\n",
    "        \n",
    "        # total_contexts += context.numel()  # Total number of context word positions evaluated\n",
    "\n",
    "# Calculate final metrics\n",
    "test_loss /= len(test_data_loader)  # Average loss per batch\n",
    "# print('correct predictions = ',correct_predictions)\n",
    "# print('out of  = ',total_contexts)\n",
    "# accuracy = correct_predictions / total_contexts  # Compute accuracy\n",
    "\n",
    "# print(f'Test set: Average loss: {test_loss:.4f}, Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Evaluate the model\n",
    "# (This would involve using a separate validation set or performing cross-validation)\n",
    "print(test_loss)\n",
    "\n",
    "mlflow.log_metric('Post training test loss',test_loss)\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10373ba-5557-46bf-aa37-8f4234b43def",
   "metadata": {},
   "source": [
    "## Preference ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377c97ca-a1c8-4a79-ad7f-f844c2b6ab59",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_user_id = 140440102666832"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb9de61-95a2-4c4d-bae9-78f17254013c",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_uid = encoder.encode(target_user_id,encoder.user_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb44d161-b86c-48e5-a2a8-228ee34d61fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_features(uid):\n",
    "\n",
    "    for i in range(0,len(all_ds)):\n",
    "\n",
    "        u,m,r = all_ds[i]\n",
    "        # print(u)\n",
    "        if u == uid:\n",
    "            yield [m,r]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca61297-1afc-439e-9a3a-81bff33d384f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ratings(e_uid,e_mid,model):\n",
    "    e_uid_tensor = torch.tensor(e_uid, dtype=torch.int64).unsqueeze(0)\n",
    "    movie_eid_tensor = torch.tensor(e_mid,dtype=torch.int64).unsqueeze(0)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        user_rating = model(e_uid_tensor,movie_eid_tensor)\n",
    "        return movie_eid_tensor,user_rating\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3780e9b-1c32-4f2b-a45e-14bdb9d35b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get user and features\n",
    "seen_features = set(tuple(fe) for fe in get_user_features(e_uid)) # Get user features if needed\n",
    "seen_movies = set(sf[0].item() for sf in seen_features)\n",
    "all_movies = set([num for num in range(30)])\n",
    "unseen_movies = all_movies - seen_movies\n",
    "unseen_features = {tuple(predict_ratings(e_uid,um,model)) for um in unseen_movies}\n",
    "all_features = seen_features | unseen_features\n",
    "\n",
    "sorted_set = sorted(all_features, key=lambda x: x[1])\n",
    "top_n = 10\n",
    "recommendations = list(filter(lambda x: x[0].item() in unseen_movies,sorted_set))\n",
    "n_recommendations = list(map(lambda x: x[0],recommendations[-1:top_n*-1:-1]))\n",
    "n_recommendations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3470ac7f-df21-40aa-94aa-32aa93606c9a",
   "metadata": {},
   "source": [
    "# Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9289eb5-57c3-42bd-ba7f-7c519590d8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all users, with all movie ratings.\n",
    "def generate_all_ratings(model,num_users,num_movies):\n",
    "    #build matrix\n",
    "\n",
    "    #data already encoded....\n",
    "    all_data = [['dummy',mnm,unm,'dummy','0/10'] for unm in range(0,num_users) for mnm in range(0,num_movies)]\n",
    "    # print(all_data)\n",
    "    # print([em for em in all_data])\n",
    "    fake_encoder = Encoder([did[2] for did in all_data],[did[1] for did in all_data])\n",
    "    all_ds = CFDataset(all_data,fake_encoder)\n",
    "    \n",
    "    data_loader = DataLoader(all_ds, batch_size=1)\n",
    "    for uid,mid,ra in data_loader:\n",
    "        # print(uid,mid)\n",
    "        prediction = model(uid,mid)\n",
    "        yield (uid.item(),mid.item(),prediction.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788b3ed2-c265-4b06-a5b9-53fa0c63b904",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ratings_tensor(data, num_users, num_movies):\n",
    "    # Initialize an empty tensor to hold the ratings\n",
    "    ratings_tensor = torch.zeros(num_users, num_movies)\n",
    "    \n",
    "    # Iterate over the data and fill the tensor\n",
    "    for entry in data:\n",
    "        user_id, movie_id, rating = entry\n",
    "        # Convert rating to float\n",
    "        rating = float(rating)\n",
    "        ratings_tensor[user_id, movie_id] = rating\n",
    "    \n",
    "    return ratings_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd12391-a846-475e-be55-b2516a0ef7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Generate Predictions\n",
    "predictions = [ra for ra in generate_all_ratings(model,num_users,num_movies)]\n",
    "# print(predictions)\n",
    "ratings_tensor = create_ratings_tensor(predictions,num_users,num_movies)\n",
    "print(ratings_tensor)\n",
    "# predictions[0]\n",
    "\n",
    "# ratings_tensor.shape\n",
    "\n",
    "# ratings_tensor[0]\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Assuming predicted_ratings is your tensor of predicted ratings\n",
    "# predicted_ratings.shape should be (num_users, num_movies)\n",
    "\n",
    "# # Calculate cosine similarity\n",
    "# user_similarities = F.cosine_similarity(ratings_tensor, ratings_tensor, dim=1)\n",
    "\n",
    "# user_similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d0c474-3481-489e-b494-047c64b14925",
   "metadata": {},
   "source": [
    "## COSINE SIMILARITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cae1de-bc62-460e-b2db-f046961f4493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Normalize predicted ratings\n",
    "normalized_ratings = F.normalize(ratings_tensor, p=2, dim=1)\n",
    "\n",
    "# Step 2: Calculate cosine similarity\n",
    "user_similarities = torch.matmul(normalized_ratings, normalized_ratings.T)\n",
    "\n",
    "# Set diagonal elements to a large negative value to exclude self-similarity\n",
    "user_similarities.fill_diagonal_(-float('inf'))\n",
    "\n",
    "# You can optionally convert the similarities tensor to a numpy array for easier manipulation\n",
    "user_similarities_np = user_similarities.numpy()\n",
    "\n",
    "# Print or use the user similarities tensor\n",
    "print(user_similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb89e2b-202f-405a-a732-26cf09959925",
   "metadata": {},
   "source": [
    "### Matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e402f0-b39d-4599-9617-5fcf23d00a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_matches = {}\n",
    "for i in range(len(user_similarities)):\n",
    "    # Sort similarities for the current user i\n",
    "    ranked_users = torch.argsort(torch.tensor(user_similarities[i]), descending=True)\n",
    "    # Exclude self from top matches (optional)\n",
    "    top_matches[i] = ranked_users[ranked_users != i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d560016-14da-4170-be81-43e27b094043",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebc2399-8831-4bef-b722-b9405c776b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.decode(71,encoder.idx_to_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cb1dfe-e30f-407f-accb-367b5bce66a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#decoded\n",
    "\n",
    "decoded_matches = {encoder.decode(k,encoder.idx_to_user):[encoder.decode(ve,encoder.idx_to_user) for ve in v.tolist()] for k,v in top_matches.items()}\n",
    "decoded_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73632e60-b177-479e-8e78-c6eea82ea863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Select Top Matches\n",
    "N = 5  # Number of top matches to select\n",
    "for user, similar_users in decoded_matches.items():\n",
    "    decoded_matches[user] = similar_users[:N]\n",
    "\n",
    "decoded_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7f3715-96e0-44a4-9a49-eee4e94bfaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#decode...."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c7c0fa-b24f-4418-b5ca-e1713254be27",
   "metadata": {},
   "source": [
    "## FINALLY FIND THE MATCHES...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd884b67-4541-453a-8bdf-87adf9d4ca39",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_user = 140440102666832\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0929f29-14e7-4cfa-acc9-9a741b7a5187",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_matches[target_user]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ed138f-43a0-4e9c-a66d-c7030de6e662",
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbour = 140440115331424"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a6a7ef-f019-4273-bc7c-c63dae3a104d",
   "metadata": {},
   "source": [
    "## Now see how they relate to the database..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b89fe2d-944d-43bc-b2a2-81af64bd6ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = db.get_table_values('Reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df7b55f-d93f-4a8d-b4c3-f0b1ea9ddf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2b3f3b-a7f8-48a6-b772-23c522f5ab85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reviews_info(user_id,reviews):\n",
    "    # print(db.get_table_values('Users'))\n",
    "    info = {}\n",
    "    info[user_id] = list(filter(lambda x :x[0] == user_id,db.get_table_values('Users')))[0][1]\n",
    "\n",
    "    \n",
    "    #TODO finish work...\n",
    "    mov_tab = db.get_table_values('Movies')\n",
    "    \n",
    "    for review in reviews:\n",
    "        mov_id = review[1]\n",
    "        info[mov_id] = {'title':list(filter(lambda x :x[0] == mov_id,mov_tab))[0][1],'rating':review[4]}\n",
    "        \n",
    "    return info\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef46e616-1299-4659-8e09-f6d3fdf44b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89593c73-41e3-4a6e-8e9c-61fd41b372dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_reviews = list(filter(lambda rv: rv[2] == target_user,reviews))\n",
    "neighbour_reviews = list(filter(lambda rv: rv[2] == neighbour,reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fd7bda-4f06-41de-89d2-7e84ce851863",
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_info = reviews_info(target_user,target_reviews)\n",
    "neigh_info = reviews_info(neighbour,neighbour_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cf7940-a7bf-4443-87b6-abd8d7d47bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78c8ccb-97d3-439a-ac8d-28ebe5e8ba91",
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6864fda6-f897-4c0a-8cf8-405e8cadd208",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae878490-881c-4345-9040-934a38615e40",
   "metadata": {},
   "source": [
    "# SAVE_MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef1a3d5-a607-4a92-82a3-237a07f15ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653f0e2d-f90d-4bc4-b72c-2dfda1b2f542",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "menv",
   "language": "python",
   "name": "menv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
