{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82c7704c-0f82-4867-b3d3-5e3d7203a66d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from db.ipynb\n",
      "best_contentfiltermodel_checkpoint.pth\tmenv\n",
      "best_model_checkpoint.pth\t\tmlruns\n",
      "best_model_state.pth\t\t\tmovie.py\n",
      "cf_model.py\t\t\t\tpath_to_your_database.db\n",
      "config.py\t\t\t\t__pycache__\n",
      "dataset.py\t\t\t\tq.py\n",
      "db.ipynb\t\t\t\tREADME.md\n",
      "done.txt\t\t\t\ttest.db\n",
      "fdstests.ipynb\t\t\t\ttestsources.ipynb\n",
      "filemanager.py\t\t\t\ttodo.txt\n",
      "holocene\t\t\t\ttrain_colfilter.ipynb\n",
      "indie_letterboxd.db\t\t\ttrain_confilter.ipynb\n",
      "indie_letterboxd_v2.db\t\t\tuser.py\n",
      "letterboxd\t\t\t\twandb\n",
      "letterboxd.db\t\t\t\tweb_spider.py\n",
      "main.ipynb\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from cf_model import MLPContentFilter\n",
    "from dataset import ConFDataset, Encoder, split_data\n",
    "import import_ipynb\n",
    "import db\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f219e0c6-d6be-4be1-ab9b-1a3633377633",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_NAME = 'indie_letterboxd_v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1f44d3-357f-4ae9-8136-24a06b785f85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d056984-e4f5-48e8-8ed8-06d6dd448c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users: 2400\n",
      "\n",
      "Movies: 771\n",
      "\n",
      "Reviews: 18504\n"
     ]
    }
   ],
   "source": [
    "db.display_size(DB_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39e7667d-7c2d-47a7-9af2-aac60eae7c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genres\n",
      "sqlite_sequence\n",
      "Directors\n",
      "Actors\n",
      "Movies\n",
      "Countries\n",
      "MoviesGenres\n",
      "MoviesActors\n",
      "Users\n",
      "Reviews\n"
     ]
    }
   ],
   "source": [
    "db.show_tables(DB_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f3c6606-7f34-4a7e-a393-a8d6c4740761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a7b132b2-ba44-40aa-80ba-7854684f7469', 'Cadejo Blanco', '2021-09-12 00:00:00', '3d62a2c2-7770-483c-893d-a9bdb162bd69', 1)\n",
      "('2570aedf-c152-4818-a57d-62b3b2ccc871', '40 Years a Prisoner', '2020-09-17 00:00:00', '6faab4b7-5f9b-4c5d-8715-988f15612615', 2)\n",
      "('8265924a-4c3e-48be-a987-6b77423d7d61', 'Get the Hell Out', '2020-06-28 00:00:00', '01cbd45b-967c-42a9-8e13-8f03f69fbcec', 3)\n",
      "('477b80c6-6eca-43cb-b45d-95eb25caf893', 'The Melting Creatures', '2022-05-23 00:00:00', '95e18c09-d78a-4f4d-af19-66a3dd014ecc', 4)\n",
      "('e856777a-7010-44e8-bf47-4c0ff965a41a', 'Death of a Whistleblower', '2023-09-09 00:00:00', '8754ab6b-35fa-42fc-9e5d-50def00808d8', 5)\n"
     ]
    }
   ],
   "source": [
    "#movie_id, title, release_date, director_id, country_id\n",
    "db.display_table('Movies',DB_NAME,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "806d8ca7-80d8-419d-b713-dc0e7fbe081c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a7b132b2-ba44-40aa-80ba-7854684f7469', 1)\n",
      "('a7b132b2-ba44-40aa-80ba-7854684f7469', 2)\n",
      "('2570aedf-c152-4818-a57d-62b3b2ccc871', 3)\n",
      "('8265924a-4c3e-48be-a987-6b77423d7d61', 4)\n",
      "('8265924a-4c3e-48be-a987-6b77423d7d61', 5)\n"
     ]
    }
   ],
   "source": [
    "db.display_table('MoviesGenres',DB_NAME,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41294a6b-24cc-4fae-a687-027761a0f487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 'Crime')\n",
      "(2, 'Drama')\n",
      "(3, 'Documentary')\n",
      "(4, 'Horror')\n",
      "(5, 'Action')\n",
      "(6, 'Comedy')\n",
      "(7, 'Science Fiction')\n",
      "(8, 'Thriller')\n",
      "(9, 'Romance')\n",
      "(10, 'Music')\n"
     ]
    }
   ],
   "source": [
    "db.display_table('Genres',DB_NAME,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ecca4e4c-70ce-451c-ba70-3a28b8551b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 'Guatemala')\n",
      "(2, 'USA')\n",
      "(3, 'Taiwan')\n",
      "(4, 'Chile')\n",
      "(5, 'South Africa')\n"
     ]
    }
   ],
   "source": [
    "db.display_table('Countries',DB_NAME,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a738a0a8-7186-4792-a272-939e261d2688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('e5b13caf-817e-4161-ad39-b4a25bf3d367', 'Matias Kivimäki')\n",
      "('42e54389-1ba4-4a78-8e3c-856b71a92d12', 'Monica Johansson')\n",
      "('d9b17abb-9b5d-42e2-8de7-c3e356797052', 'Արշավիր Դուդուկչյան')\n",
      "('90fc05a6-a309-4ed9-82ab-5b2f1f97ce7f', 'रमेश दवाडी')\n",
      "('62e6acdc-aa52-4b80-a025-bb3fc3685512', 'लिटन मानन्धर')\n"
     ]
    }
   ],
   "source": [
    "db.display_table('Users',DB_NAME,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed8d52b-fc68-4be3-ac58-1536423f10d6",
   "metadata": {},
   "source": [
    "# Import from sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c22bbf3c-70f1-4c7c-a4f9-0fc01c31d1d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "near \".\": syntax error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 39\u001b[0m\n\u001b[1;32m      8\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'''\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124m            SELECT \u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124m                Users.user_id,\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m \n\u001b[1;32m     36\u001b[0m \u001b[38;5;124m        \u001b[39m\u001b[38;5;124m'''\u001b[39m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Execute the query\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m \u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Fetch all results\u001b[39;00m\n\u001b[1;32m     42\u001b[0m results \u001b[38;5;241m=\u001b[39m cursor\u001b[38;5;241m.\u001b[39mfetchall()\n",
      "\u001b[0;31mOperationalError\u001b[0m: near \".\": syntax error"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Connect to your database\n",
    "conn = sqlite3.connect(DB_NAME+'.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# SQL query\n",
    "query = '''\n",
    "            SELECT \n",
    "                Users.user_id,\n",
    "                Users.name AS user_name,\n",
    "                GROUP_CONCAT(DISTINCT Genres.genre_id) AS genre_ids,\n",
    "                GROUP_CONCAT(DISTINCT Genres.name) AS genre_names,\n",
    "                Countries.country_id,\n",
    "                Countries.country_name,\n",
    "                Movies.movie_id,\n",
    "                Movies.title AS movie_name,\n",
    "                Reviews.rating\n",
    "            FROM \n",
    "                Reviews\n",
    "            JOIN\n",
    "                Users ON Reviews.user_id = Users.user_id\n",
    "            JOIN \n",
    "                Movies ON Reviews.movie_id = Movies.movie_id\n",
    "            JOIN \n",
    "                Countries ON Movies.country_id = Countries.country_id\n",
    "            LEFT JOIN \n",
    "                MoviesGenres ON Movies.movie_id = MoviesGenres.movie_id\n",
    "            LEFT JOIN \n",
    "                Genres ON MoviesGenres.genre_id = Genres.genre_id\n",
    "            GROUP BY \n",
    "                Reviews.review_id, Reviews.rating\n",
    "            ORDER BY \n",
    "                Movies.movie_id, Users.user_id\n",
    "\n",
    "        '''\n",
    "\n",
    "# Execute the query\n",
    "cursor.execute(query)\n",
    "\n",
    "# Fetch all results\n",
    "results = cursor.fetchall()\n",
    "\n",
    "# Close the connection\n",
    "conn.close()\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8455a64-1027-457d-a8d3-01480b9d5b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf24b1b9-aa3e-49f0-9e08-de6c9f5556fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_lists = [list(t) for t in results]\n",
    "\n",
    "# Shuffle the list of lists\n",
    "random.shuffle(list_of_lists)\n",
    "\n",
    "# Convert the shuffled list of lists back to a list of tuples\n",
    "data = [tuple(lst) for lst in list_of_lists]\n",
    "\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86ae5c2-7497-47b3-b311-8c3d9d71c623",
   "metadata": {},
   "outputs": [],
   "source": [
    "#user,genre,country\n",
    "users_data = [did[1] for did in data]\n",
    "genres_data = [did[3].split(',') for did in data]#<---- need list type\n",
    "countries_data = [did[5] for did in data]\n",
    "ratings_data = [did[-1] for did in data]\n",
    "genres_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeeea3ff-4627-4952-8dc1-0556de9cc691",
   "metadata": {},
   "outputs": [],
   "source": [
    "#deal with genres..... make it a list????\n",
    "encoder = Encoder(users=users_data,genres=genres_data,countries=countries_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564d97ba-893d-4085-8b76-6ef2263dd263",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.one_hot_encode(['Drama', 'TV Movie'],'genres')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d000f0b-74fc-4434-9a34-81c459dbbc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.encode('Gaičiūnas, Milana','users')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c891d06-dc2b-4645-8167-e446ea030ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.vocab_to_idx['genres']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66072b78-d7c7-4b65-aa62-384dd3b4cb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.one_hot_encode(['Documentary','War','Thriller'],'genres')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0c5857-5a3f-4e7e-863a-0ef88dfd3609",
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = [(did[1],did[3].split(','),did[5],did[-1]) for did in data]\n",
    "datas[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5396f0ce-30c6-443b-a40f-7c6cab5e4e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ds = ConFDataset(datas,encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f108380-ba49-46db-836f-9f935668845e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ds.country_data[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6ac428-f74b-4a5b-9be7-63a3d07f3549",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.vocab_to_idx['genres']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a415ace-f031-429d-8df0-e53abf3a4693",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ds.country_data[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5c3077-dfa8-4cc1-a264-5c302a1bfb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ds.genre_data[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf266eb-fc63-4049-a562-a69592e4102a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = split_data(datas)\n",
    "train_data, validation_data = split_data(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf53a3f5-6315-4115-9efb-01b34b64aead",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891e9180-2991-4893-8034-747fe128c6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ConFDataset(train_data,encoder)\n",
    "test_ds = ConFDataset(test_data,encoder)\n",
    "validation_ds = ConFDataset(validation_data,encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ad8475-8b6b-4a7d-91df-831cb9153277",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25b68f3-3e7a-4a6f-b5e6-55fbdae29a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "train_data_loader = DataLoader(train_ds,batch_size=BATCH_SIZE,shuffle=False,drop_last=True)\n",
    "validation_data_loader = DataLoader(validation_ds,batch_size=BATCH_SIZE,shuffle=False,drop_last=True)\n",
    "test_data_loader = DataLoader(test_ds,batch_size=BATCH_SIZE,shuffle=False,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da85c29-3ce0-49a8-9a1e-c1900567b2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ds[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5817c8d-e427-49ed-986c-eece7c665dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ds in test_data_loader:\n",
    "#     print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36edabff-0087-49fd-9cbf-c22e06c27b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_users = len(encoder.vocab_to_idx['users'])\n",
    "print(num_users)\n",
    "\n",
    "num_mv_features = len(all_ds[0][1])\n",
    "print(num_mv_features)\n",
    "\n",
    "FEATURES = 700\n",
    "model = MLPContentFilter(num_users, num_mv_features, embedding_dim=FEATURES)\n",
    "\n",
    "LEARNING_RATE = 0.001\n",
    "criterion = nn.MSELoss()\n",
    "#weight decay L2 regularization\n",
    "# optimiser = optim.SGD(model.parameters(), lr=0.001,weight_decay=1e-5)\\\n",
    "L2_REGULARIZATION = 0.1\n",
    "optimiser = optim.SGD(model.parameters(), lr=LEARNING_RATE,weight_decay=L2_REGULARIZATION)\n",
    "EPOCHS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a21177-a6c0-482f-91a4-b0ee9cf4efaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# Optional: Set MLflow experiment\n",
    "mlflow.set_experiment(\"Finetuned Content Filter\")\n",
    "\n",
    "# Log model parameters (example)\n",
    "mlflow.start_run()\n",
    "mlflow.log_param(\"epochs\", EPOCHS)\n",
    "mlflow.log_param(\"optimiser\", type(optimiser).__name__)\n",
    "mlflow.log_param(\"learning rate\", LEARNING_RATE)\n",
    "mlflow.log_param(\"batch size\", BATCH_SIZE)\n",
    "mlflow.log_param(\"L2 regularization\", L2_REGULARIZATION)\n",
    "mlflow.log_param(\"drop out\", 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623dc331-6d00-41db-b670-fe7c76a9d10b",
   "metadata": {},
   "source": [
    "# PRETEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f13953d-a5bc-4fd2-8d48-3c323be56cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test data\n",
    "model.eval()  # Switch to evaluation mode\n",
    "test_loss = 0\n",
    "correct_predictions = 0  # Fixed variable name for clarity\n",
    "total_contexts = 0\n",
    "\n",
    "with torch.no_grad():  # Disable gradient computation\n",
    "    for user_ids, movie_features, ratings in test_data_loader:\n",
    "        \n",
    "        predictions = model(user_ids,movie_features)  # Generate predictions\n",
    "        # Calculate and accumulate loss\n",
    "        loss = criterion(predictions, ratings)\n",
    "        test_loss += loss.item()\n",
    "        \n",
    "        # # Reshape predictions to match [batch_size, context_size, vocab_size]\n",
    "        # predictions = predictions.view(-1, context_size, VOCAB_SIZE)\n",
    "        \n",
    "        # # Get top prediction for each context position\n",
    "        # top_predictions = predictions.argmax(dim=2)\n",
    "        \n",
    "        # # Calculate correct predictions\n",
    "        # correct_preds = (top_predictions == context).float().sum()\n",
    "        # correct_predictions += correct_preds.item()  # Accumulate correct predictions\n",
    "        \n",
    "        # total_contexts += context.numel()  # Total number of context word positions evaluated\n",
    "\n",
    "# Calculate final metrics\n",
    "test_loss /= len(test_data_loader)  # Average loss per batch\n",
    "# print('correct predictions = ',correct_predictions)\n",
    "# print('out of  = ',total_contexts)\n",
    "# accuracy = correct_predictions / total_contexts  # Compute accuracy\n",
    "\n",
    "# print(f'Test set: Average loss: {test_loss:.4f}, Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Evaluate the model\n",
    "# (This would involve using a separate validation set or performing cross-validation)\n",
    "print(test_loss)\n",
    "mlflow.log_metric('pretraining test loss',test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39e0f67-b1d4-4a01-826a-4e37d9fb5127",
   "metadata": {},
   "source": [
    "## wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000ffd1c-94ac-4276-bede-a4b7f186794f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "import random\n",
    "\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    entity=\"jcrich\",\n",
    "    project=\"content-filter\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"architecture\": \"content filter\",\n",
    "    \"dataset\": \"imdb\",\n",
    "    \"epochs\": EPOCHS,\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef218b7-74b1-4769-9175-cfc7a9fa2f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to keep track of losses and epochs\n",
    "\n",
    "train_losses = []\n",
    "validation_losses = []\n",
    "epochs = []\n",
    "\n",
    "#get it at it's finest....\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_epoch_loss = 0\n",
    "    for batch_idx, (user_ids, movie_features, ratings) in tqdm(enumerate(train_data_loader), total=len(train_data_loader), desc=f'Epoch {epoch}'):\n",
    "\n",
    "        optimiser.zero_grad()\n",
    "        predictions = model(user_ids, movie_features)\n",
    "        loss = criterion(predictions, ratings)\n",
    "\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        \n",
    "        batch_loss = loss.item()\n",
    "        total_epoch_loss += batch_loss\n",
    "        \n",
    "    # After all batches, calculate average loss for the epoch\n",
    "    avg_epoch_loss = total_epoch_loss / len(train_data_loader)\n",
    "    train_losses.append(avg_epoch_loss)\n",
    "    epochs.append(epoch)  # Append the current epoch number\n",
    "    \n",
    "    wandb.log({\"Training Loss\": avg_epoch_loss, \"Epoch\": epoch})\n",
    "    mlflow.log_metric(\"Training Loss\", avg_epoch_loss, step=epoch)\n",
    "    # print(f'Epoch: {epoch}, Average Training Loss: {avg_epoch_loss}')\n",
    "\n",
    "    # Start validation phase\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_validation_loss = 0\n",
    "    with torch.no_grad():  # Turn off gradients for validation, saves memory and computations\n",
    "        for batch_idx, (user_ids, movie_features, ratings) in enumerate(validation_data_loader):\n",
    "            predictions = model(user_ids, movie_features)\n",
    "            loss = criterion(predictions, ratings)\n",
    "            total_validation_loss += loss.item()\n",
    "\n",
    "    \n",
    "    \n",
    "    # Calculate and log validation loss after the epoch\n",
    "    avg_validation_loss = total_validation_loss / len(validation_data_loader)\n",
    "    validation_losses.append(avg_validation_loss)\n",
    "\n",
    "    # Save the model if this epoch has the best validation loss so far\n",
    "    if avg_validation_loss < best_val_loss:\n",
    "        best_val_loss = avg_validation_loss\n",
    "        mlflow.log_metric(\"Best Validation Loss\", best_val_loss, step=epoch)\n",
    "        mlflow.pytorch.log_model(model, \"model\")\n",
    "        # Save model state\n",
    "        torch.save(model.state_dict(), 'best_model_state.pth')\n",
    "        # If you also want to save the optimizer state along with the model:\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimiser_state_dict': optimiser.state_dict(),\n",
    "            'loss': avg_validation_loss,\n",
    "        }, 'best_contentfiltermodel_checkpoint.pth')\n",
    "        # Log the model checkpoint as an artifact\n",
    "        mlflow.log_artifact('best_contentfiltermodel_checkpoint.pth')\n",
    "    \n",
    "    wandb.log({\"Validation Loss\": avg_validation_loss, \"Epoch\": epoch})\n",
    "    mlflow.log_metric(\"Validation Loss\", avg_validation_loss, step=epoch)\n",
    "    # print(f'Epoch: {epoch}, Average Validation Loss: {avg_validation_loss}')    \n",
    "    # After training and validation phases, log the losses with the epoch number\n",
    "    wandb.log({\"Epoch\": epoch, \"Training Loss\": avg_epoch_loss, \"Validation Loss\": avg_validation_loss})\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "63a5e18c-653c-4d10-9bac-b3669a38c95b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18406490683555604"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test data\n",
    "model.eval()  # Switch to evaluation mode\n",
    "test_loss = 0\n",
    "correct_predictions = 0  # Fixed variable name for clarity\n",
    "total_contexts = 0\n",
    "\n",
    "with torch.no_grad():  # Disable gradient computation\n",
    "    for user_ids, movie_features, ratings in test_data_loader:\n",
    "        predictions = model(user_ids,movie_features)  # Generate predictions\n",
    "        # Calculate and accumulate loss\n",
    "        loss = criterion(predictions, ratings)\n",
    "        test_loss += loss.item()\n",
    "        \n",
    "        # # Reshape predictions to match [batch_size, context_size, vocab_size]\n",
    "        # predictions = predictions.view(-1, context_size, VOCAB_SIZE)\n",
    "        \n",
    "        # # Get top prediction for each context position\n",
    "        # top_predictions = predictions.argmax(dim=2)\n",
    "        \n",
    "        # # Calculate correct predictions\n",
    "        # correct_preds = (top_predictions == context).float().sum()\n",
    "        # correct_predictions += correct_preds.item()  # Accumulate correct predictions\n",
    "        \n",
    "        # total_contexts += context.numel()  # Total number of context word positions evaluated\n",
    "\n",
    "# Calculate final metrics\n",
    "test_loss /= len(test_data_loader)  # Average loss per batch\n",
    "# print('correct predictions = ',correct_predictions)\n",
    "# print('out of  = ',total_contexts)\n",
    "# accuracy = correct_predictions / total_contexts  # Compute accuracy\n",
    "\n",
    "# print(f'Test set: Average loss: {test_loss:.4f}, Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Evaluate the model\n",
    "# (This would involve using a separate validation set or performing cross-validation)\n",
    "print(test_loss)\n",
    "mlflow.log_metric('evaluation posttraining test loss',test_loss)\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0d5603f5-fc2f-441f-a8bb-e020a30ec927",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">genial-fog-27</strong> at: <a href='https://wandb.ai/jcrich/collaborative%20filter%20model/runs/3u4ya87q' target=\"_blank\">https://wandb.ai/jcrich/collaborative%20filter%20model/runs/3u4ya87q</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240310_223623-3u4ya87q/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f10373ba-5557-46bf-aa37-8f4234b43def",
   "metadata": {},
   "source": [
    "## Preference ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "377c97ca-a1c8-4a79-ad7f-f844c2b6ab59",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_user_id = 140440102666832"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1fb9de61-95a2-4c4d-bae9-78f17254013c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Encoder' object has no attribute 'user_to_idx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m e_uid \u001b[38;5;241m=\u001b[39m encoder\u001b[38;5;241m.\u001b[39mencode(target_user_id,\u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser_to_idx\u001b[49m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Encoder' object has no attribute 'user_to_idx'"
     ]
    }
   ],
   "source": [
    "e_uid = encoder.encode(target_user_id,encoder.user_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb44d161-b86c-48e5-a2a8-228ee34d61fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_features(uid):\n",
    "\n",
    "    for i in range(0,len(all_ds)):\n",
    "\n",
    "        u,m,r = all_ds[i]\n",
    "        # print(u)\n",
    "        if u == uid:\n",
    "            yield [m,r]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca61297-1afc-439e-9a3a-81bff33d384f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ratings(e_uid,e_mid,model):\n",
    "    e_uid_tensor = torch.tensor(e_uid, dtype=torch.int64).unsqueeze(0)\n",
    "    movie_eid_tensor = torch.tensor(e_mid,dtype=torch.int64).unsqueeze(0)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        user_rating = model(e_uid_tensor,movie_eid_tensor)\n",
    "        return movie_eid_tensor,user_rating\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3780e9b-1c32-4f2b-a45e-14bdb9d35b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get user and features\n",
    "seen_features = set(tuple(fe) for fe in get_user_features(e_uid)) # Get user features if needed\n",
    "seen_movies = set(sf[0].item() for sf in seen_features)\n",
    "all_movies = set([num for num in range(30)])\n",
    "unseen_movies = all_movies - seen_movies\n",
    "unseen_features = {tuple(predict_ratings(e_uid,um,model)) for um in unseen_movies}\n",
    "all_features = seen_features | unseen_features\n",
    "\n",
    "sorted_set = sorted(all_features, key=lambda x: x[1])\n",
    "top_n = 10\n",
    "recommendations = list(filter(lambda x: x[0].item() in unseen_movies,sorted_set))\n",
    "n_recommendations = list(map(lambda x: x[0],recommendations[-1:top_n*-1:-1]))\n",
    "n_recommendations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3470ac7f-df21-40aa-94aa-32aa93606c9a",
   "metadata": {},
   "source": [
    "# Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9289eb5-57c3-42bd-ba7f-7c519590d8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all users, with all movie ratings.\n",
    "def generate_all_ratings(model,num_users,num_movies):\n",
    "    #build matrix\n",
    "\n",
    "    #data already encoded....\n",
    "    all_data = [['dummy',mnm,unm,'dummy','0/10'] for unm in range(0,num_users) for mnm in range(0,num_movies)]\n",
    "    # print(all_data)\n",
    "    # print([em for em in all_data])\n",
    "    fake_encoder = Encoder([did[2] for did in all_data],[did[1] for did in all_data])\n",
    "    all_ds = CFDataset(all_data,fake_encoder)\n",
    "    \n",
    "    data_loader = DataLoader(all_ds, batch_size=1)\n",
    "    for uid,mid,ra in data_loader:\n",
    "        # print(uid,mid)\n",
    "        prediction = model(uid,mid)\n",
    "        yield (uid.item(),mid.item(),prediction.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788b3ed2-c265-4b06-a5b9-53fa0c63b904",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ratings_tensor(data, num_users, num_movies):\n",
    "    # Initialize an empty tensor to hold the ratings\n",
    "    ratings_tensor = torch.zeros(num_users, num_movies)\n",
    "    \n",
    "    # Iterate over the data and fill the tensor\n",
    "    for entry in data:\n",
    "        user_id, movie_id, rating = entry\n",
    "        # Convert rating to float\n",
    "        rating = float(rating)\n",
    "        ratings_tensor[user_id, movie_id] = rating\n",
    "    \n",
    "    return ratings_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd12391-a846-475e-be55-b2516a0ef7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Generate Predictions\n",
    "predictions = [ra for ra in generate_all_ratings(model,num_users,num_movies)]\n",
    "# print(predictions)\n",
    "ratings_tensor = create_ratings_tensor(predictions,num_users,num_movies)\n",
    "print(ratings_tensor)\n",
    "# predictions[0]\n",
    "\n",
    "# ratings_tensor.shape\n",
    "\n",
    "# ratings_tensor[0]\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Assuming predicted_ratings is your tensor of predicted ratings\n",
    "# predicted_ratings.shape should be (num_users, num_movies)\n",
    "\n",
    "# # Calculate cosine similarity\n",
    "# user_similarities = F.cosine_similarity(ratings_tensor, ratings_tensor, dim=1)\n",
    "\n",
    "# user_similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d0c474-3481-489e-b494-047c64b14925",
   "metadata": {},
   "source": [
    "## COSINE SIMILARITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cae1de-bc62-460e-b2db-f046961f4493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Normalize predicted ratings\n",
    "normalized_ratings = F.normalize(ratings_tensor, p=2, dim=1)\n",
    "\n",
    "# Step 2: Calculate cosine similarity\n",
    "user_similarities = torch.matmul(normalized_ratings, normalized_ratings.T)\n",
    "\n",
    "# Set diagonal elements to a large negative value to exclude self-similarity\n",
    "user_similarities.fill_diagonal_(-float('inf'))\n",
    "\n",
    "# You can optionally convert the similarities tensor to a numpy array for easier manipulation\n",
    "user_similarities_np = user_similarities.numpy()\n",
    "\n",
    "# Print or use the user similarities tensor\n",
    "print(user_similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb89e2b-202f-405a-a732-26cf09959925",
   "metadata": {},
   "source": [
    "### Matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e402f0-b39d-4599-9617-5fcf23d00a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_matches = {}\n",
    "for i in range(len(user_similarities)):\n",
    "    # Sort similarities for the current user i\n",
    "    ranked_users = torch.argsort(torch.tensor(user_similarities[i]), descending=True)\n",
    "    # Exclude self from top matches (optional)\n",
    "    top_matches[i] = ranked_users[ranked_users != i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d560016-14da-4170-be81-43e27b094043",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebc2399-8831-4bef-b722-b9405c776b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.decode(71,encoder.idx_to_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cb1dfe-e30f-407f-accb-367b5bce66a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#decoded\n",
    "\n",
    "decoded_matches = {encoder.decode(k,encoder.idx_to_user):[encoder.decode(ve,encoder.idx_to_user) for ve in v.tolist()] for k,v in top_matches.items()}\n",
    "decoded_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73632e60-b177-479e-8e78-c6eea82ea863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Select Top Matches\n",
    "N = 5  # Number of top matches to select\n",
    "for user, similar_users in decoded_matches.items():\n",
    "    decoded_matches[user] = similar_users[:N]\n",
    "\n",
    "decoded_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7f3715-96e0-44a4-9a49-eee4e94bfaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#decode...."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c7c0fa-b24f-4418-b5ca-e1713254be27",
   "metadata": {},
   "source": [
    "## FINALLY FIND THE MATCHES...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd884b67-4541-453a-8bdf-87adf9d4ca39",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_user = 140440102666832\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0929f29-14e7-4cfa-acc9-9a741b7a5187",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_matches[target_user]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ed138f-43a0-4e9c-a66d-c7030de6e662",
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbour = 140440115331424"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a6a7ef-f019-4273-bc7c-c63dae3a104d",
   "metadata": {},
   "source": [
    "## Now see how they relate to the database..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b89fe2d-944d-43bc-b2a2-81af64bd6ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = db.get_table_values('Reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df7b55f-d93f-4a8d-b4c3-f0b1ea9ddf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2b3f3b-a7f8-48a6-b772-23c522f5ab85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reviews_info(user_id,reviews):\n",
    "    # print(db.get_table_values('Users'))\n",
    "    info = {}\n",
    "    info[user_id] = list(filter(lambda x :x[0] == user_id,db.get_table_values('Users')))[0][1]\n",
    "\n",
    "    \n",
    "    #TODO finish work...\n",
    "    mov_tab = db.get_table_values('Movies')\n",
    "    \n",
    "    for review in reviews:\n",
    "        mov_id = review[1]\n",
    "        info[mov_id] = {'title':list(filter(lambda x :x[0] == mov_id,mov_tab))[0][1],'rating':review[4]}\n",
    "        \n",
    "    return info\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef46e616-1299-4659-8e09-f6d3fdf44b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89593c73-41e3-4a6e-8e9c-61fd41b372dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_reviews = list(filter(lambda rv: rv[2] == target_user,reviews))\n",
    "neighbour_reviews = list(filter(lambda rv: rv[2] == neighbour,reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fd7bda-4f06-41de-89d2-7e84ce851863",
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_info = reviews_info(target_user,target_reviews)\n",
    "neigh_info = reviews_info(neighbour,neighbour_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cf7940-a7bf-4443-87b6-abd8d7d47bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78c8ccb-97d3-439a-ac8d-28ebe5e8ba91",
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6864fda6-f897-4c0a-8cf8-405e8cadd208",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae878490-881c-4345-9040-934a38615e40",
   "metadata": {},
   "source": [
    "# SAVE_MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef1a3d5-a607-4a92-82a3-237a07f15ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653f0e2d-f90d-4bc4-b72c-2dfda1b2f542",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "menv",
   "language": "python",
   "name": "menv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
