{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a573191-c50c-46a7-be39-8d9e9ad5ae8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from db.ipynb\n",
      "best_col_model_checkpoint.pth\t\tmain.ipynb\n",
      "best_contentfiltermodel_checkpoint.pth\tmain.py\n",
      "best_model_checkpoint.pth\t\tmatching.ipynb\n",
      "best_model_state.pth\t\t\tmenv\n",
      "cf_model.py\t\t\t\tmlruns\n",
      "col_encoder.pkl\t\t\t\tmodels\n",
      "config.py\t\t\t\tmovie.py\n",
      "dataset.py\t\t\t\tpath_to_your_database.db\n",
      "db.ipynb\t\t\t\t__pycache__\n",
      "Dockerfile\t\t\t\tq.py\n",
      "done.txt\t\t\t\tREADME.md\n",
      "encoder.pkl\t\t\t\trequirements.txt\n",
      "fdstests.ipynb\t\t\t\tserver\n",
      "filemanager.py\t\t\t\ttest.db\n",
      "final_col_model_checkpoint.pth\t\ttestsources.ipynb\n",
      "final_con_model_checkpoint.pth\t\ttodo.txt\n",
      "holocenemodels\t\t\t\ttrain_colfilter.ipynb\n",
      "indie_letterboxd.db\t\t\ttrain_confilter.ipynb\n",
      "indie_letterboxd_v2.db\t\t\tuser.py\n",
      "letterboxd\t\t\t\twandb\n",
      "letterboxd.db\t\t\t\tweb_spider.py\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from cf_model import MLPCollaborativeFilter\n",
    "from dataset import ColFDataset, Encoder, split_data\n",
    "import import_ipynb\n",
    "import db\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4967b732-0b9e-4747-a84f-07e3e8f4dd71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users: 2400\n",
      "\n",
      "Movies: 771\n",
      "\n",
      "Reviews: 18504\n"
     ]
    }
   ],
   "source": [
    "DB_NAME = 'indie_letterboxd_v2'\n",
    "db.display_size(DB_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28be6c6f-cf28-486e-9aa5-78b8ff780a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(DB_NAME+'.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# SQL query to fetch user name and ID, movie name and ID, and rating from reviews\n",
    "query = '''\n",
    "    SELECT \n",
    "        u.user_id,\n",
    "        u.name AS user_name, \n",
    "        m.movie_id,\n",
    "        m.title AS movie_title, \n",
    "        r.rating\n",
    "    FROM \n",
    "        Reviews r\n",
    "    JOIN Users u ON r.user_id = u.user_id\n",
    "    JOIN Movies m ON r.movie_id = m.movie_id\n",
    "    '''\n",
    "\n",
    "try:\n",
    "    cursor.execute(query)\n",
    "    reviews = cursor.fetchall()\n",
    "    \n",
    "    # for review in reviews:\n",
    "    #     print(\"User ID:\", review[0], \"| User Name:\", review[1], \"| Movie ID:\", review[2], \"| Movie Title:\", review[3], \"| Rating:\", review[4])\n",
    "except sqlite3.Error as e:\n",
    "    print(\"An error occurred:\", e)\n",
    "finally:\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8802ab01-9d57-4fdf-8341-215f3e0212ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Niina Moor', 'The Middle Man', 0.4), ('Maimunah Mangunsong', 'Stellar', 0.75), ('Clara Oliveira', 'Klokkenluider', 0.4), ('Bernard Jackson', 'In Between Dying', 0.4), ('Jeffrey Reese', 'Nomadland', 0.3), ('Roman Egorov', 'Volcano', 0.7), ('Maria Julia Martins', 'Human Resources', 1), ('Վահագն Ղազանչյան', 'Greed', 0.6), ('Anne Laine', 'True History of the Kelly Gang', 0.7), ('田中 花子', 'After Blue Dirty Paradise', 0.4)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(db.display_size())\n",
    "#TODO change shuffle and not shuffle\n",
    "\n",
    "# Assuming 'reviews' is a list of tuples and you've already created 'data'\n",
    "data = [tuple([did[1], did[3], did[-1]]) for did in reviews]\n",
    "\n",
    "# Shuffle 'data' in place with random.shuffle()\n",
    "random.shuffle(data)\n",
    "\n",
    "# Now 'data' is shuffled, and you can work with it\n",
    "print(data[:10])\n",
    "#found invalid values earlier.\n",
    "\n",
    "data = [pre for pre in data if pre[-1] > 0]\n",
    "data.count(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a8d4986-83f4-4fbc-8bba-429cda6a865e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test data is holdout for Kfolds cross data validation evaluation.\n",
    "train_data, test_data = split_data(data)\n",
    "train_data, validation_data = split_data(train_data,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2eccd133-b107-4dfa-9b16-b07bc58b375b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13241"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2438744-302d-4f6b-9a63-d49b2eadbd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_users_data = [ud[0] for ud in train_data]\n",
    "train_movies_data = [md[1] for md in train_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1b2649-c1fb-48bd-ab55-030b640199e7",
   "metadata": {},
   "source": [
    "## after splitting, only use the training data --- for encoding!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfbc8886-5861-4526-9488-ed53e502a2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(users=train_users_data,movies=train_movies_data)###<---- important\n",
    "import pickle\n",
    "\n",
    "# Assuming 'encoder' is your encoder object\n",
    "with open('col_encoder.pkl', 'wb') as file:\n",
    "    pickle.dump(encoder, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "319e8a9c-44a3-44d4-99c7-ad3b042220e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ColFDataset(train_data,encoder)\n",
    "test_ds = ColFDataset(test_data,encoder)\n",
    "validation_ds = ColFDataset(validation_data,encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af64661d-4749-45e3-a3a9-dcfdb27b43ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1471"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds.movie_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a97f9cb-f0f1-45db-b175-4293529fa52d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.movie_ids.count(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae3212ab-ac1f-4efa-b29a-a509247f08d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13241"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validation_ds.movie_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75750116-45b4-4af9-be6e-d3b828a05311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7136"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_ds.user_ids.count(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2a65e0d-f096-4312-b02e-2ccbbb1a38af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1826"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_ds.movie_ids.count(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cbc125dc-8fca-445a-8f90-ea504b004900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3679"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_ds.movie_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83a82ced-863d-4bd4-b00d-a99a24047c89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "508"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds.movie_ids.count(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75616c10-0c39-460b-a33d-9507b9043ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "train_data_loader = DataLoader(train_ds,batch_size=BATCH_SIZE,shuffle=False,drop_last=True)\n",
    "validation_data_loader = DataLoader(validation_ds,batch_size=BATCH_SIZE,shuffle=False,drop_last=True)\n",
    "test_data_loader = DataLoader(test_ds,batch_size=BATCH_SIZE,shuffle=False,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36edabff-0087-49fd-9cbf-c22e06c27b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users = len(encoder.vocab_to_idx['users'])\n",
    "num_movies = len(encoder.vocab_to_idx['movies'])\n",
    "FEATURES=700\n",
    "model = MLPCollaborativeFilter(num_users + 1, num_movies + 1, embedding_dim=FEATURES)\n",
    "\n",
    "\n",
    "LEARNING_RATE = 0.001\n",
    "criterion = nn.MSELoss()\n",
    "#weight decay L2 regularization\n",
    "# optimiser = optim.SGD(model.parameters(), lr=0.001,weight_decay=1e-5)\\\n",
    "L2_REGULARIZATION=0.1\n",
    "optimiser = optim.SGD(model.parameters(), lr=LEARNING_RATE,weight_decay=L2_REGULARIZATION)\n",
    "EPOCHS = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "157cd204-76fe-473a-9e57-a7adda48b185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18391\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1471"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(data))\n",
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0aea2d-2597-4a50-9b6d-d65231ed49eb",
   "metadata": {},
   "source": [
    "# EDA "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadfc0f4-b4df-4835-85b1-ebae07f11741",
   "metadata": {},
   "source": [
    "    Examine the data for patterns, anomalies, or characteristics.\n",
    "    Check for data quality issues such as missing values, outliers, or incorrect data types. <--- either predropped or imputed.\n",
    "    Get a sense of the distributions of your variables and the relationships between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1228065-7071-4b93-b460-a4204e6fbbda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  user             movie  ratings\n",
      "0           Niina Moor    The Middle Man     0.40\n",
      "1  Maimunah Mangunsong           Stellar     0.75\n",
      "2       Clara Oliveira     Klokkenluider     0.40\n",
      "3      Bernard Jackson  In Between Dying     0.40\n",
      "4        Jeffrey Reese         Nomadland     0.30\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 18391 entries, 0 to 18390\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   user     18391 non-null  object \n",
      " 1   movie    18391 non-null  object \n",
      " 2   ratings  18391 non-null  float64\n",
      "dtypes: float64(1), object(2)\n",
      "memory usage: 431.2+ KB\n",
      "None\n",
      "            ratings\n",
      "count  18391.000000\n",
      "mean       0.674751\n",
      "std        0.210540\n",
      "min        0.020000\n",
      "25%        0.540000\n",
      "50%        0.700000\n",
      "75%        0.800000\n",
      "max        1.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "column_names = ['user', 'movie','ratings']\n",
    "\n",
    "df = pd.DataFrame(data, columns=column_names)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(df.head())\n",
    "\n",
    "# Get a summary of the dataset\n",
    "print(df.info())\n",
    "\n",
    "# Generate descriptive statistics\n",
    "print(df.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c02bbd90-8966-44b8-9ddd-59d31ce3d82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user       0\n",
      "movie      0\n",
      "ratings    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "51da5313-6bec-42a9-835e-2c535197d382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x1500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 250x250 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Histograms for numeric features\n",
    "df.hist(bins=50, figsize=(20,15))\n",
    "plt.show()\n",
    "\n",
    "# Box plots for distributions and spotting outliers\n",
    "sns.boxplot(data=df)\n",
    "plt.show()\n",
    "\n",
    "# Pairplot to visualize pairwise relationships between features\n",
    "sns.pairplot(df)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6b05fd44-31ed-4c75-b0b2-5558d525bd0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>movie</th>\n",
       "      <th>ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [user, movie, ratings]\n",
       "Index: []"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['ratings'] == -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5fe39638-e29b-49ca-bf75-4d8d44ace1c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    14\n",
       "1     7\n",
       "2    13\n",
       "3    16\n",
       "4     9\n",
       "Name: title_length, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['title_length'] = df['movie'].str.len()\n",
    "df['title_length'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6c4fe575-c792-46de-9fac-c4e280e4889f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "df['title_length']\n",
    "# Correlation matrix\n",
    "\n",
    "corr_matrix = df[['title_length','ratings']].corr()\n",
    "\n",
    "# Heatmap of the correlation matrix\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\")\n",
    "plt.show()\n",
    "\n",
    "# Specifically look at correlations with the target variable if defined\n",
    "# target_corr = corr_matrix[\"YourTargetVariable\"].sort_values(ascending=False)\n",
    "# print(target_corr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dc364699-eaa9-4bd5-9e46-359b19c0271f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26\n",
      "                           user             movie  ratings  title_length\n",
      "17             Brandon Williams     Good Joe Bell      0.1            13\n",
      "103              Jolanta Akelis           Poolman      0.1             7\n",
      "263     Орлов Парамон Антонович            Africa      0.1             6\n",
      "344    Маслов Филимон Филатович     Good Joe Bell      0.1            13\n",
      "406                 Анжела Ярош  Dear Evan Hansen      0.1            16\n",
      "...                         ...               ...      ...           ...\n",
      "18051          पुजा राजकर्णिकार      Knuckle City      0.1            12\n",
      "18068        Claudia Dumitrescu    Simple Passion      0.1            14\n",
      "18123      Βαλάντης Χαραλάμπους        Quickening      0.1            10\n",
      "18247                       劉雅慧     Human Capital      0.1            13\n",
      "18272         კარინე ახვლედიანი       The Gravity      0.1            11\n",
      "\n",
      "[300 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Example of finding outliers for a 'feature_name'\n",
    "Q1 = df['ratings'].quantile(0.25)\n",
    "Q3 = df['ratings'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "print(IQR)\n",
    "outlier_condition = ((df['ratings'] < (Q1 - 1.5 * IQR)) | (df['ratings'] > (Q3 + 1.5 * IQR)))\n",
    "outliers = df[outlier_condition]\n",
    "print(outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5dbea2-ea83-4f1a-b206-60a993873cfe",
   "metadata": {},
   "source": [
    "## MLFLOw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ae300159-0298-450a-b512-ddf65ad3d207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///home/james/Desktop/torchex/movie-users/mlruns/827152334394031969', creation_time=1710099404696, experiment_id='827152334394031969', last_update_time=1710099404696, lifecycle_stage='active', name='Finetuned Collaborative Filter', tags={}>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# Optional: Set MLflow experiment\n",
    "mlflow.set_experiment(\"Finetuned Collaborative Filter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5bebaeb2-02a1-47d8-8f20-28d488b69201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Log model parameters (example)\n",
    "mlflow.start_run()\n",
    "mlflow.log_param(\"epochs\", EPOCHS)\n",
    "mlflow.log_param(\"optimizer\", type(optimiser).__name__)\n",
    "mlflow.log_param(\"learning rate\", LEARNING_RATE)\n",
    "mlflow.log_param(\"batch size\", BATCH_SIZE)\n",
    "mlflow.log_param(\"L2 regularization\", L2_REGULARIZATION)\n",
    "mlflow.log_param(\"drop out\", 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623dc331-6d00-41db-b670-fe7c76a9d10b",
   "metadata": {},
   "source": [
    "# PRETEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1f13953d-a5bc-4fd2-8d48-3c323be56cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2900565724100983\n"
     ]
    }
   ],
   "source": [
    "#test data\n",
    "model.eval()  # Switch to evaluation mode\n",
    "test_loss = 0\n",
    "correct_predictions = 0  # Fixed variable name for clarity\n",
    "total_contexts = 0\n",
    "\n",
    "with torch.no_grad():  # Disable gradient computation\n",
    "    for user_ids, movie_ids, ratings in test_data_loader:\n",
    "        predictions = model(user_ids,movie_ids)  # Generate predictions\n",
    "        # Calculate and accumulate loss\n",
    "        loss = criterion(predictions, ratings)\n",
    "        test_loss += loss.item()\n",
    "        \n",
    "\n",
    "test_loss /= len(test_data_loader)  # Average loss per batch\n",
    "\n",
    "print(test_loss)\n",
    "\n",
    "mlflow.log_metric('pretraining test loss',test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39e0f67-b1d4-4a01-826a-4e37d9fb5127",
   "metadata": {},
   "source": [
    "## wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "000ffd1c-94ac-4276-bede-a4b7f186794f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjcrich\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/james/Desktop/torchex/movie-users/wandb/run-20240312_162719-ow39d2po</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jcrich/collaborative%20filter%20model/runs/ow39d2po' target=\"_blank\">happy-armadillo-44</a></strong> to <a href='https://wandb.ai/jcrich/collaborative%20filter%20model' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jcrich/collaborative%20filter%20model' target=\"_blank\">https://wandb.ai/jcrich/collaborative%20filter%20model</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jcrich/collaborative%20filter%20model/runs/ow39d2po' target=\"_blank\">https://wandb.ai/jcrich/collaborative%20filter%20model/runs/ow39d2po</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/jcrich/collaborative%20filter%20model/runs/ow39d2po?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f38d103beb0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "import random\n",
    "\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    entity=\"jcrich\",\n",
    "    project=\"collaborative filter model\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"architecture\": \"collaborative filter\",\n",
    "    \"dataset\": \"imdb\",\n",
    "    \"epochs\": EPOCHS,\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959003c7-6d39-4392-b3f5-e9a75a36d0b2",
   "metadata": {},
   "source": [
    "# KFOLDS CROSS VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8631385b-7cfe-427b-b2db-427241171649",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPLit\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Configuration\n",
    "num_folds = 10\n",
    "data_size = len(test_ds)  # Assuming 'dataset' is a PyTorch Dataset\n",
    "\n",
    "# Define the K-fold Cross Validator\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8b0830cd-43f1-454c-8e81-72e5d73549fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0\n",
      "--------------------------------\n",
      "Validation Loss for fold 0: 0.5348609685897827\n",
      "Validation Loss for fold 0: 0.41624722878138226\n",
      "Validation Loss for fold 0: 0.3097187578678131\n",
      "Validation Loss for fold 0: 0.24142803251743317\n",
      "Validation Loss for fold 0: 0.18550364673137665\n",
      "Validation Loss for fold 0: 0.16286053756872812\n",
      "Validation Loss for fold 0: 0.1582765281200409\n",
      "Validation Loss for fold 0: 0.1365689883629481\n",
      "Validation Loss for fold 0: 0.14289681861797968\n",
      "Validation Loss for fold 0: 0.12026918431123097\n",
      "Validation Loss for fold 0: 0.11266972869634628\n",
      "Validation Loss for fold 0: 0.10975380490223567\n",
      "Validation Loss for fold 0: 0.0973427693049113\n",
      "Validation Loss for fold 0: 0.10335625956455867\n",
      "Validation Loss for fold 0: 0.10399427264928818\n",
      "Validation Loss for fold 0: 0.10314736515283585\n",
      "Validation Loss for fold 0: 0.10104122757911682\n",
      "Validation Loss for fold 0: 0.09289968758821487\n",
      "Validation Loss for fold 0: 0.09820746878782909\n",
      "Validation Loss for fold 0: 0.09975286076466243\n",
      "Validation Loss for fold 0: 0.10199997574090958\n",
      "Validation Loss for fold 0: 0.0964032659928004\n",
      "Validation Loss for fold 0: 0.09624386827150981\n",
      "Validation Loss for fold 0: 0.10077885786692302\n",
      "Validation Loss for fold 0: 0.09583257138729095\n",
      "Validation Loss for fold 0: 0.09645156810681026\n",
      "Validation Loss for fold 0: 0.10046820839246114\n",
      "Validation Loss for fold 0: 0.09258137394984563\n",
      "Validation Loss for fold 0: 0.1014396498600642\n",
      "Validation Loss for fold 0: 0.09206539640824\n",
      "Validation Loss for fold 0: 0.09985072662432988\n",
      "Validation Loss for fold 0: 0.10631638020277023\n",
      "Validation Loss for fold 0: 0.09072086960077286\n",
      "Validation Loss for fold 0: 0.0837508166829745\n",
      "Validation Loss for fold 0: 0.0959734246134758\n",
      "Validation Loss for fold 0: 0.09346628934144974\n",
      "Validation Loss for fold 0: 0.08626485367616017\n",
      "Validation Loss for fold 0: 0.10014195740222931\n",
      "Validation Loss for fold 0: 0.08647552380959193\n",
      "Validation Loss for fold 0: 0.08714660008748372\n",
      "Validation Loss for fold 0: 0.08700662602980931\n",
      "Validation Loss for fold 0: 0.09070396920045216\n",
      "Validation Loss for fold 0: 0.08557713031768799\n",
      "Validation Loss for fold 0: 0.08700979501008987\n",
      "Validation Loss for fold 0: 0.08740650862455368\n",
      "Validation Loss for fold 0: 0.08255277574062347\n",
      "Validation Loss for fold 0: 0.08293680846691132\n",
      "Validation Loss for fold 0: 0.08947402983903885\n",
      "Validation Loss for fold 0: 0.08139188836018245\n",
      "Validation Loss for fold 0: 0.08260861039161682\n",
      "Validation Loss for fold 0: 0.08456513285636902\n",
      "Validation Loss for fold 0: 0.08238337437311809\n",
      "Validation Loss for fold 0: 0.08291709174712499\n",
      "Validation Loss for fold 0: 0.08323997010787328\n",
      "Validation Loss for fold 0: 0.0837178702155749\n",
      "Validation Loss for fold 0: 0.08676210045814514\n",
      "Validation Loss for fold 0: 0.07973812023798625\n",
      "Validation Loss for fold 0: 0.07851142436265945\n",
      "Validation Loss for fold 0: 0.08520104736089706\n",
      "Validation Loss for fold 0: 0.08582702527443568\n",
      "Validation Loss for fold 0: 0.08428086588780086\n",
      "Validation Loss for fold 0: 0.07829347749551137\n",
      "Validation Loss for fold 0: 0.08053504923979442\n",
      "Validation Loss for fold 0: 0.07867717742919922\n",
      "Validation Loss for fold 0: 0.07772205770015717\n",
      "Validation Loss for fold 0: 0.07724562535683314\n",
      "Validation Loss for fold 0: 0.08172028015057246\n",
      "Validation Loss for fold 0: 0.07972983519236247\n",
      "Validation Loss for fold 0: 0.08206146707137425\n",
      "Validation Loss for fold 0: 0.08450418462355931\n",
      "Validation Loss for fold 0: 0.07920561979214351\n",
      "Validation Loss for fold 0: 0.07684476176897685\n",
      "Validation Loss for fold 0: 0.07836152364810307\n",
      "Validation Loss for fold 0: 0.07901412745316823\n",
      "Validation Loss for fold 0: 0.0827232797940572\n",
      "Validation Loss for fold 0: 0.07555095106363297\n",
      "Validation Loss for fold 0: 0.07129402707020442\n",
      "Validation Loss for fold 0: 0.07894759873549144\n",
      "Validation Loss for fold 0: 0.07585298766692479\n",
      "Validation Loss for fold 0: 0.07111736014485359\n",
      "Validation Loss for fold 0: 0.0792016088962555\n",
      "Validation Loss for fold 0: 0.06765755514303844\n",
      "Validation Loss for fold 0: 0.07356042663256328\n",
      "Validation Loss for fold 0: 0.08015429973602295\n",
      "Validation Loss for fold 0: 0.07134666045506795\n",
      "Validation Loss for fold 0: 0.06786193698644638\n",
      "Validation Loss for fold 0: 0.07644100859761238\n",
      "Validation Loss for fold 0: 0.07946790009737015\n",
      "Validation Loss for fold 0: 0.07330872863531113\n",
      "Validation Loss for fold 0: 0.07217310865720113\n",
      "Validation Loss for fold 0: 0.07711632798115413\n",
      "Validation Loss for fold 0: 0.06755716477831204\n",
      "Validation Loss for fold 0: 0.07298806061347325\n",
      "Validation Loss for fold 0: 0.07419846951961517\n",
      "Validation Loss for fold 0: 0.06671362494428952\n",
      "Validation Loss for fold 0: 0.07436809192101161\n",
      "Validation Loss for fold 0: 0.07453171163797379\n",
      "Validation Loss for fold 0: 0.0652565819521745\n",
      "Validation Loss for fold 0: 0.07625781123836835\n",
      "Validation Loss for fold 0: 0.0630275122821331\n",
      "Validation Loss for fold 0: 0.0701799566547076\n",
      "Validation Loss for fold 0: 0.07128077993790309\n",
      "Validation Loss for fold 0: 0.07295878728230794\n",
      "Validation Loss for fold 0: 0.06547378127773602\n",
      "Validation Loss for fold 0: 0.0656568706035614\n",
      "Validation Loss for fold 0: 0.0705387940009435\n",
      "Validation Loss for fold 0: 0.07284172624349594\n",
      "Validation Loss for fold 0: 0.06996845702330272\n",
      "Validation Loss for fold 0: 0.07116018484036128\n",
      "Validation Loss for fold 0: 0.06998827680945396\n",
      "Validation Loss for fold 0: 0.06528914098938306\n",
      "Validation Loss for fold 0: 0.06550395488739014\n",
      "Validation Loss for fold 0: 0.06614535798629124\n",
      "Validation Loss for fold 0: 0.07278784985343616\n",
      "Validation Loss for fold 0: 0.06968135883410771\n",
      "Validation Loss for fold 0: 0.06727928668260574\n",
      "Validation Loss for fold 0: 0.07445292919874191\n",
      "Validation Loss for fold 0: 0.06757985552151997\n",
      "Validation Loss for fold 0: 0.07051735495527585\n",
      "Validation Loss for fold 0: 0.07219896962245305\n",
      "Validation Loss for fold 0: 0.06866794327894847\n",
      "Validation Loss for fold 0: 0.07066406806310017\n",
      "Validation Loss for fold 0: 0.06928588822484016\n",
      "Validation Loss for fold 0: 0.06503335138161977\n",
      "Validation Loss for fold 0: 0.07235022510091464\n",
      "Validation Loss for fold 0: 0.06443811083833377\n",
      "Validation Loss for fold 0: 0.0649386780957381\n",
      "Validation Loss for fold 0: 0.06659377242128055\n",
      "Validation Loss for fold 0: 0.06531785428524017\n",
      "Validation Loss for fold 0: 0.06632802883783977\n",
      "Validation Loss for fold 0: 0.06944289679328601\n",
      "Validation Loss for fold 0: 0.06523238619168599\n",
      "Validation Loss for fold 0: 0.06567038347323735\n",
      "Validation Loss for fold 0: 0.0636221319437027\n",
      "Validation Loss for fold 0: 0.06492625176906586\n",
      "Validation Loss for fold 0: 0.05816386640071869\n",
      "Validation Loss for fold 0: 0.062152259051799774\n",
      "Validation Loss for fold 0: 0.0628986582159996\n",
      "Validation Loss for fold 0: 0.06879213203986485\n",
      "Validation Loss for fold 0: 0.06166632225116094\n",
      "Validation Loss for fold 0: 0.0644645815094312\n",
      "Validation Loss for fold 0: 0.06523435687025388\n",
      "Validation Loss for fold 0: 0.065250130991141\n",
      "Validation Loss for fold 0: 0.05889614298939705\n",
      "Validation Loss for fold 0: 0.061260721335808434\n",
      "Validation Loss for fold 0: 0.06462779144446056\n",
      "Validation Loss for fold 0: 0.06112882619102796\n",
      "Validation Loss for fold 0: 0.06439244126280148\n",
      "Validation Loss for fold 0: 0.058804755409558616\n",
      "Validation Loss for fold 0: 0.06290878603855769\n",
      "Validation Loss for fold 0: 0.0652319888273875\n",
      "Validation Loss for fold 0: 0.06427339588602383\n",
      "Validation Loss for fold 0: 0.061423116674025856\n",
      "Validation Loss for fold 0: 0.06198142717281977\n",
      "Validation Loss for fold 0: 0.06414881224433581\n",
      "Validation Loss for fold 0: 0.06063042084376017\n",
      "Validation Loss for fold 0: 0.0634867325425148\n",
      "Validation Loss for fold 0: 0.06418950607379277\n",
      "Validation Loss for fold 0: 0.06450384358565013\n",
      "Validation Loss for fold 0: 0.0584752211968104\n",
      "Validation Loss for fold 0: 0.06362760066986084\n",
      "Validation Loss for fold 0: 0.06561217457056046\n",
      "Validation Loss for fold 0: 0.06310026720166206\n",
      "Validation Loss for fold 0: 0.058211794743935265\n",
      "Validation Loss for fold 0: 0.061009335021177925\n",
      "Validation Loss for fold 0: 0.06371054550011952\n",
      "Validation Loss for fold 0: 0.059831651548544564\n",
      "Validation Loss for fold 0: 0.06186023851235708\n",
      "Validation Loss for fold 0: 0.06199407329161962\n",
      "Validation Loss for fold 0: 0.06304141630729039\n",
      "Validation Loss for fold 0: 0.06921254222591718\n",
      "Validation Loss for fold 0: 0.06940015902121861\n",
      "Validation Loss for fold 0: 0.0586970808605353\n",
      "Validation Loss for fold 0: 0.06117254992326101\n",
      "Validation Loss for fold 0: 0.06426452100276947\n",
      "Validation Loss for fold 0: 0.06893885384003322\n",
      "Validation Loss for fold 0: 0.06015752380092939\n",
      "Validation Loss for fold 0: 0.05414005493124326\n",
      "Validation Loss for fold 0: 0.062426132460435234\n",
      "Validation Loss for fold 0: 0.06613552197813988\n",
      "Validation Loss for fold 0: 0.0654284010330836\n",
      "Validation Loss for fold 0: 0.06323740134636562\n",
      "Validation Loss for fold 0: 0.07473160450657208\n",
      "Validation Loss for fold 0: 0.06557393694917361\n",
      "Validation Loss for fold 0: 0.06066174805164337\n",
      "Validation Loss for fold 0: 0.06278439983725548\n",
      "Validation Loss for fold 0: 0.06656517585118611\n",
      "Validation Loss for fold 0: 0.05942846337954203\n",
      "Validation Loss for fold 0: 0.057014886289834976\n",
      "Validation Loss for fold 0: 0.06356540073951085\n",
      "Validation Loss for fold 0: 0.057580843567848206\n",
      "Validation Loss for fold 0: 0.06788916140794754\n",
      "Validation Loss for fold 0: 0.05906233688195547\n",
      "Validation Loss for fold 0: 0.05819450691342354\n",
      "Validation Loss for fold 0: 0.061298408855994545\n",
      "Validation Loss for fold 0: 0.059942878782749176\n",
      "Validation Loss for fold 0: 0.06310196965932846\n",
      "Validation Loss for fold 0: 0.0584513396024704\n",
      "Validation Loss for fold 0: 0.06105262537797292\n",
      "Validation Loss for fold 0: 0.059101544320583344\n",
      "Validation Loss for fold 0: 0.057377426574627556\n",
      "Validation Loss for fold 0: 0.0623063954214255\n",
      "Validation Loss for fold 0: 0.06339951107899348\n",
      "Validation Loss for fold 0: 0.06006017451484998\n",
      "Validation Loss for fold 0: 0.06519166380167007\n",
      "Validation Loss for fold 0: 0.06197426716486613\n",
      "Validation Loss for fold 0: 0.06348489845792453\n",
      "Validation Loss for fold 0: 0.06716242805123329\n",
      "Validation Loss for fold 0: 0.06035268430908521\n",
      "Validation Loss for fold 0: 0.06429054960608482\n",
      "Validation Loss for fold 0: 0.05604484553138415\n",
      "Validation Loss for fold 0: 0.06573045005400975\n",
      "Validation Loss for fold 0: 0.056966666132211685\n",
      "Validation Loss for fold 0: 0.059011227140824\n",
      "Validation Loss for fold 0: 0.05884680772821108\n",
      "Validation Loss for fold 0: 0.060444133977095284\n",
      "Validation Loss for fold 0: 0.06438032413522403\n",
      "Validation Loss for fold 0: 0.06352163354555766\n",
      "Validation Loss for fold 0: 0.05560537427663803\n",
      "Validation Loss for fold 0: 0.06105089311798414\n",
      "Validation Loss for fold 0: 0.055637295047442116\n",
      "Validation Loss for fold 0: 0.061558676262696586\n",
      "Validation Loss for fold 0: 0.05773227537671725\n",
      "Validation Loss for fold 0: 0.055860704431931175\n",
      "Validation Loss for fold 0: 0.05988387018442154\n",
      "Validation Loss for fold 0: 0.05605859185258547\n",
      "Validation Loss for fold 0: 0.05905697743097941\n",
      "Validation Loss for fold 0: 0.06044640392065048\n",
      "Validation Loss for fold 0: 0.05702642103036245\n",
      "Validation Loss for fold 0: 0.0613260418176651\n",
      "Validation Loss for fold 0: 0.058256095896164574\n",
      "Validation Loss for fold 0: 0.05884751801689466\n",
      "Validation Loss for fold 0: 0.05964066212375959\n",
      "Validation Loss for fold 0: 0.05924708147843679\n",
      "Validation Loss for fold 0: 0.06053216631213824\n",
      "Validation Loss for fold 0: 0.057761043310165405\n",
      "Validation Loss for fold 0: 0.06588042775789897\n",
      "Validation Loss for fold 0: 0.06146661937236786\n",
      "Validation Loss for fold 0: 0.05443901941180229\n",
      "Validation Loss for fold 0: 0.05231943167746067\n",
      "Validation Loss for fold 0: 0.0629320852458477\n",
      "Validation Loss for fold 0: 0.060174424201250076\n",
      "Validation Loss for fold 0: 0.06380996356407802\n",
      "Validation Loss for fold 0: 0.05957690750559171\n",
      "Validation Loss for fold 0: 0.061926870296398796\n",
      "Validation Loss for fold 0: 0.05904803425073624\n",
      "Validation Loss for fold 0: 0.057326216250658035\n",
      "Validation Loss for fold 0: 0.060065608471632004\n",
      "Validation Loss for fold 0: 0.05948482205470403\n",
      "Validation Loss for fold 0: 0.05693179244796435\n",
      "Validation Loss for fold 0: 0.05332970122496287\n",
      "Validation Loss for fold 0: 0.05812715614835421\n",
      "Validation Loss for fold 0: 0.0642844686905543\n",
      "Validation Loss for fold 0: 0.05823448300361633\n",
      "Validation Loss for fold 0: 0.05970659479498863\n",
      "Validation Loss for fold 0: 0.05595062921444575\n",
      "Validation Loss for fold 0: 0.05850191662708918\n",
      "Validation Loss for fold 0: 0.05693758154908816\n",
      "Validation Loss for fold 0: 0.06673540795842807\n",
      "Validation Loss for fold 0: 0.057181073973576226\n",
      "Validation Loss for fold 0: 0.06312689930200577\n",
      "Validation Loss for fold 0: 0.05361948534846306\n",
      "Validation Loss for fold 0: 0.06260017926494281\n",
      "Validation Loss for fold 0: 0.058164642502864204\n",
      "Validation Loss for fold 0: 0.05826157579819361\n",
      "Validation Loss for fold 0: 0.05911961073676745\n",
      "Validation Loss for fold 0: 0.05662554378310839\n",
      "Validation Loss for fold 0: 0.05655565609534582\n",
      "Validation Loss for fold 0: 0.057431415965159736\n",
      "Validation Loss for fold 0: 0.057247936725616455\n",
      "Validation Loss for fold 0: 0.055861908942461014\n",
      "Validation Loss for fold 0: 0.052744595954815544\n",
      "Validation Loss for fold 0: 0.0534077857931455\n",
      "Validation Loss for fold 0: 0.06064904356996218\n",
      "Validation Loss for fold 0: 0.054521024227142334\n",
      "Validation Loss for fold 0: 0.06095733866095543\n",
      "Validation Loss for fold 0: 0.053689868499835335\n",
      "Validation Loss for fold 0: 0.052743187795082726\n",
      "Validation Loss for fold 0: 0.06068833048144976\n",
      "Validation Loss for fold 0: 0.05763748909036318\n",
      "Validation Loss for fold 0: 0.06154447793960571\n",
      "Validation Loss for fold 0: 0.06042527531584104\n",
      "Validation Loss for fold 0: 0.06082132955392202\n",
      "Validation Loss for fold 0: 0.06074073165655136\n",
      "Validation Loss for fold 0: 0.058825235813856125\n",
      "Validation Loss for fold 0: 0.0579795204102993\n",
      "Validation Loss for fold 0: 0.057878961165746055\n",
      "Validation Loss for fold 0: 0.05484843502442042\n",
      "Validation Loss for fold 0: 0.05395648628473282\n",
      "Validation Loss for fold 0: 0.05658861498037974\n",
      "Validation Loss for fold 0: 0.05754443754752477\n",
      "Validation Loss for fold 0: 0.052926273395617805\n",
      "Validation Loss for fold 0: 0.059013161808252335\n",
      "Validation Loss for fold 0: 0.056716584910949074\n",
      "Validation Loss for fold 0: 0.054751516630252205\n",
      "Validation Loss for fold 0: 0.05879338209827741\n",
      "Validation Loss for fold 0: 0.055482624719540276\n",
      "Validation Loss for fold 0: 0.053406890481710434\n",
      "Validation Loss for fold 0: 0.06233899916211764\n",
      "Validation Loss for fold 0: 0.06647229194641113\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Validation Loss for fold 1: 0.4393719434738159\n",
      "Validation Loss for fold 1: 0.3317651649316152\n",
      "Validation Loss for fold 1: 0.24622844656308493\n",
      "Validation Loss for fold 1: 0.2088161955277125\n",
      "Validation Loss for fold 1: 0.1535851334532102\n",
      "Validation Loss for fold 1: 0.16157635549704233\n",
      "Validation Loss for fold 1: 0.14682120084762573\n",
      "Validation Loss for fold 1: 0.12942271927992502\n",
      "Validation Loss for fold 1: 0.12014861156543095\n",
      "Validation Loss for fold 1: 0.11202387263377507\n",
      "Validation Loss for fold 1: 0.11014244457085927\n",
      "Validation Loss for fold 1: 0.1103837142388026\n",
      "Validation Loss for fold 1: 0.10582449038823445\n",
      "Validation Loss for fold 1: 0.1174011081457138\n",
      "Validation Loss for fold 1: 0.09162853161493938\n",
      "Validation Loss for fold 1: 0.09410033375024796\n",
      "Validation Loss for fold 1: 0.08783264706532161\n",
      "Validation Loss for fold 1: 0.08599290748437245\n",
      "Validation Loss for fold 1: 0.09425345311562221\n",
      "Validation Loss for fold 1: 0.08104747037092845\n",
      "Validation Loss for fold 1: 0.09616540869077046\n",
      "Validation Loss for fold 1: 0.08409774055083592\n",
      "Validation Loss for fold 1: 0.07550292710463206\n",
      "Validation Loss for fold 1: 0.08191683391729991\n",
      "Validation Loss for fold 1: 0.08238320300976436\n",
      "Validation Loss for fold 1: 0.08467382937669754\n",
      "Validation Loss for fold 1: 0.08270377665758133\n",
      "Validation Loss for fold 1: 0.0792061984539032\n",
      "Validation Loss for fold 1: 0.07879719386498134\n",
      "Validation Loss for fold 1: 0.07845811545848846\n",
      "Validation Loss for fold 1: 0.080423504114151\n",
      "Validation Loss for fold 1: 0.08604989697535832\n",
      "Validation Loss for fold 1: 0.07184367130200069\n",
      "Validation Loss for fold 1: 0.07358452056845029\n",
      "Validation Loss for fold 1: 0.08547213425238927\n",
      "Validation Loss for fold 1: 0.07988595465819041\n",
      "Validation Loss for fold 1: 0.07506538182497025\n",
      "Validation Loss for fold 1: 0.0804237425327301\n",
      "Validation Loss for fold 1: 0.07555937270323436\n",
      "Validation Loss for fold 1: 0.07662472873926163\n",
      "Validation Loss for fold 1: 0.0754952331384023\n",
      "Validation Loss for fold 1: 0.07729383806387584\n",
      "Validation Loss for fold 1: 0.06758166725436847\n",
      "Validation Loss for fold 1: 0.06870989377299945\n",
      "Validation Loss for fold 1: 0.06699593365192413\n",
      "Validation Loss for fold 1: 0.07198231170574824\n",
      "Validation Loss for fold 1: 0.07533284276723862\n",
      "Validation Loss for fold 1: 0.07255676885445912\n",
      "Validation Loss for fold 1: 0.07842870056629181\n",
      "Validation Loss for fold 1: 0.069749865680933\n",
      "Validation Loss for fold 1: 0.06248841558893522\n",
      "Validation Loss for fold 1: 0.06415639941891034\n",
      "Validation Loss for fold 1: 0.07426353792349498\n",
      "Validation Loss for fold 1: 0.06968579441308975\n",
      "Validation Loss for fold 1: 0.07443873832623164\n",
      "Validation Loss for fold 1: 0.07719386368989944\n",
      "Validation Loss for fold 1: 0.0659489743411541\n",
      "Validation Loss for fold 1: 0.07302975654602051\n",
      "Validation Loss for fold 1: 0.061306371043125786\n",
      "Validation Loss for fold 1: 0.06605250760912895\n",
      "Validation Loss for fold 1: 0.06677632903059323\n",
      "Validation Loss for fold 1: 0.07633338620265324\n",
      "Validation Loss for fold 1: 0.06714681039253871\n",
      "Validation Loss for fold 1: 0.06654134889443715\n",
      "Validation Loss for fold 1: 0.06794578582048416\n",
      "Validation Loss for fold 1: 0.07159098610281944\n",
      "Validation Loss for fold 1: 0.07568181057771046\n",
      "Validation Loss for fold 1: 0.07285021493832271\n",
      "Validation Loss for fold 1: 0.07321105649073918\n",
      "Validation Loss for fold 1: 0.07115025818347931\n",
      "Validation Loss for fold 1: 0.06116687754789988\n",
      "Validation Loss for fold 1: 0.06600771968563397\n",
      "Validation Loss for fold 1: 0.07174089054266612\n",
      "Validation Loss for fold 1: 0.060937367379665375\n",
      "Validation Loss for fold 1: 0.07018902773658435\n",
      "Validation Loss for fold 1: 0.07164261490106583\n",
      "Validation Loss for fold 1: 0.0622590978940328\n",
      "Validation Loss for fold 1: 0.0656078855196635\n",
      "Validation Loss for fold 1: 0.06453907738129298\n",
      "Validation Loss for fold 1: 0.074934517343839\n",
      "Validation Loss for fold 1: 0.06330526992678642\n",
      "Validation Loss for fold 1: 0.06682676697770755\n",
      "Validation Loss for fold 1: 0.06718779231111209\n",
      "Validation Loss for fold 1: 0.05893303950627645\n",
      "Validation Loss for fold 1: 0.06289372344811757\n",
      "Validation Loss for fold 1: 0.06355470915635426\n",
      "Validation Loss for fold 1: 0.06607309107979138\n",
      "Validation Loss for fold 1: 0.06096791476011276\n",
      "Validation Loss for fold 1: 0.06078155959645907\n",
      "Validation Loss for fold 1: 0.05968472237388293\n",
      "Validation Loss for fold 1: 0.058628699431816735\n",
      "Validation Loss for fold 1: 0.059842134515444435\n",
      "Validation Loss for fold 1: 0.06003765513499578\n",
      "Validation Loss for fold 1: 0.06905717651049297\n",
      "Validation Loss for fold 1: 0.06256076445182164\n",
      "Validation Loss for fold 1: 0.06048983335494995\n",
      "Validation Loss for fold 1: 0.06401857361197472\n",
      "Validation Loss for fold 1: 0.06784749403595924\n",
      "Validation Loss for fold 1: 0.05794817705949148\n",
      "Validation Loss for fold 1: 0.06973771005868912\n",
      "Validation Loss for fold 1: 0.06352308889230092\n",
      "Validation Loss for fold 1: 0.059377919882535934\n",
      "Validation Loss for fold 1: 0.057776867101589836\n",
      "Validation Loss for fold 1: 0.06297400966286659\n",
      "Validation Loss for fold 1: 0.06584275389711063\n",
      "Validation Loss for fold 1: 0.058771913250287376\n",
      "Validation Loss for fold 1: 0.06674102445443471\n",
      "Validation Loss for fold 1: 0.05908410002787908\n",
      "Validation Loss for fold 1: 0.05933478971322378\n",
      "Validation Loss for fold 1: 0.0613949882487456\n",
      "Validation Loss for fold 1: 0.06294489651918411\n",
      "Validation Loss for fold 1: 0.054484626899162926\n",
      "Validation Loss for fold 1: 0.06373102093736331\n",
      "Validation Loss for fold 1: 0.06391728172699611\n",
      "Validation Loss for fold 1: 0.0624308151503404\n",
      "Validation Loss for fold 1: 0.060752990345160164\n",
      "Validation Loss for fold 1: 0.061211166282494865\n",
      "Validation Loss for fold 1: 0.06538657595713933\n",
      "Validation Loss for fold 1: 0.0662798173725605\n",
      "Validation Loss for fold 1: 0.056966753055651985\n",
      "Validation Loss for fold 1: 0.0644085742533207\n",
      "Validation Loss for fold 1: 0.060314697523911796\n",
      "Validation Loss for fold 1: 0.055237079660097756\n",
      "Validation Loss for fold 1: 0.053706477085749306\n",
      "Validation Loss for fold 1: 0.06244136889775594\n",
      "Validation Loss for fold 1: 0.05796205004056295\n",
      "Validation Loss for fold 1: 0.06283751378456752\n",
      "Validation Loss for fold 1: 0.05787390594681104\n",
      "Validation Loss for fold 1: 0.05188907869160175\n",
      "Validation Loss for fold 1: 0.05792563036084175\n",
      "Validation Loss for fold 1: 0.05597286547223727\n",
      "Validation Loss for fold 1: 0.06535253673791885\n",
      "Validation Loss for fold 1: 0.06455865999062856\n",
      "Validation Loss for fold 1: 0.057197640339533486\n",
      "Validation Loss for fold 1: 0.05689581111073494\n",
      "Validation Loss for fold 1: 0.06066194797555605\n",
      "Validation Loss for fold 1: 0.05918781831860542\n",
      "Validation Loss for fold 1: 0.05641261115670204\n",
      "Validation Loss for fold 1: 0.055375564843416214\n",
      "Validation Loss for fold 1: 0.059353324274222054\n",
      "Validation Loss for fold 1: 0.05813112482428551\n",
      "Validation Loss for fold 1: 0.05543465415636698\n",
      "Validation Loss for fold 1: 0.06322190910577774\n",
      "Validation Loss for fold 1: 0.055288951843976974\n",
      "Validation Loss for fold 1: 0.060513333727916084\n",
      "Validation Loss for fold 1: 0.05611556271711985\n",
      "Validation Loss for fold 1: 0.05324853335817655\n",
      "Validation Loss for fold 1: 0.05433201293150584\n",
      "Validation Loss for fold 1: 0.055881292869647346\n",
      "Validation Loss for fold 1: 0.05441552152236303\n",
      "Validation Loss for fold 1: 0.0551422027250131\n",
      "Validation Loss for fold 1: 0.06212360908587774\n",
      "Validation Loss for fold 1: 0.055312883108854294\n",
      "Validation Loss for fold 1: 0.06286816174785297\n",
      "Validation Loss for fold 1: 0.057292343427737556\n",
      "Validation Loss for fold 1: 0.05616003523270289\n",
      "Validation Loss for fold 1: 0.0573755552371343\n",
      "Validation Loss for fold 1: 0.052646602193514504\n",
      "Validation Loss for fold 1: 0.05656419321894646\n",
      "Validation Loss for fold 1: 0.06721039240558942\n",
      "Validation Loss for fold 1: 0.05909861003359159\n",
      "Validation Loss for fold 1: 0.055193718522787094\n",
      "Validation Loss for fold 1: 0.05572086448470751\n",
      "Validation Loss for fold 1: 0.05644124373793602\n",
      "Validation Loss for fold 1: 0.054821088910102844\n",
      "Validation Loss for fold 1: 0.05306867013374964\n",
      "Validation Loss for fold 1: 0.058493963132301964\n",
      "Validation Loss for fold 1: 0.048983593160907425\n",
      "Validation Loss for fold 1: 0.05548206220070521\n",
      "Validation Loss for fold 1: 0.0539006715019544\n",
      "Validation Loss for fold 1: 0.06017149363954862\n",
      "Validation Loss for fold 1: 0.058980426440636315\n",
      "Validation Loss for fold 1: 0.062057687590519585\n",
      "Validation Loss for fold 1: 0.05968632052342097\n",
      "Validation Loss for fold 1: 0.05315753320852915\n",
      "Validation Loss for fold 1: 0.057251282036304474\n",
      "Validation Loss for fold 1: 0.058908965438604355\n",
      "Validation Loss for fold 1: 0.05357578520973524\n",
      "Validation Loss for fold 1: 0.05635699381430944\n",
      "Validation Loss for fold 1: 0.05352196842432022\n",
      "Validation Loss for fold 1: 0.047649018466472626\n",
      "Validation Loss for fold 1: 0.0479881918678681\n",
      "Validation Loss for fold 1: 0.05062275379896164\n",
      "Validation Loss for fold 1: 0.055500982950131096\n",
      "Validation Loss for fold 1: 0.05152243375778198\n",
      "Validation Loss for fold 1: 0.04860090526441733\n",
      "Validation Loss for fold 1: 0.05878287677963575\n",
      "Validation Loss for fold 1: 0.0588654950261116\n",
      "Validation Loss for fold 1: 0.054055627435445786\n",
      "Validation Loss for fold 1: 0.04982294142246246\n",
      "Validation Loss for fold 1: 0.048943923165400825\n",
      "Validation Loss for fold 1: 0.053641450901826225\n",
      "Validation Loss for fold 1: 0.05168458695213\n",
      "Validation Loss for fold 1: 0.052769843488931656\n",
      "Validation Loss for fold 1: 0.0599142462015152\n",
      "Validation Loss for fold 1: 0.05647978683312734\n",
      "Validation Loss for fold 1: 0.05173628404736519\n",
      "Validation Loss for fold 1: 0.050888544569412865\n",
      "Validation Loss for fold 1: 0.05389339104294777\n",
      "Validation Loss for fold 1: 0.055201402554909386\n",
      "Validation Loss for fold 1: 0.051330712934335075\n",
      "Validation Loss for fold 1: 0.05082456891735395\n",
      "Validation Loss for fold 1: 0.044789946327606835\n",
      "Validation Loss for fold 1: 0.05530040586988131\n",
      "Validation Loss for fold 1: 0.05798112725218137\n",
      "Validation Loss for fold 1: 0.05853322148323059\n",
      "Validation Loss for fold 1: 0.051678950587908425\n",
      "Validation Loss for fold 1: 0.054179114600022636\n",
      "Validation Loss for fold 1: 0.05267853910724322\n",
      "Validation Loss for fold 1: 0.04972730576992035\n",
      "Validation Loss for fold 1: 0.05338453873991966\n",
      "Validation Loss for fold 1: 0.051420219242572784\n",
      "Validation Loss for fold 1: 0.05697898815075556\n",
      "Validation Loss for fold 1: 0.054042376577854156\n",
      "Validation Loss for fold 1: 0.051897428929805756\n",
      "Validation Loss for fold 1: 0.051067868868509926\n",
      "Validation Loss for fold 1: 0.05370715136329333\n",
      "Validation Loss for fold 1: 0.054836234698692955\n",
      "Validation Loss for fold 1: 0.05465258906284968\n",
      "Validation Loss for fold 1: 0.05821657801667849\n",
      "Validation Loss for fold 1: 0.04838134969274203\n",
      "Validation Loss for fold 1: 0.05132219319542249\n",
      "Validation Loss for fold 1: 0.051678928236166634\n",
      "Validation Loss for fold 1: 0.05433603748679161\n",
      "Validation Loss for fold 1: 0.05137254297733307\n",
      "Validation Loss for fold 1: 0.05768572042385737\n",
      "Validation Loss for fold 1: 0.05367967113852501\n",
      "Validation Loss for fold 1: 0.050417639315128326\n",
      "Validation Loss for fold 1: 0.05261513342459997\n",
      "Validation Loss for fold 1: 0.04897420729200045\n",
      "Validation Loss for fold 1: 0.05169094353914261\n",
      "Validation Loss for fold 1: 0.054121557623147964\n",
      "Validation Loss for fold 1: 0.052210683623949684\n",
      "Validation Loss for fold 1: 0.05007524415850639\n",
      "Validation Loss for fold 1: 0.05371078352133433\n",
      "Validation Loss for fold 1: 0.051264444986979164\n",
      "Validation Loss for fold 1: 0.05060227091113726\n",
      "Validation Loss for fold 1: 0.050921328365802765\n",
      "Validation Loss for fold 1: 0.05398038774728775\n",
      "Validation Loss for fold 1: 0.05415082350373268\n",
      "Validation Loss for fold 1: 0.05023459096749624\n",
      "Validation Loss for fold 1: 0.05725155398249626\n",
      "Validation Loss for fold 1: 0.053364510337511696\n",
      "Validation Loss for fold 1: 0.04891320193807284\n",
      "Validation Loss for fold 1: 0.05500056718786558\n",
      "Validation Loss for fold 1: 0.048177688072125115\n",
      "Validation Loss for fold 1: 0.052055761218070984\n",
      "Validation Loss for fold 1: 0.057818313439687095\n",
      "Validation Loss for fold 1: 0.051837446788946785\n",
      "Validation Loss for fold 1: 0.0485732431213061\n",
      "Validation Loss for fold 1: 0.05015609785914421\n",
      "Validation Loss for fold 1: 0.05892904847860336\n",
      "Validation Loss for fold 1: 0.05160833025972048\n",
      "Validation Loss for fold 1: 0.054177417109409966\n",
      "Validation Loss for fold 1: 0.054561289648214974\n",
      "Validation Loss for fold 1: 0.05570122102896372\n",
      "Validation Loss for fold 1: 0.05154429997007052\n",
      "Validation Loss for fold 1: 0.051787457118431725\n",
      "Validation Loss for fold 1: 0.05729742720723152\n",
      "Validation Loss for fold 1: 0.05468633398413658\n",
      "Validation Loss for fold 1: 0.05731012672185898\n",
      "Validation Loss for fold 1: 0.055954753110806145\n",
      "Validation Loss for fold 1: 0.0482270655532678\n",
      "Validation Loss for fold 1: 0.053091827780008316\n",
      "Validation Loss for fold 1: 0.049255563567082085\n",
      "Validation Loss for fold 1: 0.054304756224155426\n",
      "Validation Loss for fold 1: 0.05194565405448278\n",
      "Validation Loss for fold 1: 0.05112031102180481\n",
      "Validation Loss for fold 1: 0.054179380337397255\n",
      "Validation Loss for fold 1: 0.05542640512188276\n",
      "Validation Loss for fold 1: 0.04868873332937559\n",
      "Validation Loss for fold 1: 0.0501897819340229\n",
      "Validation Loss for fold 1: 0.05804304281870524\n",
      "Validation Loss for fold 1: 0.05309048667550087\n",
      "Validation Loss for fold 1: 0.05110998203357061\n",
      "Validation Loss for fold 1: 0.05891552194952965\n",
      "Validation Loss for fold 1: 0.04890009885032972\n",
      "Validation Loss for fold 1: 0.05036479483048121\n",
      "Validation Loss for fold 1: 0.05159276475509008\n",
      "Validation Loss for fold 1: 0.05190717428922653\n",
      "Validation Loss for fold 1: 0.047388747334480286\n",
      "Validation Loss for fold 1: 0.052284407118956246\n",
      "Validation Loss for fold 1: 0.0497312918305397\n",
      "Validation Loss for fold 1: 0.05093952144185702\n",
      "Validation Loss for fold 1: 0.04958811650673548\n",
      "Validation Loss for fold 1: 0.05045304695765177\n",
      "Validation Loss for fold 1: 0.05452504878242811\n",
      "Validation Loss for fold 1: 0.05304581051071485\n",
      "Validation Loss for fold 1: 0.05159050847093264\n",
      "Validation Loss for fold 1: 0.048676940302054085\n",
      "Validation Loss for fold 1: 0.05013185739517212\n",
      "Validation Loss for fold 1: 0.060234141846497856\n",
      "Validation Loss for fold 1: 0.05870914955933889\n",
      "Validation Loss for fold 1: 0.049558146546284355\n",
      "Validation Loss for fold 1: 0.05371222645044327\n",
      "Validation Loss for fold 1: 0.047417858615517616\n",
      "Validation Loss for fold 1: 0.05494337653120359\n",
      "Validation Loss for fold 1: 0.051228697101275124\n",
      "Validation Loss for fold 1: 0.058637858678897224\n",
      "Validation Loss for fold 1: 0.05514624839027723\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Validation Loss for fold 2: 0.3691605528195699\n",
      "Validation Loss for fold 2: 0.2778271436691284\n",
      "Validation Loss for fold 2: 0.23164235552152\n",
      "Validation Loss for fold 2: 0.18845061461130777\n",
      "Validation Loss for fold 2: 0.1570658485094706\n",
      "Validation Loss for fold 2: 0.12841802835464478\n",
      "Validation Loss for fold 2: 0.11524895578622818\n",
      "Validation Loss for fold 2: 0.11096269885698955\n",
      "Validation Loss for fold 2: 0.10475843648115794\n",
      "Validation Loss for fold 2: 0.09495773663123448\n",
      "Validation Loss for fold 2: 0.10792309542496999\n",
      "Validation Loss for fold 2: 0.10239744186401367\n",
      "Validation Loss for fold 2: 0.09035439292589824\n",
      "Validation Loss for fold 2: 0.09305069347222646\n",
      "Validation Loss for fold 2: 0.0995560313264529\n",
      "Validation Loss for fold 2: 0.0878572811683019\n",
      "Validation Loss for fold 2: 0.09856722752253215\n",
      "Validation Loss for fold 2: 0.09083987524112065\n",
      "Validation Loss for fold 2: 0.09044614434242249\n",
      "Validation Loss for fold 2: 0.0891114870707194\n",
      "Validation Loss for fold 2: 0.08107663691043854\n",
      "Validation Loss for fold 2: 0.08117241909106572\n",
      "Validation Loss for fold 2: 0.08966607103745143\n",
      "Validation Loss for fold 2: 0.08254766712586085\n",
      "Validation Loss for fold 2: 0.08346973111232121\n",
      "Validation Loss for fold 2: 0.08408192545175552\n",
      "Validation Loss for fold 2: 0.08298680186271667\n",
      "Validation Loss for fold 2: 0.08136961857477824\n",
      "Validation Loss for fold 2: 0.08665715903043747\n",
      "Validation Loss for fold 2: 0.08604910969734192\n",
      "Validation Loss for fold 2: 0.08493364850680034\n",
      "Validation Loss for fold 2: 0.07599681615829468\n",
      "Validation Loss for fold 2: 0.07343181719382604\n",
      "Validation Loss for fold 2: 0.0771824320157369\n",
      "Validation Loss for fold 2: 0.07741797467072804\n",
      "Validation Loss for fold 2: 0.07847516735394795\n",
      "Validation Loss for fold 2: 0.07357627650101979\n",
      "Validation Loss for fold 2: 0.07317631443341573\n",
      "Validation Loss for fold 2: 0.07909028977155685\n",
      "Validation Loss for fold 2: 0.07845202585061391\n",
      "Validation Loss for fold 2: 0.07557755708694458\n",
      "Validation Loss for fold 2: 0.07258971532185872\n",
      "Validation Loss for fold 2: 0.0728554700811704\n",
      "Validation Loss for fold 2: 0.07282762229442596\n",
      "Validation Loss for fold 2: 0.07472964872916539\n",
      "Validation Loss for fold 2: 0.07400616506735484\n",
      "Validation Loss for fold 2: 0.07321977615356445\n",
      "Validation Loss for fold 2: 0.0730056216319402\n",
      "Validation Loss for fold 2: 0.06824443116784096\n",
      "Validation Loss for fold 2: 0.07793131346503894\n",
      "Validation Loss for fold 2: 0.07054563115040462\n",
      "Validation Loss for fold 2: 0.07181809097528458\n",
      "Validation Loss for fold 2: 0.06582894300421079\n",
      "Validation Loss for fold 2: 0.07894956072171529\n",
      "Validation Loss for fold 2: 0.06606833760937054\n",
      "Validation Loss for fold 2: 0.07571995382507642\n",
      "Validation Loss for fold 2: 0.06539367015163104\n",
      "Validation Loss for fold 2: 0.07102059076229732\n",
      "Validation Loss for fold 2: 0.07376483827829361\n",
      "Validation Loss for fold 2: 0.0713802898923556\n",
      "Validation Loss for fold 2: 0.06805725892384847\n",
      "Validation Loss for fold 2: 0.0790498877565066\n",
      "Validation Loss for fold 2: 0.06580102443695068\n",
      "Validation Loss for fold 2: 0.06835945571462314\n",
      "Validation Loss for fold 2: 0.07273026555776596\n",
      "Validation Loss for fold 2: 0.0754329872628053\n",
      "Validation Loss for fold 2: 0.07432040323813756\n",
      "Validation Loss for fold 2: 0.06325824931263924\n",
      "Validation Loss for fold 2: 0.06550080205003421\n",
      "Validation Loss for fold 2: 0.07275256514549255\n",
      "Validation Loss for fold 2: 0.06675868729750316\n",
      "Validation Loss for fold 2: 0.07582929233709972\n",
      "Validation Loss for fold 2: 0.06584336360295613\n",
      "Validation Loss for fold 2: 0.06538472448786099\n",
      "Validation Loss for fold 2: 0.06865221137801807\n",
      "Validation Loss for fold 2: 0.06699363887310028\n",
      "Validation Loss for fold 2: 0.06807068238655727\n",
      "Validation Loss for fold 2: 0.06411698833107948\n",
      "Validation Loss for fold 2: 0.07005663216114044\n",
      "Validation Loss for fold 2: 0.06328377376000087\n",
      "Validation Loss for fold 2: 0.07071386774381001\n",
      "Validation Loss for fold 2: 0.07156310478846233\n",
      "Validation Loss for fold 2: 0.06850106020768483\n",
      "Validation Loss for fold 2: 0.06359791134794553\n",
      "Validation Loss for fold 2: 0.062119352320830025\n",
      "Validation Loss for fold 2: 0.06560734907786052\n",
      "Validation Loss for fold 2: 0.060194273789723717\n",
      "Validation Loss for fold 2: 0.06570730110009511\n",
      "Validation Loss for fold 2: 0.06147721285621325\n",
      "Validation Loss for fold 2: 0.06267363453904788\n",
      "Validation Loss for fold 2: 0.06026211753487587\n",
      "Validation Loss for fold 2: 0.06306351721286774\n",
      "Validation Loss for fold 2: 0.0640668086707592\n",
      "Validation Loss for fold 2: 0.06443562358617783\n",
      "Validation Loss for fold 2: 0.06629211207230885\n",
      "Validation Loss for fold 2: 0.06130116184552511\n",
      "Validation Loss for fold 2: 0.0684828410545985\n",
      "Validation Loss for fold 2: 0.06722367430726688\n",
      "Validation Loss for fold 2: 0.06258548175295194\n",
      "Validation Loss for fold 2: 0.06622126325964928\n",
      "Validation Loss for fold 2: 0.058688888947168984\n",
      "Validation Loss for fold 2: 0.06254476308822632\n",
      "Validation Loss for fold 2: 0.06548840800921123\n",
      "Validation Loss for fold 2: 0.056169417997201286\n",
      "Validation Loss for fold 2: 0.06124922508994738\n",
      "Validation Loss for fold 2: 0.064906341334184\n",
      "Validation Loss for fold 2: 0.06282914057374\n",
      "Validation Loss for fold 2: 0.0560511431346337\n",
      "Validation Loss for fold 2: 0.06663420423865318\n",
      "Validation Loss for fold 2: 0.059798850367466606\n",
      "Validation Loss for fold 2: 0.06656317288676898\n",
      "Validation Loss for fold 2: 0.06315287450949351\n",
      "Validation Loss for fold 2: 0.06392064566413562\n",
      "Validation Loss for fold 2: 0.06708408643802007\n",
      "Validation Loss for fold 2: 0.061799295246601105\n",
      "Validation Loss for fold 2: 0.05798595150311788\n",
      "Validation Loss for fold 2: 0.0643256555000941\n",
      "Validation Loss for fold 2: 0.06704015036424\n",
      "Validation Loss for fold 2: 0.06166477625568708\n",
      "Validation Loss for fold 2: 0.0623935138185819\n",
      "Validation Loss for fold 2: 0.06343563149372737\n",
      "Validation Loss for fold 2: 0.0633893037835757\n",
      "Validation Loss for fold 2: 0.06090398753682772\n",
      "Validation Loss for fold 2: 0.06421614189942677\n",
      "Validation Loss for fold 2: 0.05825915808478991\n",
      "Validation Loss for fold 2: 0.06241542845964432\n",
      "Validation Loss for fold 2: 0.06477743263045947\n",
      "Validation Loss for fold 2: 0.06505172327160835\n",
      "Validation Loss for fold 2: 0.06675905485947926\n",
      "Validation Loss for fold 2: 0.06178495908776919\n",
      "Validation Loss for fold 2: 0.06414575750629108\n",
      "Validation Loss for fold 2: 0.06234360237916311\n",
      "Validation Loss for fold 2: 0.06138998021682104\n",
      "Validation Loss for fold 2: 0.05974102268616358\n",
      "Validation Loss for fold 2: 0.0653823787967364\n",
      "Validation Loss for fold 2: 0.0628128523627917\n",
      "Validation Loss for fold 2: 0.0651798148949941\n",
      "Validation Loss for fold 2: 0.0628486971060435\n",
      "Validation Loss for fold 2: 0.05702117457985878\n",
      "Validation Loss for fold 2: 0.05796623354156812\n",
      "Validation Loss for fold 2: 0.0549286442498366\n",
      "Validation Loss for fold 2: 0.06807907422383626\n",
      "Validation Loss for fold 2: 0.05852783843874931\n",
      "Validation Loss for fold 2: 0.06146850188573202\n",
      "Validation Loss for fold 2: 0.06314271315932274\n",
      "Validation Loss for fold 2: 0.06119424228866895\n",
      "Validation Loss for fold 2: 0.05529392883181572\n",
      "Validation Loss for fold 2: 0.05933352808157603\n",
      "Validation Loss for fold 2: 0.0617662121852239\n",
      "Validation Loss for fold 2: 0.06245144456624985\n",
      "Validation Loss for fold 2: 0.05898489182194074\n",
      "Validation Loss for fold 2: 0.057773299515247345\n",
      "Validation Loss for fold 2: 0.06320051848888397\n",
      "Validation Loss for fold 2: 0.0549837201833725\n",
      "Validation Loss for fold 2: 0.05707244947552681\n",
      "Validation Loss for fold 2: 0.060718088100353874\n",
      "Validation Loss for fold 2: 0.06157641361157099\n",
      "Validation Loss for fold 2: 0.05915601799885432\n",
      "Validation Loss for fold 2: 0.06256611893574397\n",
      "Validation Loss for fold 2: 0.05921653161446253\n",
      "Validation Loss for fold 2: 0.06689090405901273\n",
      "Validation Loss for fold 2: 0.0560309998691082\n",
      "Validation Loss for fold 2: 0.05793198570609093\n",
      "Validation Loss for fold 2: 0.05690564587712288\n",
      "Validation Loss for fold 2: 0.056963843603928886\n",
      "Validation Loss for fold 2: 0.068437064687411\n",
      "Validation Loss for fold 2: 0.057706197102864586\n",
      "Validation Loss for fold 2: 0.05950851862629255\n",
      "Validation Loss for fold 2: 0.06419292837381363\n",
      "Validation Loss for fold 2: 0.05508580803871155\n",
      "Validation Loss for fold 2: 0.060955531895160675\n",
      "Validation Loss for fold 2: 0.060446725537379585\n",
      "Validation Loss for fold 2: 0.05860114097595215\n",
      "Validation Loss for fold 2: 0.06106474374731382\n",
      "Validation Loss for fold 2: 0.059833316753307976\n",
      "Validation Loss for fold 2: 0.06640637169281642\n",
      "Validation Loss for fold 2: 0.062137054900328316\n",
      "Validation Loss for fold 2: 0.05591480806469917\n",
      "Validation Loss for fold 2: 0.05348584552605947\n",
      "Validation Loss for fold 2: 0.05979809785882632\n",
      "Validation Loss for fold 2: 0.06299277519186337\n",
      "Validation Loss for fold 2: 0.057828341921170555\n",
      "Validation Loss for fold 2: 0.05352072914441427\n",
      "Validation Loss for fold 2: 0.061624810099601746\n",
      "Validation Loss for fold 2: 0.05214952056606611\n",
      "Validation Loss for fold 2: 0.061117518693208694\n",
      "Validation Loss for fold 2: 0.0567621166507403\n",
      "Validation Loss for fold 2: 0.057027194648981094\n",
      "Validation Loss for fold 2: 0.055768306056658425\n",
      "Validation Loss for fold 2: 0.061947815120220184\n",
      "Validation Loss for fold 2: 0.0538240410387516\n",
      "Validation Loss for fold 2: 0.06035728504260381\n",
      "Validation Loss for fold 2: 0.06050409376621246\n",
      "Validation Loss for fold 2: 0.059525432686010994\n",
      "Validation Loss for fold 2: 0.054919095089038215\n",
      "Validation Loss for fold 2: 0.0566526527206103\n",
      "Validation Loss for fold 2: 0.06648418928186099\n",
      "Validation Loss for fold 2: 0.061504977444807686\n",
      "Validation Loss for fold 2: 0.06243694697817167\n",
      "Validation Loss for fold 2: 0.06101146092017492\n",
      "Validation Loss for fold 2: 0.06185979520281156\n",
      "Validation Loss for fold 2: 0.05774648611744245\n",
      "Validation Loss for fold 2: 0.05663247654835383\n",
      "Validation Loss for fold 2: 0.05477974688013395\n",
      "Validation Loss for fold 2: 0.06488659357031186\n",
      "Validation Loss for fold 2: 0.05551536629597346\n",
      "Validation Loss for fold 2: 0.05584744984904925\n",
      "Validation Loss for fold 2: 0.05994472528497378\n",
      "Validation Loss for fold 2: 0.058156080543994904\n",
      "Validation Loss for fold 2: 0.04989960603415966\n",
      "Validation Loss for fold 2: 0.057034542163213096\n",
      "Validation Loss for fold 2: 0.05902897318204244\n",
      "Validation Loss for fold 2: 0.05731832360227903\n",
      "Validation Loss for fold 2: 0.05604358514149984\n",
      "Validation Loss for fold 2: 0.055700318266948066\n",
      "Validation Loss for fold 2: 0.0619457612435023\n",
      "Validation Loss for fold 2: 0.056211166083812714\n",
      "Validation Loss for fold 2: 0.05575664589802424\n",
      "Validation Loss for fold 2: 0.054096671442190804\n",
      "Validation Loss for fold 2: 0.059129188458124794\n",
      "Validation Loss for fold 2: 0.052053229262431465\n",
      "Validation Loss for fold 2: 0.05599012474219004\n",
      "Validation Loss for fold 2: 0.060935440162817635\n",
      "Validation Loss for fold 2: 0.05516455446680387\n",
      "Validation Loss for fold 2: 0.05919677640000979\n",
      "Validation Loss for fold 2: 0.05572317292292913\n",
      "Validation Loss for fold 2: 0.05297417317827543\n",
      "Validation Loss for fold 2: 0.05613833665847778\n",
      "Validation Loss for fold 2: 0.054019976407289505\n",
      "Validation Loss for fold 2: 0.06020480394363403\n",
      "Validation Loss for fold 2: 0.061105385422706604\n",
      "Validation Loss for fold 2: 0.06302616496880849\n",
      "Validation Loss for fold 2: 0.05677669122815132\n",
      "Validation Loss for fold 2: 0.060253133376439415\n",
      "Validation Loss for fold 2: 0.05808200687170029\n",
      "Validation Loss for fold 2: 0.05956030637025833\n",
      "Validation Loss for fold 2: 0.057680377115805946\n",
      "Validation Loss for fold 2: 0.060765008131663\n",
      "Validation Loss for fold 2: 0.05779934177796046\n",
      "Validation Loss for fold 2: 0.059304567674795784\n",
      "Validation Loss for fold 2: 0.05874990920225779\n",
      "Validation Loss for fold 2: 0.053952199717362724\n",
      "Validation Loss for fold 2: 0.05636756122112274\n",
      "Validation Loss for fold 2: 0.059949501107136406\n",
      "Validation Loss for fold 2: 0.05706558749079704\n",
      "Validation Loss for fold 2: 0.061505054434140526\n",
      "Validation Loss for fold 2: 0.055149717877308525\n",
      "Validation Loss for fold 2: 0.0548104519645373\n",
      "Validation Loss for fold 2: 0.056976248820622764\n",
      "Validation Loss for fold 2: 0.052755482494831085\n",
      "Validation Loss for fold 2: 0.05464082087079684\n",
      "Validation Loss for fold 2: 0.05120531593759855\n",
      "Validation Loss for fold 2: 0.056702401489019394\n",
      "Validation Loss for fold 2: 0.054328771928946175\n",
      "Validation Loss for fold 2: 0.055707044899463654\n",
      "Validation Loss for fold 2: 0.058753655602534614\n",
      "Validation Loss for fold 2: 0.052744846791028976\n",
      "Validation Loss for fold 2: 0.05873946100473404\n",
      "Validation Loss for fold 2: 0.05830401057998339\n",
      "Validation Loss for fold 2: 0.054322717090447746\n",
      "Validation Loss for fold 2: 0.06033621604243914\n",
      "Validation Loss for fold 2: 0.058857894192139305\n",
      "Validation Loss for fold 2: 0.055441200733184814\n",
      "Validation Loss for fold 2: 0.0589815154671669\n",
      "Validation Loss for fold 2: 0.05220965047677358\n",
      "Validation Loss for fold 2: 0.05339383706450462\n",
      "Validation Loss for fold 2: 0.05504548425475756\n",
      "Validation Loss for fold 2: 0.05378371228774389\n",
      "Validation Loss for fold 2: 0.06013673171401024\n",
      "Validation Loss for fold 2: 0.06476449221372604\n",
      "Validation Loss for fold 2: 0.05686985328793526\n",
      "Validation Loss for fold 2: 0.06360241522391637\n",
      "Validation Loss for fold 2: 0.05822126567363739\n",
      "Validation Loss for fold 2: 0.05490214253465334\n",
      "Validation Loss for fold 2: 0.059822590400775276\n",
      "Validation Loss for fold 2: 0.05763967831929525\n",
      "Validation Loss for fold 2: 0.0570428433517615\n",
      "Validation Loss for fold 2: 0.05878236144781113\n",
      "Validation Loss for fold 2: 0.05460350960493088\n",
      "Validation Loss for fold 2: 0.054103780537843704\n",
      "Validation Loss for fold 2: 0.05556478351354599\n",
      "Validation Loss for fold 2: 0.05806596949696541\n",
      "Validation Loss for fold 2: 0.06226768468817075\n",
      "Validation Loss for fold 2: 0.05672855551044146\n",
      "Validation Loss for fold 2: 0.05886498341957728\n",
      "Validation Loss for fold 2: 0.0592814435561498\n",
      "Validation Loss for fold 2: 0.05340086047848066\n",
      "Validation Loss for fold 2: 0.05938546732068062\n",
      "Validation Loss for fold 2: 0.0488133200754722\n",
      "Validation Loss for fold 2: 0.05794281388322512\n",
      "Validation Loss for fold 2: 0.05203048512339592\n",
      "Validation Loss for fold 2: 0.05498569582899412\n",
      "Validation Loss for fold 2: 0.05732447778185209\n",
      "Validation Loss for fold 2: 0.05698709314068159\n",
      "Validation Loss for fold 2: 0.05751714358727137\n",
      "Validation Loss for fold 2: 0.05536531160275141\n",
      "Validation Loss for fold 2: 0.05664104719956716\n",
      "Validation Loss for fold 2: 0.0642746314406395\n",
      "Validation Loss for fold 2: 0.054410647600889206\n",
      "Validation Loss for fold 2: 0.054973081996043525\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "Validation Loss for fold 3: 0.6409778197606405\n",
      "Validation Loss for fold 3: 0.4940628111362457\n",
      "Validation Loss for fold 3: 0.39763233065605164\n",
      "Validation Loss for fold 3: 0.33119556307792664\n",
      "Validation Loss for fold 3: 0.27354322373867035\n",
      "Validation Loss for fold 3: 0.24473931888739267\n",
      "Validation Loss for fold 3: 0.23583610852559408\n",
      "Validation Loss for fold 3: 0.2102864682674408\n",
      "Validation Loss for fold 3: 0.20569292704264322\n",
      "Validation Loss for fold 3: 0.18255397180716196\n",
      "Validation Loss for fold 3: 0.18092206120491028\n",
      "Validation Loss for fold 3: 0.16317729651927948\n",
      "Validation Loss for fold 3: 0.16308500369389853\n",
      "Validation Loss for fold 3: 0.15772431592146555\n",
      "Validation Loss for fold 3: 0.15006917218367258\n",
      "Validation Loss for fold 3: 0.15175667901833853\n",
      "Validation Loss for fold 3: 0.15281103054682413\n",
      "Validation Loss for fold 3: 0.15469145278135935\n",
      "Validation Loss for fold 3: 0.143490731716156\n",
      "Validation Loss for fold 3: 0.12802170465389887\n",
      "Validation Loss for fold 3: 0.12686252097288767\n",
      "Validation Loss for fold 3: 0.1386591891447703\n",
      "Validation Loss for fold 3: 0.13299198945363364\n",
      "Validation Loss for fold 3: 0.14200473576784134\n",
      "Validation Loss for fold 3: 0.1167329450448354\n",
      "Validation Loss for fold 3: 0.12666853020588556\n",
      "Validation Loss for fold 3: 0.12653498351573944\n",
      "Validation Loss for fold 3: 0.11874934782584508\n",
      "Validation Loss for fold 3: 0.11786569406588872\n",
      "Validation Loss for fold 3: 0.11238689968983333\n",
      "Validation Loss for fold 3: 0.11628885567188263\n",
      "Validation Loss for fold 3: 0.11492060621579488\n",
      "Validation Loss for fold 3: 0.1139472524325053\n",
      "Validation Loss for fold 3: 0.11053431530793507\n",
      "Validation Loss for fold 3: 0.12006390591462453\n",
      "Validation Loss for fold 3: 0.1122844045360883\n",
      "Validation Loss for fold 3: 0.12443498025337855\n",
      "Validation Loss for fold 3: 0.11644489566485088\n",
      "Validation Loss for fold 3: 0.10436441500981648\n",
      "Validation Loss for fold 3: 0.1066316490372022\n",
      "Validation Loss for fold 3: 0.10761409252882004\n",
      "Validation Loss for fold 3: 0.10307367146015167\n",
      "Validation Loss for fold 3: 0.11685811976591746\n",
      "Validation Loss for fold 3: 0.10168194770812988\n",
      "Validation Loss for fold 3: 0.11031715075174968\n",
      "Validation Loss for fold 3: 0.10901409139235814\n",
      "Validation Loss for fold 3: 0.10308229178190231\n",
      "Validation Loss for fold 3: 0.09112439552942912\n",
      "Validation Loss for fold 3: 0.11135639746983846\n",
      "Validation Loss for fold 3: 0.09973922123511632\n",
      "Validation Loss for fold 3: 0.09068478395541509\n",
      "Validation Loss for fold 3: 0.09849575410286586\n",
      "Validation Loss for fold 3: 0.09718579302231471\n",
      "Validation Loss for fold 3: 0.09578634798526764\n",
      "Validation Loss for fold 3: 0.1020821084578832\n",
      "Validation Loss for fold 3: 0.09477006892363231\n",
      "Validation Loss for fold 3: 0.09512940794229507\n",
      "Validation Loss for fold 3: 0.0895464022954305\n",
      "Validation Loss for fold 3: 0.09660367916027705\n",
      "Validation Loss for fold 3: 0.09033096084992091\n",
      "Validation Loss for fold 3: 0.09016600499550502\n",
      "Validation Loss for fold 3: 0.09752849241097768\n",
      "Validation Loss for fold 3: 0.0991878757874171\n",
      "Validation Loss for fold 3: 0.09101881335179011\n",
      "Validation Loss for fold 3: 0.08726976315180461\n",
      "Validation Loss for fold 3: 0.09414245933294296\n",
      "Validation Loss for fold 3: 0.08581873526175816\n",
      "Validation Loss for fold 3: 0.08922246843576431\n",
      "Validation Loss for fold 3: 0.09221263974905014\n",
      "Validation Loss for fold 3: 0.0871509313583374\n",
      "Validation Loss for fold 3: 0.08596617231766383\n",
      "Validation Loss for fold 3: 0.09080585837364197\n",
      "Validation Loss for fold 3: 0.08784614006678264\n",
      "Validation Loss for fold 3: 0.08943170060714085\n",
      "Validation Loss for fold 3: 0.08450122674306233\n",
      "Validation Loss for fold 3: 0.08232438067595164\n",
      "Validation Loss for fold 3: 0.08331375072399776\n",
      "Validation Loss for fold 3: 0.07451700170834859\n",
      "Validation Loss for fold 3: 0.08420245597759883\n",
      "Validation Loss for fold 3: 0.08141625672578812\n",
      "Validation Loss for fold 3: 0.08614685634771983\n",
      "Validation Loss for fold 3: 0.07946265488862991\n",
      "Validation Loss for fold 3: 0.08858685940504074\n",
      "Validation Loss for fold 3: 0.08228776107231776\n",
      "Validation Loss for fold 3: 0.08287030706803004\n",
      "Validation Loss for fold 3: 0.08421847720940907\n",
      "Validation Loss for fold 3: 0.07641856869061787\n",
      "Validation Loss for fold 3: 0.08235002060731252\n",
      "Validation Loss for fold 3: 0.08226311951875687\n",
      "Validation Loss for fold 3: 0.08025168627500534\n",
      "Validation Loss for fold 3: 0.0764964371919632\n",
      "Validation Loss for fold 3: 0.07548293968041737\n",
      "Validation Loss for fold 3: 0.08214331169923146\n",
      "Validation Loss for fold 3: 0.07887433717648189\n",
      "Validation Loss for fold 3: 0.08085966855287552\n",
      "Validation Loss for fold 3: 0.08331088970104854\n",
      "Validation Loss for fold 3: 0.0785214826464653\n",
      "Validation Loss for fold 3: 0.08423570295174916\n",
      "Validation Loss for fold 3: 0.07913857201735179\n",
      "Validation Loss for fold 3: 0.08099272350470225\n",
      "Validation Loss for fold 3: 0.07663428783416748\n",
      "Validation Loss for fold 3: 0.0790935680270195\n",
      "Validation Loss for fold 3: 0.07558068633079529\n",
      "Validation Loss for fold 3: 0.08360267182191213\n",
      "Validation Loss for fold 3: 0.07515677561362584\n",
      "Validation Loss for fold 3: 0.07777050882577896\n",
      "Validation Loss for fold 3: 0.07656964908043544\n",
      "Validation Loss for fold 3: 0.07606605192025502\n",
      "Validation Loss for fold 3: 0.07506369302670161\n",
      "Validation Loss for fold 3: 0.07554402947425842\n",
      "Validation Loss for fold 3: 0.07022399827837944\n",
      "Validation Loss for fold 3: 0.06796597689390182\n",
      "Validation Loss for fold 3: 0.06635738785068195\n",
      "Validation Loss for fold 3: 0.070601520438989\n",
      "Validation Loss for fold 3: 0.07759282737970352\n",
      "Validation Loss for fold 3: 0.07224784791469574\n",
      "Validation Loss for fold 3: 0.07255460818608601\n",
      "Validation Loss for fold 3: 0.06775588542222977\n",
      "Validation Loss for fold 3: 0.06986497342586517\n",
      "Validation Loss for fold 3: 0.07179272174835205\n",
      "Validation Loss for fold 3: 0.07287781188885371\n",
      "Validation Loss for fold 3: 0.07311353584130605\n",
      "Validation Loss for fold 3: 0.0775253822406133\n",
      "Validation Loss for fold 3: 0.0694703534245491\n",
      "Validation Loss for fold 3: 0.07312359288334846\n",
      "Validation Loss for fold 3: 0.07075503220160802\n",
      "Validation Loss for fold 3: 0.07276477664709091\n",
      "Validation Loss for fold 3: 0.0658622682094574\n",
      "Validation Loss for fold 3: 0.06665331497788429\n",
      "Validation Loss for fold 3: 0.0780881866812706\n",
      "Validation Loss for fold 3: 0.07221181939045589\n",
      "Validation Loss for fold 3: 0.06865157683690389\n",
      "Validation Loss for fold 3: 0.06437526022394498\n",
      "Validation Loss for fold 3: 0.06525972733894984\n",
      "Validation Loss for fold 3: 0.07244483133157094\n",
      "Validation Loss for fold 3: 0.07599292695522308\n",
      "Validation Loss for fold 3: 0.06807175278663635\n",
      "Validation Loss for fold 3: 0.06543686613440514\n",
      "Validation Loss for fold 3: 0.07187594473361969\n",
      "Validation Loss for fold 3: 0.06588639815648396\n",
      "Validation Loss for fold 3: 0.06490729500850041\n",
      "Validation Loss for fold 3: 0.06877898424863815\n",
      "Validation Loss for fold 3: 0.06139060979088148\n",
      "Validation Loss for fold 3: 0.06657043347756068\n",
      "Validation Loss for fold 3: 0.06860097249348958\n",
      "Validation Loss for fold 3: 0.06382691239317258\n",
      "Validation Loss for fold 3: 0.06120384857058525\n",
      "Validation Loss for fold 3: 0.06738295157750447\n",
      "Validation Loss for fold 3: 0.06948090096314748\n",
      "Validation Loss for fold 3: 0.0677942434946696\n",
      "Validation Loss for fold 3: 0.06284044807155927\n",
      "Validation Loss for fold 3: 0.07043254872163136\n",
      "Validation Loss for fold 3: 0.06663769607742627\n",
      "Validation Loss for fold 3: 0.0719061903655529\n",
      "Validation Loss for fold 3: 0.06367372224728267\n",
      "Validation Loss for fold 3: 0.06532266984383266\n",
      "Validation Loss for fold 3: 0.06451690817872684\n",
      "Validation Loss for fold 3: 0.06844779228170712\n",
      "Validation Loss for fold 3: 0.05931959797938665\n",
      "Validation Loss for fold 3: 0.06869952504833539\n",
      "Validation Loss for fold 3: 0.061570290476083755\n",
      "Validation Loss for fold 3: 0.06218380729357401\n",
      "Validation Loss for fold 3: 0.062468682726224266\n",
      "Validation Loss for fold 3: 0.06398479143778484\n",
      "Validation Loss for fold 3: 0.06125824525952339\n",
      "Validation Loss for fold 3: 0.06590540707111359\n",
      "Validation Loss for fold 3: 0.06834806750218074\n",
      "Validation Loss for fold 3: 0.06424724186460178\n",
      "Validation Loss for fold 3: 0.060226257890462875\n",
      "Validation Loss for fold 3: 0.06542334333062172\n",
      "Validation Loss for fold 3: 0.058408992985884346\n",
      "Validation Loss for fold 3: 0.06525575121243794\n",
      "Validation Loss for fold 3: 0.06988940263787906\n",
      "Validation Loss for fold 3: 0.06136326119303703\n",
      "Validation Loss for fold 3: 0.062056779861450195\n",
      "Validation Loss for fold 3: 0.0667029619216919\n",
      "Validation Loss for fold 3: 0.05739500870307287\n",
      "Validation Loss for fold 3: 0.057995653400818505\n",
      "Validation Loss for fold 3: 0.059024748702843986\n",
      "Validation Loss for fold 3: 0.05755224327246348\n",
      "Validation Loss for fold 3: 0.06281483297546704\n",
      "Validation Loss for fold 3: 0.061961724112431206\n",
      "Validation Loss for fold 3: 0.06270663067698479\n",
      "Validation Loss for fold 3: 0.05881313607096672\n",
      "Validation Loss for fold 3: 0.05624667058388392\n",
      "Validation Loss for fold 3: 0.0625381867090861\n",
      "Validation Loss for fold 3: 0.06225050364931425\n",
      "Validation Loss for fold 3: 0.06419708703955014\n",
      "Validation Loss for fold 3: 0.06448936959107716\n",
      "Validation Loss for fold 3: 0.06538127238551776\n",
      "Validation Loss for fold 3: 0.057451989501714706\n",
      "Validation Loss for fold 3: 0.06273330996433894\n",
      "Validation Loss for fold 3: 0.06384733070929845\n",
      "Validation Loss for fold 3: 0.05855875462293625\n",
      "Validation Loss for fold 3: 0.05939655750989914\n",
      "Validation Loss for fold 3: 0.06357001637419064\n",
      "Validation Loss for fold 3: 0.05754454433917999\n",
      "Validation Loss for fold 3: 0.06318016350269318\n",
      "Validation Loss for fold 3: 0.0580696277320385\n",
      "Validation Loss for fold 3: 0.05763704453905424\n",
      "Validation Loss for fold 3: 0.06355159357190132\n",
      "Validation Loss for fold 3: 0.06569984927773476\n",
      "Validation Loss for fold 3: 0.05698637788494428\n",
      "Validation Loss for fold 3: 0.0641995370388031\n",
      "Validation Loss for fold 3: 0.06469761704405148\n",
      "Validation Loss for fold 3: 0.05803165336449941\n",
      "Validation Loss for fold 3: 0.06183842942118645\n",
      "Validation Loss for fold 3: 0.06238392988840739\n",
      "Validation Loss for fold 3: 0.06309761727849643\n",
      "Validation Loss for fold 3: 0.06074622025092443\n",
      "Validation Loss for fold 3: 0.05606830989321073\n",
      "Validation Loss for fold 3: 0.05702690904339155\n",
      "Validation Loss for fold 3: 0.06091256067156792\n",
      "Validation Loss for fold 3: 0.05995833997925123\n",
      "Validation Loss for fold 3: 0.06114094083507856\n",
      "Validation Loss for fold 3: 0.06901572768886884\n",
      "Validation Loss for fold 3: 0.061894221852223076\n",
      "Validation Loss for fold 3: 0.06343095004558563\n",
      "Validation Loss for fold 3: 0.06238963082432747\n",
      "Validation Loss for fold 3: 0.05453443403045336\n",
      "Validation Loss for fold 3: 0.0633753277361393\n",
      "Validation Loss for fold 3: 0.05673165867726008\n",
      "Validation Loss for fold 3: 0.05731398363908132\n",
      "Validation Loss for fold 3: 0.06124874825278918\n",
      "Validation Loss for fold 3: 0.058669387052456536\n",
      "Validation Loss for fold 3: 0.062129261593023934\n",
      "Validation Loss for fold 3: 0.06432945529619853\n",
      "Validation Loss for fold 3: 0.059453852474689484\n",
      "Validation Loss for fold 3: 0.06027407323320707\n",
      "Validation Loss for fold 3: 0.05445998286207517\n",
      "Validation Loss for fold 3: 0.05767425522208214\n",
      "Validation Loss for fold 3: 0.06264783317844073\n",
      "Validation Loss for fold 3: 0.06051555275917053\n",
      "Validation Loss for fold 3: 0.06418661524852116\n",
      "Validation Loss for fold 3: 0.0641302118698756\n",
      "Validation Loss for fold 3: 0.05777309959133466\n",
      "Validation Loss for fold 3: 0.06187432135144869\n",
      "Validation Loss for fold 3: 0.055403572817643486\n",
      "Validation Loss for fold 3: 0.05853058894475301\n",
      "Validation Loss for fold 3: 0.05887941891948382\n",
      "Validation Loss for fold 3: 0.05722894395391146\n",
      "Validation Loss for fold 3: 0.05994995062549909\n",
      "Validation Loss for fold 3: 0.057108563681443535\n",
      "Validation Loss for fold 3: 0.060245382289091744\n",
      "Validation Loss for fold 3: 0.05579902604222298\n",
      "Validation Loss for fold 3: 0.05814653386672338\n",
      "Validation Loss for fold 3: 0.05520055194695791\n",
      "Validation Loss for fold 3: 0.05799040695031484\n",
      "Validation Loss for fold 3: 0.0638945351044337\n",
      "Validation Loss for fold 3: 0.06283656756083171\n",
      "Validation Loss for fold 3: 0.05221195394794146\n",
      "Validation Loss for fold 3: 0.05425768345594406\n",
      "Validation Loss for fold 3: 0.061169158667325974\n",
      "Validation Loss for fold 3: 0.057018473744392395\n",
      "Validation Loss for fold 3: 0.05794666334986687\n",
      "Validation Loss for fold 3: 0.05726524690786997\n",
      "Validation Loss for fold 3: 0.056497191389401756\n",
      "Validation Loss for fold 3: 0.05515768130620321\n",
      "Validation Loss for fold 3: 0.05299286295970281\n",
      "Validation Loss for fold 3: 0.05595423405369123\n",
      "Validation Loss for fold 3: 0.0585885134836038\n",
      "Validation Loss for fold 3: 0.06038592755794525\n",
      "Validation Loss for fold 3: 0.05759884292880694\n",
      "Validation Loss for fold 3: 0.05953647072116534\n",
      "Validation Loss for fold 3: 0.055815729002157845\n",
      "Validation Loss for fold 3: 0.05738268792629242\n",
      "Validation Loss for fold 3: 0.05590177451570829\n",
      "Validation Loss for fold 3: 0.054533977061510086\n",
      "Validation Loss for fold 3: 0.05879770591855049\n",
      "Validation Loss for fold 3: 0.06329450632135074\n",
      "Validation Loss for fold 3: 0.06560254593690236\n",
      "Validation Loss for fold 3: 0.054454303036133446\n",
      "Validation Loss for fold 3: 0.05725544939438502\n",
      "Validation Loss for fold 3: 0.058841673036416374\n",
      "Validation Loss for fold 3: 0.055735127379496895\n",
      "Validation Loss for fold 3: 0.054435412089029946\n",
      "Validation Loss for fold 3: 0.05200878530740738\n",
      "Validation Loss for fold 3: 0.05887988209724426\n",
      "Validation Loss for fold 3: 0.05847255885601044\n",
      "Validation Loss for fold 3: 0.05428232252597809\n",
      "Validation Loss for fold 3: 0.055396334578593574\n",
      "Validation Loss for fold 3: 0.058522820472717285\n",
      "Validation Loss for fold 3: 0.05751737828056017\n",
      "Validation Loss for fold 3: 0.05334404607613882\n",
      "Validation Loss for fold 3: 0.05880198751886686\n",
      "Validation Loss for fold 3: 0.05606381098429362\n",
      "Validation Loss for fold 3: 0.05550427238146464\n",
      "Validation Loss for fold 3: 0.056833562751611076\n",
      "Validation Loss for fold 3: 0.055670263866583504\n",
      "Validation Loss for fold 3: 0.05590511982639631\n",
      "Validation Loss for fold 3: 0.05225853373607\n",
      "Validation Loss for fold 3: 0.0595230907201767\n",
      "Validation Loss for fold 3: 0.05411549657583237\n",
      "Validation Loss for fold 3: 0.05357691024740537\n",
      "Validation Loss for fold 3: 0.05812008182207743\n",
      "Validation Loss for fold 3: 0.0571909062564373\n",
      "Validation Loss for fold 3: 0.0635679302116235\n",
      "Validation Loss for fold 3: 0.053350030134121575\n",
      "Validation Loss for fold 3: 0.05453215166926384\n",
      "Validation Loss for fold 3: 0.05792438114682833\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "Validation Loss for fold 4: 0.25040560960769653\n",
      "Validation Loss for fold 4: 0.18831472098827362\n",
      "Validation Loss for fold 4: 0.1466394563515981\n",
      "Validation Loss for fold 4: 0.13871658345063528\n",
      "Validation Loss for fold 4: 0.11653964221477509\n",
      "Validation Loss for fold 4: 0.1110022912422816\n",
      "Validation Loss for fold 4: 0.0987430065870285\n",
      "Validation Loss for fold 4: 0.10048421223958333\n",
      "Validation Loss for fold 4: 0.08885893474022548\n",
      "Validation Loss for fold 4: 0.09603552520275116\n",
      "Validation Loss for fold 4: 0.08476047962903976\n",
      "Validation Loss for fold 4: 0.08637483417987823\n",
      "Validation Loss for fold 4: 0.08602094650268555\n",
      "Validation Loss for fold 4: 0.08622761319080989\n",
      "Validation Loss for fold 4: 0.07927507658799489\n",
      "Validation Loss for fold 4: 0.08433376252651215\n",
      "Validation Loss for fold 4: 0.08000416060288747\n",
      "Validation Loss for fold 4: 0.09084952871004741\n",
      "Validation Loss for fold 4: 0.08572477598985036\n",
      "Validation Loss for fold 4: 0.08435400078694026\n",
      "Validation Loss for fold 4: 0.07994767526785533\n",
      "Validation Loss for fold 4: 0.07632069413860638\n",
      "Validation Loss for fold 4: 0.07463587696353595\n",
      "Validation Loss for fold 4: 0.08320108304421107\n",
      "Validation Loss for fold 4: 0.08046459158261617\n",
      "Validation Loss for fold 4: 0.08407150705655415\n",
      "Validation Loss for fold 4: 0.07159137725830078\n",
      "Validation Loss for fold 4: 0.07745389143625896\n",
      "Validation Loss for fold 4: 0.07779734085003535\n",
      "Validation Loss for fold 4: 0.07475212464729945\n",
      "Validation Loss for fold 4: 0.0802126054962476\n",
      "Validation Loss for fold 4: 0.08080808569987614\n",
      "Validation Loss for fold 4: 0.08683108290036519\n",
      "Validation Loss for fold 4: 0.07853335390488307\n",
      "Validation Loss for fold 4: 0.07922881096601486\n",
      "Validation Loss for fold 4: 0.0777244120836258\n",
      "Validation Loss for fold 4: 0.07369131222367287\n",
      "Validation Loss for fold 4: 0.07970348497231801\n",
      "Validation Loss for fold 4: 0.0758743832508723\n",
      "Validation Loss for fold 4: 0.07543635119994481\n",
      "Validation Loss for fold 4: 0.08240476250648499\n",
      "Validation Loss for fold 4: 0.07203834752241771\n",
      "Validation Loss for fold 4: 0.06669542317589124\n",
      "Validation Loss for fold 4: 0.0730782983203729\n",
      "Validation Loss for fold 4: 0.07625620563824971\n",
      "Validation Loss for fold 4: 0.08217928310235341\n",
      "Validation Loss for fold 4: 0.07672631740570068\n",
      "Validation Loss for fold 4: 0.07452842593193054\n",
      "Validation Loss for fold 4: 0.07681890328725179\n",
      "Validation Loss for fold 4: 0.07732123384873073\n",
      "Validation Loss for fold 4: 0.07764859249194463\n",
      "Validation Loss for fold 4: 0.07070149232943852\n",
      "Validation Loss for fold 4: 0.07651033749183019\n",
      "Validation Loss for fold 4: 0.07806002845366795\n",
      "Validation Loss for fold 4: 0.06863044574856758\n",
      "Validation Loss for fold 4: 0.0718552718559901\n",
      "Validation Loss for fold 4: 0.0751969317595164\n",
      "Validation Loss for fold 4: 0.08003109196821849\n",
      "Validation Loss for fold 4: 0.07373306900262833\n",
      "Validation Loss for fold 4: 0.06614985689520836\n",
      "Validation Loss for fold 4: 0.07561881591876347\n",
      "Validation Loss for fold 4: 0.07379763076702754\n",
      "Validation Loss for fold 4: 0.07134179025888443\n",
      "Validation Loss for fold 4: 0.06756073857347171\n",
      "Validation Loss for fold 4: 0.06979722778002422\n",
      "Validation Loss for fold 4: 0.06101637954513232\n",
      "Validation Loss for fold 4: 0.06621689225236575\n",
      "Validation Loss for fold 4: 0.07477398589253426\n",
      "Validation Loss for fold 4: 0.07076136022806168\n",
      "Validation Loss for fold 4: 0.07679928094148636\n",
      "Validation Loss for fold 4: 0.06888535618782043\n",
      "Validation Loss for fold 4: 0.06845664108792941\n",
      "Validation Loss for fold 4: 0.0696418508887291\n",
      "Validation Loss for fold 4: 0.07339885334173839\n",
      "Validation Loss for fold 4: 0.06717820093035698\n",
      "Validation Loss for fold 4: 0.07722897330919902\n",
      "Validation Loss for fold 4: 0.07000863055388133\n",
      "Validation Loss for fold 4: 0.06833949064215024\n",
      "Validation Loss for fold 4: 0.06295473625262578\n",
      "Validation Loss for fold 4: 0.0649274153014024\n",
      "Validation Loss for fold 4: 0.06752470632394154\n",
      "Validation Loss for fold 4: 0.07311100264390309\n",
      "Validation Loss for fold 4: 0.060343882689873375\n",
      "Validation Loss for fold 4: 0.06433721880118053\n",
      "Validation Loss for fold 4: 0.06751220549146335\n",
      "Validation Loss for fold 4: 0.06341230620940526\n",
      "Validation Loss for fold 4: 0.058356149743000664\n",
      "Validation Loss for fold 4: 0.06767106552918752\n",
      "Validation Loss for fold 4: 0.07060728470484416\n",
      "Validation Loss for fold 4: 0.06796351075172424\n",
      "Validation Loss for fold 4: 0.07257870212197304\n",
      "Validation Loss for fold 4: 0.06666579097509384\n",
      "Validation Loss for fold 4: 0.06729076306025188\n",
      "Validation Loss for fold 4: 0.05716709792613983\n",
      "Validation Loss for fold 4: 0.0650070458650589\n",
      "Validation Loss for fold 4: 0.06138545895616213\n",
      "Validation Loss for fold 4: 0.061694600929816566\n",
      "Validation Loss for fold 4: 0.05870393539468447\n",
      "Validation Loss for fold 4: 0.05969086165229479\n",
      "Validation Loss for fold 4: 0.05615453732510408\n",
      "Validation Loss for fold 4: 0.05588770595689615\n",
      "Validation Loss for fold 4: 0.06478822479645412\n",
      "Validation Loss for fold 4: 0.06771647805968921\n",
      "Validation Loss for fold 4: 0.06935617700219154\n",
      "Validation Loss for fold 4: 0.06471861650546391\n",
      "Validation Loss for fold 4: 0.06189568464954694\n",
      "Validation Loss for fold 4: 0.06286537150541942\n",
      "Validation Loss for fold 4: 0.05920420090357462\n",
      "Validation Loss for fold 4: 0.0635462465385596\n",
      "Validation Loss for fold 4: 0.06278243412574132\n",
      "Validation Loss for fold 4: 0.06257942567269008\n",
      "Validation Loss for fold 4: 0.05949253340562185\n",
      "Validation Loss for fold 4: 0.062400855123996735\n",
      "Validation Loss for fold 4: 0.058404166251420975\n",
      "Validation Loss for fold 4: 0.06185577188928922\n",
      "Validation Loss for fold 4: 0.0589839369058609\n",
      "Validation Loss for fold 4: 0.06565687308708827\n",
      "Validation Loss for fold 4: 0.06283711393674214\n",
      "Validation Loss for fold 4: 0.06310502315560977\n",
      "Validation Loss for fold 4: 0.061565566807985306\n",
      "Validation Loss for fold 4: 0.055316259463628135\n",
      "Validation Loss for fold 4: 0.05568212270736694\n",
      "Validation Loss for fold 4: 0.057478939493497215\n",
      "Validation Loss for fold 4: 0.05871206521987915\n",
      "Validation Loss for fold 4: 0.06223053112626076\n",
      "Validation Loss for fold 4: 0.05784916256864866\n",
      "Validation Loss for fold 4: 0.060329037408034004\n",
      "Validation Loss for fold 4: 0.055565256625413895\n",
      "Validation Loss for fold 4: 0.05683323120077451\n",
      "Validation Loss for fold 4: 0.05860602358977\n",
      "Validation Loss for fold 4: 0.05186573788523674\n",
      "Validation Loss for fold 4: 0.06331401566664378\n",
      "Validation Loss for fold 4: 0.06043091416358948\n",
      "Validation Loss for fold 4: 0.056443460285663605\n",
      "Validation Loss for fold 4: 0.056927682211001716\n",
      "Validation Loss for fold 4: 0.054674496253331505\n",
      "Validation Loss for fold 4: 0.058313330014546715\n",
      "Validation Loss for fold 4: 0.057975590229034424\n",
      "Validation Loss for fold 4: 0.06196265667676926\n",
      "Validation Loss for fold 4: 0.061610509951909385\n",
      "Validation Loss for fold 4: 0.0568119411667188\n",
      "Validation Loss for fold 4: 0.06653937076528867\n",
      "Validation Loss for fold 4: 0.05409680555264155\n",
      "Validation Loss for fold 4: 0.05681906516353289\n",
      "Validation Loss for fold 4: 0.050725659976402916\n",
      "Validation Loss for fold 4: 0.056704473992188774\n",
      "Validation Loss for fold 4: 0.05682223786910375\n",
      "Validation Loss for fold 4: 0.05832694346706072\n",
      "Validation Loss for fold 4: 0.054723733415206276\n",
      "Validation Loss for fold 4: 0.054007348914941154\n",
      "Validation Loss for fold 4: 0.06274073322614034\n",
      "Validation Loss for fold 4: 0.05371511851747831\n",
      "Validation Loss for fold 4: 0.056587960571050644\n",
      "Validation Loss for fold 4: 0.05809790020187696\n",
      "Validation Loss for fold 4: 0.058934482435385384\n",
      "Validation Loss for fold 4: 0.054591625928878784\n",
      "Validation Loss for fold 4: 0.058047069857517876\n",
      "Validation Loss for fold 4: 0.05744358276327451\n",
      "Validation Loss for fold 4: 0.05892233674724897\n",
      "Validation Loss for fold 4: 0.055133115500211716\n",
      "Validation Loss for fold 4: 0.05693582072854042\n",
      "Validation Loss for fold 4: 0.05973022306958834\n",
      "Validation Loss for fold 4: 0.06149849916497866\n",
      "Validation Loss for fold 4: 0.04877557853857676\n",
      "Validation Loss for fold 4: 0.05620777358611425\n",
      "Validation Loss for fold 4: 0.05339665959278742\n",
      "Validation Loss for fold 4: 0.05677386869986852\n",
      "Validation Loss for fold 4: 0.052036830534537636\n",
      "Validation Loss for fold 4: 0.05426492293675741\n",
      "Validation Loss for fold 4: 0.0497038463751475\n",
      "Validation Loss for fold 4: 0.05897845700383186\n",
      "Validation Loss for fold 4: 0.0502678689857324\n",
      "Validation Loss for fold 4: 0.05922406787673632\n",
      "Validation Loss for fold 4: 0.0579306793709596\n",
      "Validation Loss for fold 4: 0.052545548727115\n",
      "Validation Loss for fold 4: 0.060230287412802376\n",
      "Validation Loss for fold 4: 0.060024622827768326\n",
      "Validation Loss for fold 4: 0.058176927268505096\n",
      "Validation Loss for fold 4: 0.05654366686940193\n",
      "Validation Loss for fold 4: 0.04753344878554344\n",
      "Validation Loss for fold 4: 0.05158958584070206\n",
      "Validation Loss for fold 4: 0.049232584734757744\n",
      "Validation Loss for fold 4: 0.05709452927112579\n",
      "Validation Loss for fold 4: 0.05203079556425413\n",
      "Validation Loss for fold 4: 0.05468074480692545\n",
      "Validation Loss for fold 4: 0.04962565874059995\n",
      "Validation Loss for fold 4: 0.05171399315198263\n",
      "Validation Loss for fold 4: 0.05307773997386297\n",
      "Validation Loss for fold 4: 0.04664349555969238\n",
      "Validation Loss for fold 4: 0.06046112502614657\n",
      "Validation Loss for fold 4: 0.052062260607878365\n",
      "Validation Loss for fold 4: 0.05711378653844198\n",
      "Validation Loss for fold 4: 0.05625916769107183\n",
      "Validation Loss for fold 4: 0.05392337217926979\n",
      "Validation Loss for fold 4: 0.05027078464627266\n",
      "Validation Loss for fold 4: 0.05390245715777079\n",
      "Validation Loss for fold 4: 0.051255730291207634\n",
      "Validation Loss for fold 4: 0.052121938516696296\n",
      "Validation Loss for fold 4: 0.05343978231151899\n",
      "Validation Loss for fold 4: 0.054638784378767014\n",
      "Validation Loss for fold 4: 0.049123531828324\n",
      "Validation Loss for fold 4: 0.04984437177578608\n",
      "Validation Loss for fold 4: 0.04636280486981074\n",
      "Validation Loss for fold 4: 0.049069530020157494\n",
      "Validation Loss for fold 4: 0.053350300838549934\n",
      "Validation Loss for fold 4: 0.05296564226349195\n",
      "Validation Loss for fold 4: 0.0507450004418691\n",
      "Validation Loss for fold 4: 0.05261105795701345\n",
      "Validation Loss for fold 4: 0.0470023105541865\n",
      "Validation Loss for fold 4: 0.052600195010503135\n",
      "Validation Loss for fold 4: 0.05090935652454694\n",
      "Validation Loss for fold 4: 0.054148549834887184\n",
      "Validation Loss for fold 4: 0.05437567581733068\n",
      "Validation Loss for fold 4: 0.04772550240159035\n",
      "Validation Loss for fold 4: 0.04968181997537613\n",
      "Validation Loss for fold 4: 0.0459175289918979\n",
      "Validation Loss for fold 4: 0.050919936348994575\n",
      "Validation Loss for fold 4: 0.05326323087016741\n",
      "Validation Loss for fold 4: 0.05125032116969427\n",
      "Validation Loss for fold 4: 0.049765683710575104\n",
      "Validation Loss for fold 4: 0.04773072153329849\n",
      "Validation Loss for fold 4: 0.04876288895805677\n",
      "Validation Loss for fold 4: 0.05155760794878006\n",
      "Validation Loss for fold 4: 0.05745735143621763\n",
      "Validation Loss for fold 4: 0.050920091569423676\n",
      "Validation Loss for fold 4: 0.04745821530620257\n",
      "Validation Loss for fold 4: 0.05066685378551483\n",
      "Validation Loss for fold 4: 0.04534200703104337\n",
      "Validation Loss for fold 4: 0.04741217692693075\n",
      "Validation Loss for fold 4: 0.04974481835961342\n",
      "Validation Loss for fold 4: 0.047872052838404976\n",
      "Validation Loss for fold 4: 0.04683930551012357\n",
      "Validation Loss for fold 4: 0.05038800090551376\n",
      "Validation Loss for fold 4: 0.04782002046704292\n",
      "Validation Loss for fold 4: 0.048449382185935974\n",
      "Validation Loss for fold 4: 0.04753902927041054\n",
      "Validation Loss for fold 4: 0.048084902266661324\n",
      "Validation Loss for fold 4: 0.04794661576549212\n",
      "Validation Loss for fold 4: 0.0477344219883283\n",
      "Validation Loss for fold 4: 0.05023606618245443\n",
      "Validation Loss for fold 4: 0.051448799669742584\n",
      "Validation Loss for fold 4: 0.04871981218457222\n",
      "Validation Loss for fold 4: 0.05044537037611008\n",
      "Validation Loss for fold 4: 0.04831662277380625\n",
      "Validation Loss for fold 4: 0.05054680133859316\n",
      "Validation Loss for fold 4: 0.054232808450857796\n",
      "Validation Loss for fold 4: 0.0500630276898543\n",
      "Validation Loss for fold 4: 0.054064386834700905\n",
      "Validation Loss for fold 4: 0.04840845242142677\n",
      "Validation Loss for fold 4: 0.05060198033849398\n",
      "Validation Loss for fold 4: 0.04429212585091591\n",
      "Validation Loss for fold 4: 0.047477097560962044\n",
      "Validation Loss for fold 4: 0.05199119200309118\n",
      "Validation Loss for fold 4: 0.05462077011664709\n",
      "Validation Loss for fold 4: 0.053556532909472786\n",
      "Validation Loss for fold 4: 0.05185724049806595\n",
      "Validation Loss for fold 4: 0.0514308325946331\n",
      "Validation Loss for fold 4: 0.052005668481191\n",
      "Validation Loss for fold 4: 0.049375295639038086\n",
      "Validation Loss for fold 4: 0.05209862068295479\n",
      "Validation Loss for fold 4: 0.04655409976840019\n",
      "Validation Loss for fold 4: 0.04739439611633619\n",
      "Validation Loss for fold 4: 0.055087972432374954\n",
      "Validation Loss for fold 4: 0.04692366470893224\n",
      "Validation Loss for fold 4: 0.05221926048398018\n",
      "Validation Loss for fold 4: 0.04952147727211317\n",
      "Validation Loss for fold 4: 0.04440110735595226\n",
      "Validation Loss for fold 4: 0.05258827283978462\n",
      "Validation Loss for fold 4: 0.05417575314640999\n",
      "Validation Loss for fold 4: 0.048302944749593735\n",
      "Validation Loss for fold 4: 0.043361092607180275\n",
      "Validation Loss for fold 4: 0.05210082481304804\n",
      "Validation Loss for fold 4: 0.04877053449551264\n",
      "Validation Loss for fold 4: 0.046898296723763146\n",
      "Validation Loss for fold 4: 0.05187325303753217\n",
      "Validation Loss for fold 4: 0.052478399127721786\n",
      "Validation Loss for fold 4: 0.05051169296105703\n",
      "Validation Loss for fold 4: 0.04881743217507998\n",
      "Validation Loss for fold 4: 0.04674689844250679\n",
      "Validation Loss for fold 4: 0.04919593408703804\n",
      "Validation Loss for fold 4: 0.05002700537443161\n",
      "Validation Loss for fold 4: 0.04927413538098335\n",
      "Validation Loss for fold 4: 0.04820260281364123\n",
      "Validation Loss for fold 4: 0.044535299142201744\n",
      "Validation Loss for fold 4: 0.04992531488339106\n",
      "Validation Loss for fold 4: 0.048005059361457825\n",
      "Validation Loss for fold 4: 0.05107606699069341\n",
      "Validation Loss for fold 4: 0.04837797085444132\n",
      "Validation Loss for fold 4: 0.04787405580282211\n",
      "Validation Loss for fold 4: 0.04581602290272713\n",
      "Validation Loss for fold 4: 0.045595201353232064\n",
      "Validation Loss for fold 4: 0.05018149192134539\n",
      "Validation Loss for fold 4: 0.04705266157786051\n",
      "Validation Loss for fold 4: 0.05084033558766047\n",
      "Validation Loss for fold 4: 0.05201371262470881\n",
      "Validation Loss for fold 4: 0.04565616821249326\n",
      "Validation Loss for fold 4: 0.04845790937542915\n",
      "Validation Loss for fold 4: 0.04895342141389847\n",
      "Validation Loss for fold 4: 0.05060142402847608\n",
      "Validation Loss for fold 4: 0.050237792233626045\n",
      "--------------------------------\n",
      "FOLD 5\n",
      "--------------------------------\n",
      "Validation Loss for fold 5: 0.49488865335782367\n",
      "Validation Loss for fold 5: 0.3658146560192108\n",
      "Validation Loss for fold 5: 0.2939503143231074\n",
      "Validation Loss for fold 5: 0.26353514194488525\n",
      "Validation Loss for fold 5: 0.2385112295548121\n",
      "Validation Loss for fold 5: 0.2176076223452886\n",
      "Validation Loss for fold 5: 0.21475579837958017\n",
      "Validation Loss for fold 5: 0.18380034963289896\n",
      "Validation Loss for fold 5: 0.19126509626706442\n",
      "Validation Loss for fold 5: 0.17421454191207886\n",
      "Validation Loss for fold 5: 0.17725825309753418\n",
      "Validation Loss for fold 5: 0.1449358562628428\n",
      "Validation Loss for fold 5: 0.17552325626214346\n",
      "Validation Loss for fold 5: 0.18140639861424765\n",
      "Validation Loss for fold 5: 0.16148392856121063\n",
      "Validation Loss for fold 5: 0.17753160993258157\n",
      "Validation Loss for fold 5: 0.16060679157574972\n",
      "Validation Loss for fold 5: 0.15934734046459198\n",
      "Validation Loss for fold 5: 0.14282971372207007\n",
      "Validation Loss for fold 5: 0.16285333534081778\n",
      "Validation Loss for fold 5: 0.14763214190800986\n",
      "Validation Loss for fold 5: 0.15116296708583832\n",
      "Validation Loss for fold 5: 0.13507096966107687\n",
      "Validation Loss for fold 5: 0.14619413514931998\n",
      "Validation Loss for fold 5: 0.13470598061879477\n",
      "Validation Loss for fold 5: 0.1465142716964086\n",
      "Validation Loss for fold 5: 0.131952665746212\n",
      "Validation Loss for fold 5: 0.1302080824971199\n",
      "Validation Loss for fold 5: 0.13028011719385782\n",
      "Validation Loss for fold 5: 0.1337940121690432\n",
      "Validation Loss for fold 5: 0.12821677327156067\n",
      "Validation Loss for fold 5: 0.12018136928478877\n",
      "Validation Loss for fold 5: 0.12273117651542027\n",
      "Validation Loss for fold 5: 0.12466677774985631\n",
      "Validation Loss for fold 5: 0.11991783479849498\n",
      "Validation Loss for fold 5: 0.12733488778273264\n",
      "Validation Loss for fold 5: 0.11598460624615352\n",
      "Validation Loss for fold 5: 0.11582762002944946\n",
      "Validation Loss for fold 5: 0.1274671902259191\n",
      "Validation Loss for fold 5: 0.13052621483802795\n",
      "Validation Loss for fold 5: 0.12484218676884969\n",
      "Validation Loss for fold 5: 0.1058753381172816\n",
      "Validation Loss for fold 5: 0.11391631017128627\n",
      "Validation Loss for fold 5: 0.12265975773334503\n",
      "Validation Loss for fold 5: 0.1192496046423912\n",
      "Validation Loss for fold 5: 0.1105991577108701\n",
      "Validation Loss for fold 5: 0.12371017038822174\n",
      "Validation Loss for fold 5: 0.11751679579416911\n",
      "Validation Loss for fold 5: 0.11242106805245082\n",
      "Validation Loss for fold 5: 0.11009946713844936\n",
      "Validation Loss for fold 5: 0.1119413748383522\n",
      "Validation Loss for fold 5: 0.11479426175355911\n",
      "Validation Loss for fold 5: 0.11438915381828944\n",
      "Validation Loss for fold 5: 0.10045420378446579\n",
      "Validation Loss for fold 5: 0.10496919602155685\n",
      "Validation Loss for fold 5: 0.10486924896637599\n",
      "Validation Loss for fold 5: 0.11142414559920628\n",
      "Validation Loss for fold 5: 0.10641753176848094\n",
      "Validation Loss for fold 5: 0.10908058285713196\n",
      "Validation Loss for fold 5: 0.10498402267694473\n",
      "Validation Loss for fold 5: 0.10485538840293884\n",
      "Validation Loss for fold 5: 0.10152094562848409\n",
      "Validation Loss for fold 5: 0.09875643998384476\n",
      "Validation Loss for fold 5: 0.09786157061656316\n",
      "Validation Loss for fold 5: 0.10281355927387874\n",
      "Validation Loss for fold 5: 0.10599146534999211\n",
      "Validation Loss for fold 5: 0.09907756000757217\n",
      "Validation Loss for fold 5: 0.09960699329773585\n",
      "Validation Loss for fold 5: 0.09397291888793309\n",
      "Validation Loss for fold 5: 0.09942730516195297\n",
      "Validation Loss for fold 5: 0.09949024766683578\n",
      "Validation Loss for fold 5: 0.10021275281906128\n",
      "Validation Loss for fold 5: 0.09926938265562057\n",
      "Validation Loss for fold 5: 0.09309772402048111\n",
      "Validation Loss for fold 5: 0.09861621260643005\n",
      "Validation Loss for fold 5: 0.0941508486866951\n",
      "Validation Loss for fold 5: 0.09646038711071014\n",
      "Validation Loss for fold 5: 0.0930447851618131\n",
      "Validation Loss for fold 5: 0.08612127602100372\n",
      "Validation Loss for fold 5: 0.09799295912186305\n",
      "Validation Loss for fold 5: 0.10242120673259099\n",
      "Validation Loss for fold 5: 0.08864025523265202\n",
      "Validation Loss for fold 5: 0.10221352676550548\n",
      "Validation Loss for fold 5: 0.09219623853762944\n",
      "Validation Loss for fold 5: 0.1060478041569392\n",
      "Validation Loss for fold 5: 0.08717960615952809\n",
      "Validation Loss for fold 5: 0.1058216542005539\n",
      "Validation Loss for fold 5: 0.0892788643638293\n",
      "Validation Loss for fold 5: 0.09483336408933003\n",
      "Validation Loss for fold 5: 0.08981747180223465\n",
      "Validation Loss for fold 5: 0.08878400673468907\n",
      "Validation Loss for fold 5: 0.08420231193304062\n",
      "Validation Loss for fold 5: 0.08932362496852875\n",
      "Validation Loss for fold 5: 0.08357596149047215\n",
      "Validation Loss for fold 5: 0.09212860961755116\n",
      "Validation Loss for fold 5: 0.08335154379407565\n",
      "Validation Loss for fold 5: 0.08484091113011043\n",
      "Validation Loss for fold 5: 0.07906399543086688\n",
      "Validation Loss for fold 5: 0.08504434178272884\n",
      "Validation Loss for fold 5: 0.08977014323075612\n",
      "Validation Loss for fold 5: 0.08726192514101665\n",
      "Validation Loss for fold 5: 0.08082691580057144\n",
      "Validation Loss for fold 5: 0.08601070195436478\n",
      "Validation Loss for fold 5: 0.07733861481149991\n",
      "Validation Loss for fold 5: 0.08296298732360204\n",
      "Validation Loss for fold 5: 0.08139918247858684\n",
      "Validation Loss for fold 5: 0.0836755633354187\n",
      "Validation Loss for fold 5: 0.08407698074976604\n",
      "Validation Loss for fold 5: 0.08074609190225601\n",
      "Validation Loss for fold 5: 0.08343585580587387\n",
      "Validation Loss for fold 5: 0.08221173783143361\n",
      "Validation Loss for fold 5: 0.08173634608586629\n",
      "Validation Loss for fold 5: 0.0800066888332367\n",
      "Validation Loss for fold 5: 0.08004248887300491\n",
      "Validation Loss for fold 5: 0.07615801692008972\n",
      "Validation Loss for fold 5: 0.07957899818817775\n",
      "Validation Loss for fold 5: 0.07935597747564316\n",
      "Validation Loss for fold 5: 0.08011722813049953\n",
      "Validation Loss for fold 5: 0.07911234100659688\n",
      "Validation Loss for fold 5: 0.07355398933092754\n",
      "Validation Loss for fold 5: 0.07357007389267285\n",
      "Validation Loss for fold 5: 0.08028188596169154\n",
      "Validation Loss for fold 5: 0.07905797411998113\n",
      "Validation Loss for fold 5: 0.07716719682017963\n",
      "Validation Loss for fold 5: 0.07960473001003265\n",
      "Validation Loss for fold 5: 0.08736088871955872\n",
      "Validation Loss for fold 5: 0.07872618983189265\n",
      "Validation Loss for fold 5: 0.0790685663620631\n",
      "Validation Loss for fold 5: 0.08229348560174306\n",
      "Validation Loss for fold 5: 0.07624715318282445\n",
      "Validation Loss for fold 5: 0.08001719415187836\n",
      "Validation Loss for fold 5: 0.0782292236884435\n",
      "Validation Loss for fold 5: 0.08387409895658493\n",
      "Validation Loss for fold 5: 0.07692620406548183\n",
      "Validation Loss for fold 5: 0.07707685480515163\n",
      "Validation Loss for fold 5: 0.07965071747700374\n",
      "Validation Loss for fold 5: 0.07554532090822856\n",
      "Validation Loss for fold 5: 0.07678100963433583\n",
      "Validation Loss for fold 5: 0.07723279669880867\n",
      "Validation Loss for fold 5: 0.07691210756699245\n",
      "Validation Loss for fold 5: 0.07716751843690872\n",
      "Validation Loss for fold 5: 0.07531361033519109\n",
      "Validation Loss for fold 5: 0.07628569255272548\n",
      "Validation Loss for fold 5: 0.06757489467660587\n",
      "Validation Loss for fold 5: 0.07404286911090215\n",
      "Validation Loss for fold 5: 0.07067660242319107\n",
      "Validation Loss for fold 5: 0.07037314027547836\n",
      "Validation Loss for fold 5: 0.0703646478553613\n",
      "Validation Loss for fold 5: 0.07185066243012746\n",
      "Validation Loss for fold 5: 0.07208529363075893\n",
      "Validation Loss for fold 5: 0.07295212149620056\n",
      "Validation Loss for fold 5: 0.06930568565924962\n",
      "Validation Loss for fold 5: 0.07516705244779587\n",
      "Validation Loss for fold 5: 0.07542504618565242\n",
      "Validation Loss for fold 5: 0.06508661185701688\n",
      "Validation Loss for fold 5: 0.07873486603299777\n",
      "Validation Loss for fold 5: 0.07101588944594066\n",
      "Validation Loss for fold 5: 0.07196477303902309\n",
      "Validation Loss for fold 5: 0.07055187473694484\n",
      "Validation Loss for fold 5: 0.07729220141967137\n",
      "Validation Loss for fold 5: 0.0646236886580785\n",
      "Validation Loss for fold 5: 0.0763279100259145\n",
      "Validation Loss for fold 5: 0.07071577260891597\n",
      "Validation Loss for fold 5: 0.06874739130338033\n",
      "Validation Loss for fold 5: 0.07470892369747162\n",
      "Validation Loss for fold 5: 0.07371025035778682\n",
      "Validation Loss for fold 5: 0.06625158091386159\n",
      "Validation Loss for fold 5: 0.06842799857258797\n",
      "Validation Loss for fold 5: 0.07310588906208675\n",
      "Validation Loss for fold 5: 0.07798861960570018\n",
      "Validation Loss for fold 5: 0.06949695944786072\n",
      "Validation Loss for fold 5: 0.075258602698644\n",
      "Validation Loss for fold 5: 0.07113242894411087\n",
      "Validation Loss for fold 5: 0.06508855894207954\n",
      "Validation Loss for fold 5: 0.07397200788060825\n",
      "Validation Loss for fold 5: 0.06411337852478027\n",
      "Validation Loss for fold 5: 0.06598679969708125\n",
      "Validation Loss for fold 5: 0.06892925004164378\n",
      "Validation Loss for fold 5: 0.06830199062824249\n",
      "Validation Loss for fold 5: 0.06321350112557411\n",
      "Validation Loss for fold 5: 0.07994403814276059\n",
      "Validation Loss for fold 5: 0.06955146292845409\n",
      "Validation Loss for fold 5: 0.07056637853384018\n",
      "Validation Loss for fold 5: 0.07231966157754262\n",
      "Validation Loss for fold 5: 0.0662861242890358\n",
      "Validation Loss for fold 5: 0.07056525101264317\n",
      "Validation Loss for fold 5: 0.06985106319189072\n",
      "Validation Loss for fold 5: 0.068024596820275\n",
      "Validation Loss for fold 5: 0.06367378309369087\n",
      "Validation Loss for fold 5: 0.06919946397344272\n",
      "Validation Loss for fold 5: 0.06365352869033813\n",
      "Validation Loss for fold 5: 0.068931279083093\n",
      "Validation Loss for fold 5: 0.06399845083554585\n",
      "Validation Loss for fold 5: 0.05759336240589619\n",
      "Validation Loss for fold 5: 0.07238256682952245\n",
      "Validation Loss for fold 5: 0.06996749838193257\n",
      "Validation Loss for fold 5: 0.0672735869884491\n",
      "Validation Loss for fold 5: 0.07048485428094864\n",
      "Validation Loss for fold 5: 0.06787260621786118\n",
      "Validation Loss for fold 5: 0.06824279700716336\n",
      "Validation Loss for fold 5: 0.06725840518871944\n",
      "Validation Loss for fold 5: 0.06478895246982574\n",
      "Validation Loss for fold 5: 0.06400606160362561\n",
      "Validation Loss for fold 5: 0.07293037946025531\n",
      "Validation Loss for fold 5: 0.061597598095734916\n",
      "Validation Loss for fold 5: 0.0673774778842926\n",
      "Validation Loss for fold 5: 0.06557632610201836\n",
      "Validation Loss for fold 5: 0.06731664141019185\n",
      "Validation Loss for fold 5: 0.06867026537656784\n",
      "Validation Loss for fold 5: 0.07132205367088318\n",
      "Validation Loss for fold 5: 0.07626427213350932\n",
      "Validation Loss for fold 5: 0.06353675077358882\n",
      "Validation Loss for fold 5: 0.06796036660671234\n",
      "Validation Loss for fold 5: 0.0689393828312556\n",
      "Validation Loss for fold 5: 0.06589304655790329\n",
      "Validation Loss for fold 5: 0.06505405778686206\n",
      "Validation Loss for fold 5: 0.06880412623286247\n",
      "Validation Loss for fold 5: 0.07653317724665006\n",
      "Validation Loss for fold 5: 0.06119486317038536\n",
      "Validation Loss for fold 5: 0.06295091783006986\n",
      "Validation Loss for fold 5: 0.06685546785593033\n",
      "Validation Loss for fold 5: 0.06131005038817724\n",
      "Validation Loss for fold 5: 0.060152397801478706\n",
      "Validation Loss for fold 5: 0.060160212218761444\n",
      "Validation Loss for fold 5: 0.06614683320124944\n",
      "Validation Loss for fold 5: 0.06245872254172961\n",
      "Validation Loss for fold 5: 0.06684469804167747\n",
      "Validation Loss for fold 5: 0.06691306581099828\n",
      "Validation Loss for fold 5: 0.06364660213390987\n",
      "Validation Loss for fold 5: 0.06383975719412167\n",
      "Validation Loss for fold 5: 0.06779368097583453\n",
      "Validation Loss for fold 5: 0.06798476353287697\n",
      "Validation Loss for fold 5: 0.06952976311246555\n",
      "Validation Loss for fold 5: 0.0688590444624424\n",
      "Validation Loss for fold 5: 0.06215562050541242\n",
      "Validation Loss for fold 5: 0.060257560263077416\n",
      "Validation Loss for fold 5: 0.06458677972356479\n",
      "Validation Loss for fold 5: 0.060768965631723404\n",
      "Validation Loss for fold 5: 0.0696820579469204\n",
      "Validation Loss for fold 5: 0.06225182364384333\n",
      "Validation Loss for fold 5: 0.059149276465177536\n",
      "Validation Loss for fold 5: 0.06463908776640892\n",
      "Validation Loss for fold 5: 0.06771615768472354\n",
      "Validation Loss for fold 5: 0.0588527445991834\n",
      "Validation Loss for fold 5: 0.057199535270531975\n",
      "Validation Loss for fold 5: 0.0582940640548865\n",
      "Validation Loss for fold 5: 0.06271083652973175\n",
      "Validation Loss for fold 5: 0.06132027258475622\n",
      "Validation Loss for fold 5: 0.0617151694993178\n",
      "Validation Loss for fold 5: 0.0644237610201041\n",
      "Validation Loss for fold 5: 0.06199733912944794\n",
      "Validation Loss for fold 5: 0.06151993448535601\n",
      "Validation Loss for fold 5: 0.0641360878944397\n",
      "Validation Loss for fold 5: 0.061996426433324814\n",
      "Validation Loss for fold 5: 0.06665505841374397\n",
      "Validation Loss for fold 5: 0.06704494481285413\n",
      "Validation Loss for fold 5: 0.06628028303384781\n",
      "Validation Loss for fold 5: 0.06438406805197398\n",
      "Validation Loss for fold 5: 0.06417671715219815\n",
      "Validation Loss for fold 5: 0.062095762540896736\n",
      "Validation Loss for fold 5: 0.06438487023115158\n",
      "Validation Loss for fold 5: 0.06255355974038442\n",
      "Validation Loss for fold 5: 0.06156733011205991\n",
      "Validation Loss for fold 5: 0.060677990317344666\n",
      "Validation Loss for fold 5: 0.05952651177843412\n",
      "Validation Loss for fold 5: 0.05842511976758639\n",
      "Validation Loss for fold 5: 0.06392214571436246\n",
      "Validation Loss for fold 5: 0.05735629424452782\n",
      "Validation Loss for fold 5: 0.0661381147801876\n",
      "Validation Loss for fold 5: 0.06309367095430692\n",
      "Validation Loss for fold 5: 0.05799144754807154\n",
      "Validation Loss for fold 5: 0.06382504974802335\n",
      "Validation Loss for fold 5: 0.06627931073307991\n",
      "Validation Loss for fold 5: 0.06181540712714195\n",
      "Validation Loss for fold 5: 0.06155281886458397\n",
      "Validation Loss for fold 5: 0.06437896440426509\n",
      "Validation Loss for fold 5: 0.06302783638238907\n",
      "Validation Loss for fold 5: 0.06638848036527634\n",
      "Validation Loss for fold 5: 0.06241646657387415\n",
      "Validation Loss for fold 5: 0.05958010877172152\n",
      "Validation Loss for fold 5: 0.06178601955374082\n",
      "Validation Loss for fold 5: 0.06171345462401708\n",
      "Validation Loss for fold 5: 0.062074134747187294\n",
      "Validation Loss for fold 5: 0.060356066872676216\n",
      "Validation Loss for fold 5: 0.06597153594096501\n",
      "Validation Loss for fold 5: 0.06291106715798378\n",
      "Validation Loss for fold 5: 0.06137999892234802\n",
      "Validation Loss for fold 5: 0.05838693678379059\n",
      "Validation Loss for fold 5: 0.06384389847517014\n",
      "Validation Loss for fold 5: 0.06586645543575287\n",
      "Validation Loss for fold 5: 0.06308792158961296\n",
      "Validation Loss for fold 5: 0.057166251043478646\n",
      "Validation Loss for fold 5: 0.0639436071117719\n",
      "Validation Loss for fold 5: 0.06203903630375862\n",
      "Validation Loss for fold 5: 0.06306410332520802\n",
      "Validation Loss for fold 5: 0.056517623364925385\n",
      "Validation Loss for fold 5: 0.06424775471289952\n",
      "Validation Loss for fold 5: 0.06337951372067134\n",
      "Validation Loss for fold 5: 0.057470026115576424\n",
      "Validation Loss for fold 5: 0.05806354930003484\n",
      "--------------------------------\n",
      "FOLD 6\n",
      "--------------------------------\n",
      "Validation Loss for fold 6: 0.2636616031328837\n",
      "Validation Loss for fold 6: 0.24391657610734305\n",
      "Validation Loss for fold 6: 0.21021765967210135\n",
      "Validation Loss for fold 6: 0.17893206079800925\n",
      "Validation Loss for fold 6: 0.1626145193974177\n",
      "Validation Loss for fold 6: 0.1525666465361913\n",
      "Validation Loss for fold 6: 0.14305897057056427\n",
      "Validation Loss for fold 6: 0.1344512477517128\n",
      "Validation Loss for fold 6: 0.1306151400009791\n",
      "Validation Loss for fold 6: 0.13164241115252176\n",
      "Validation Loss for fold 6: 0.12429386128981908\n",
      "Validation Loss for fold 6: 0.12758649388949075\n",
      "Validation Loss for fold 6: 0.1341757078965505\n",
      "Validation Loss for fold 6: 0.12253917505343755\n",
      "Validation Loss for fold 6: 0.11432432383298874\n",
      "Validation Loss for fold 6: 0.11577191452185313\n",
      "Validation Loss for fold 6: 0.10739534099896748\n",
      "Validation Loss for fold 6: 0.10313236713409424\n",
      "Validation Loss for fold 6: 0.10359295705954234\n",
      "Validation Loss for fold 6: 0.11636316031217575\n",
      "Validation Loss for fold 6: 0.10201797386010487\n",
      "Validation Loss for fold 6: 0.10785757501920064\n",
      "Validation Loss for fold 6: 0.09830202410618465\n",
      "Validation Loss for fold 6: 0.09456692139307658\n",
      "Validation Loss for fold 6: 0.08899146070082982\n",
      "Validation Loss for fold 6: 0.08904400964577992\n",
      "Validation Loss for fold 6: 0.09075429290533066\n",
      "Validation Loss for fold 6: 0.09902654588222504\n",
      "Validation Loss for fold 6: 0.09688801070054372\n",
      "Validation Loss for fold 6: 0.10136586676041286\n",
      "Validation Loss for fold 6: 0.10134147107601166\n",
      "Validation Loss for fold 6: 0.08759954571723938\n",
      "Validation Loss for fold 6: 0.09260221819082896\n",
      "Validation Loss for fold 6: 0.09817445526520412\n",
      "Validation Loss for fold 6: 0.08856012423833211\n",
      "Validation Loss for fold 6: 0.0967344914873441\n",
      "Validation Loss for fold 6: 0.09376879284779231\n",
      "Validation Loss for fold 6: 0.09258036067088445\n",
      "Validation Loss for fold 6: 0.08968485891819\n",
      "Validation Loss for fold 6: 0.09568517655134201\n",
      "Validation Loss for fold 6: 0.09716301411390305\n",
      "Validation Loss for fold 6: 0.08509354044993718\n",
      "Validation Loss for fold 6: 0.0925849253932635\n",
      "Validation Loss for fold 6: 0.09081947555144627\n",
      "Validation Loss for fold 6: 0.08676380912462871\n",
      "Validation Loss for fold 6: 0.0900098904967308\n",
      "Validation Loss for fold 6: 0.08191177000602086\n",
      "Validation Loss for fold 6: 0.07768889516592026\n",
      "Validation Loss for fold 6: 0.080807164311409\n",
      "Validation Loss for fold 6: 0.08917450408140819\n",
      "Validation Loss for fold 6: 0.08444197724262874\n",
      "Validation Loss for fold 6: 0.08991472919782002\n",
      "Validation Loss for fold 6: 0.07913644363482793\n",
      "Validation Loss for fold 6: 0.08004600306351979\n",
      "Validation Loss for fold 6: 0.0873732715845108\n",
      "Validation Loss for fold 6: 0.07322013129790624\n",
      "Validation Loss for fold 6: 0.08027943720420201\n",
      "Validation Loss for fold 6: 0.08158901830514272\n",
      "Validation Loss for fold 6: 0.07415604094664256\n",
      "Validation Loss for fold 6: 0.08675267547369003\n",
      "Validation Loss for fold 6: 0.0765932450691859\n",
      "Validation Loss for fold 6: 0.08428872873385747\n",
      "Validation Loss for fold 6: 0.07747374475002289\n",
      "Validation Loss for fold 6: 0.07771752526362737\n",
      "Validation Loss for fold 6: 0.08311490217844646\n",
      "Validation Loss for fold 6: 0.08307366321484248\n",
      "Validation Loss for fold 6: 0.0722944363951683\n",
      "Validation Loss for fold 6: 0.07415831585725148\n",
      "Validation Loss for fold 6: 0.07474718739589055\n",
      "Validation Loss for fold 6: 0.0764416034022967\n",
      "Validation Loss for fold 6: 0.0800547127922376\n",
      "Validation Loss for fold 6: 0.07609336823225021\n",
      "Validation Loss for fold 6: 0.07589567949374516\n",
      "Validation Loss for fold 6: 0.07657718906799953\n",
      "Validation Loss for fold 6: 0.07975506037473679\n",
      "Validation Loss for fold 6: 0.07485217601060867\n",
      "Validation Loss for fold 6: 0.07205359886089961\n",
      "Validation Loss for fold 6: 0.07318009187777837\n",
      "Validation Loss for fold 6: 0.07303323596715927\n",
      "Validation Loss for fold 6: 0.07883858929077785\n",
      "Validation Loss for fold 6: 0.07367349912722905\n",
      "Validation Loss for fold 6: 0.0798875167965889\n",
      "Validation Loss for fold 6: 0.0731268326441447\n",
      "Validation Loss for fold 6: 0.07449635118246078\n",
      "Validation Loss for fold 6: 0.06570561230182648\n",
      "Validation Loss for fold 6: 0.07160407801469167\n",
      "Validation Loss for fold 6: 0.07425818592309952\n",
      "Validation Loss for fold 6: 0.07733610272407532\n",
      "Validation Loss for fold 6: 0.06932543963193893\n",
      "Validation Loss for fold 6: 0.0749290535847346\n",
      "Validation Loss for fold 6: 0.06824626276890437\n",
      "Validation Loss for fold 6: 0.06954835231105487\n",
      "Validation Loss for fold 6: 0.07570104921857516\n",
      "Validation Loss for fold 6: 0.07434685279925664\n",
      "Validation Loss for fold 6: 0.06848352899154027\n",
      "Validation Loss for fold 6: 0.07212967425584793\n",
      "Validation Loss for fold 6: 0.06984898447990417\n",
      "Validation Loss for fold 6: 0.06900177896022797\n",
      "Validation Loss for fold 6: 0.0692376047372818\n",
      "Validation Loss for fold 6: 0.06420244773228963\n",
      "Validation Loss for fold 6: 0.07004729410012563\n",
      "Validation Loss for fold 6: 0.07263074442744255\n",
      "Validation Loss for fold 6: 0.07360272482037544\n",
      "Validation Loss for fold 6: 0.07054929931958516\n",
      "Validation Loss for fold 6: 0.07311918586492538\n",
      "Validation Loss for fold 6: 0.07328620304663976\n",
      "Validation Loss for fold 6: 0.07346800963083903\n",
      "Validation Loss for fold 6: 0.06924275308847427\n",
      "Validation Loss for fold 6: 0.07145729164282481\n",
      "Validation Loss for fold 6: 0.0687560463945071\n",
      "Validation Loss for fold 6: 0.06655235091845195\n",
      "Validation Loss for fold 6: 0.06522377207875252\n",
      "Validation Loss for fold 6: 0.061544244488080345\n",
      "Validation Loss for fold 6: 0.07036451001962025\n",
      "Validation Loss for fold 6: 0.06215014308691025\n",
      "Validation Loss for fold 6: 0.06648328900337219\n",
      "Validation Loss for fold 6: 0.06384586915373802\n",
      "Validation Loss for fold 6: 0.06558905293544133\n",
      "Validation Loss for fold 6: 0.0686299130320549\n",
      "Validation Loss for fold 6: 0.07095623264710109\n",
      "Validation Loss for fold 6: 0.06283379718661308\n",
      "Validation Loss for fold 6: 0.07345178226629893\n",
      "Validation Loss for fold 6: 0.06510177254676819\n",
      "Validation Loss for fold 6: 0.07255883514881134\n",
      "Validation Loss for fold 6: 0.06430776293079059\n",
      "Validation Loss for fold 6: 0.0671655498445034\n",
      "Validation Loss for fold 6: 0.06746293604373932\n",
      "Validation Loss for fold 6: 0.06094886238376299\n",
      "Validation Loss for fold 6: 0.058834766348203026\n",
      "Validation Loss for fold 6: 0.060063235461711884\n",
      "Validation Loss for fold 6: 0.06136584281921387\n",
      "Validation Loss for fold 6: 0.060208904246489205\n",
      "Validation Loss for fold 6: 0.06080999970436096\n",
      "Validation Loss for fold 6: 0.061256452153126396\n",
      "Validation Loss for fold 6: 0.06691855068008105\n",
      "Validation Loss for fold 6: 0.0629269431034724\n",
      "Validation Loss for fold 6: 0.0678078110019366\n",
      "Validation Loss for fold 6: 0.06118618696928024\n",
      "Validation Loss for fold 6: 0.05991692095994949\n",
      "Validation Loss for fold 6: 0.06762212763230006\n",
      "Validation Loss for fold 6: 0.06246492142478625\n",
      "Validation Loss for fold 6: 0.06142200902104378\n",
      "Validation Loss for fold 6: 0.06215892111261686\n",
      "Validation Loss for fold 6: 0.056672910849253334\n",
      "Validation Loss for fold 6: 0.06464431186517079\n",
      "Validation Loss for fold 6: 0.06116404508550962\n",
      "Validation Loss for fold 6: 0.05844721322258314\n",
      "Validation Loss for fold 6: 0.059724707156419754\n",
      "Validation Loss for fold 6: 0.05976477389534315\n",
      "Validation Loss for fold 6: 0.06061497703194618\n",
      "Validation Loss for fold 6: 0.0682079829275608\n",
      "Validation Loss for fold 6: 0.06300521269440651\n",
      "Validation Loss for fold 6: 0.06603165715932846\n",
      "Validation Loss for fold 6: 0.05540439486503601\n",
      "Validation Loss for fold 6: 0.0601887454589208\n",
      "Validation Loss for fold 6: 0.06498735149701436\n",
      "Validation Loss for fold 6: 0.061576202511787415\n",
      "Validation Loss for fold 6: 0.06903352340062459\n",
      "Validation Loss for fold 6: 0.06527247528235118\n",
      "Validation Loss for fold 6: 0.06085596978664398\n",
      "Validation Loss for fold 6: 0.06358535587787628\n",
      "Validation Loss for fold 6: 0.05785774812102318\n",
      "Validation Loss for fold 6: 0.06203845888376236\n",
      "Validation Loss for fold 6: 0.05931171029806137\n",
      "Validation Loss for fold 6: 0.06552414099375407\n",
      "Validation Loss for fold 6: 0.06416343276699384\n",
      "Validation Loss for fold 6: 0.07143768295645714\n",
      "Validation Loss for fold 6: 0.05993047853310903\n",
      "Validation Loss for fold 6: 0.059173207730054855\n",
      "Validation Loss for fold 6: 0.05891517053047816\n",
      "Validation Loss for fold 6: 0.0640927826364835\n",
      "Validation Loss for fold 6: 0.059997779627641044\n",
      "Validation Loss for fold 6: 0.06470975528160731\n",
      "Validation Loss for fold 6: 0.05913493285576502\n",
      "Validation Loss for fold 6: 0.06465762108564377\n",
      "Validation Loss for fold 6: 0.060522335271040596\n",
      "Validation Loss for fold 6: 0.05542321254809698\n",
      "Validation Loss for fold 6: 0.05820182835062345\n",
      "Validation Loss for fold 6: 0.06461349005500476\n",
      "Validation Loss for fold 6: 0.05578642959396044\n",
      "Validation Loss for fold 6: 0.06188058853149414\n",
      "Validation Loss for fold 6: 0.05627157042423884\n",
      "Validation Loss for fold 6: 0.06090198208888372\n",
      "Validation Loss for fold 6: 0.05605385328332583\n",
      "Validation Loss for fold 6: 0.06002739444375038\n",
      "Validation Loss for fold 6: 0.05707798898220062\n",
      "Validation Loss for fold 6: 0.05805565416812897\n",
      "Validation Loss for fold 6: 0.06208596626917521\n",
      "Validation Loss for fold 6: 0.054444571336110435\n",
      "Validation Loss for fold 6: 0.050473169113198914\n",
      "Validation Loss for fold 6: 0.05512737358609835\n",
      "Validation Loss for fold 6: 0.06181082377831141\n",
      "Validation Loss for fold 6: 0.05467846244573593\n",
      "Validation Loss for fold 6: 0.057983742405970894\n",
      "Validation Loss for fold 6: 0.055097835759321846\n",
      "Validation Loss for fold 6: 0.05564279109239578\n",
      "Validation Loss for fold 6: 0.05793225516875585\n",
      "Validation Loss for fold 6: 0.05436316877603531\n",
      "Validation Loss for fold 6: 0.05628457417090734\n",
      "Validation Loss for fold 6: 0.06197420756022135\n",
      "Validation Loss for fold 6: 0.05825283254186312\n",
      "Validation Loss for fold 6: 0.06642925490935643\n",
      "Validation Loss for fold 6: 0.0582147849102815\n",
      "Validation Loss for fold 6: 0.057334523648023605\n",
      "Validation Loss for fold 6: 0.061222742001215615\n",
      "Validation Loss for fold 6: 0.05155952895681063\n",
      "Validation Loss for fold 6: 0.05549921095371246\n",
      "Validation Loss for fold 6: 0.06218693032860756\n",
      "Validation Loss for fold 6: 0.05827864011128744\n",
      "Validation Loss for fold 6: 0.05639008805155754\n",
      "Validation Loss for fold 6: 0.05608363449573517\n",
      "Validation Loss for fold 6: 0.05955748756726583\n",
      "Validation Loss for fold 6: 0.05789100999633471\n",
      "Validation Loss for fold 6: 0.0594110960761706\n",
      "Validation Loss for fold 6: 0.054587967693805695\n",
      "Validation Loss for fold 6: 0.05493239313364029\n",
      "Validation Loss for fold 6: 0.0542408749461174\n",
      "Validation Loss for fold 6: 0.05800762648383776\n",
      "Validation Loss for fold 6: 0.05948271229863167\n",
      "Validation Loss for fold 6: 0.05872657895088196\n",
      "Validation Loss for fold 6: 0.0553460530936718\n",
      "Validation Loss for fold 6: 0.05806795383493105\n",
      "Validation Loss for fold 6: 0.060989232112964\n",
      "Validation Loss for fold 6: 0.05657236402233442\n",
      "Validation Loss for fold 6: 0.05859058226148287\n",
      "Validation Loss for fold 6: 0.06097274273633957\n",
      "Validation Loss for fold 6: 0.057276097436745964\n",
      "Validation Loss for fold 6: 0.055970331033070884\n",
      "Validation Loss for fold 6: 0.062243020782868065\n",
      "Validation Loss for fold 6: 0.06250750149289767\n",
      "Validation Loss for fold 6: 0.0598747581243515\n",
      "Validation Loss for fold 6: 0.05783099432786306\n",
      "Validation Loss for fold 6: 0.056628528982400894\n",
      "Validation Loss for fold 6: 0.060080026586850487\n",
      "Validation Loss for fold 6: 0.052867001543442406\n",
      "Validation Loss for fold 6: 0.053494823475678764\n",
      "Validation Loss for fold 6: 0.058489797015984855\n",
      "Validation Loss for fold 6: 0.05662316083908081\n",
      "Validation Loss for fold 6: 0.06372185175617535\n",
      "Validation Loss for fold 6: 0.05594128742814064\n",
      "Validation Loss for fold 6: 0.055737496664126716\n",
      "Validation Loss for fold 6: 0.05831595013538996\n",
      "Validation Loss for fold 6: 0.05076513563593229\n",
      "Validation Loss for fold 6: 0.06926365569233894\n",
      "Validation Loss for fold 6: 0.058436889201402664\n",
      "Validation Loss for fold 6: 0.061243647088607155\n",
      "Validation Loss for fold 6: 0.05555775140722593\n",
      "Validation Loss for fold 6: 0.0499448012560606\n",
      "Validation Loss for fold 6: 0.05619387080272039\n",
      "Validation Loss for fold 6: 0.06002262979745865\n",
      "Validation Loss for fold 6: 0.061385699858268104\n",
      "Validation Loss for fold 6: 0.054260374357302986\n",
      "Validation Loss for fold 6: 0.06392091636856397\n",
      "Validation Loss for fold 6: 0.05999728788932165\n",
      "Validation Loss for fold 6: 0.060996061811844506\n",
      "Validation Loss for fold 6: 0.052384174118439354\n",
      "Validation Loss for fold 6: 0.05642681817213694\n",
      "Validation Loss for fold 6: 0.05332871153950691\n",
      "Validation Loss for fold 6: 0.05517113705476125\n",
      "Validation Loss for fold 6: 0.0573980746169885\n",
      "Validation Loss for fold 6: 0.06370279068748157\n",
      "Validation Loss for fold 6: 0.05642723540465037\n",
      "Validation Loss for fold 6: 0.05406622216105461\n",
      "Validation Loss for fold 6: 0.061994023621082306\n",
      "Validation Loss for fold 6: 0.0545646088818709\n",
      "Validation Loss for fold 6: 0.05945179611444473\n",
      "Validation Loss for fold 6: 0.049540779242912926\n",
      "Validation Loss for fold 6: 0.05399592717488607\n",
      "Validation Loss for fold 6: 0.053645920008420944\n",
      "Validation Loss for fold 6: 0.05035203198591868\n",
      "Validation Loss for fold 6: 0.057893351962169014\n",
      "Validation Loss for fold 6: 0.061985241870085396\n",
      "Validation Loss for fold 6: 0.06010377158721288\n",
      "Validation Loss for fold 6: 0.05450478196144104\n",
      "Validation Loss for fold 6: 0.0576108048359553\n",
      "Validation Loss for fold 6: 0.05798296009500822\n",
      "Validation Loss for fold 6: 0.052008974055449166\n",
      "Validation Loss for fold 6: 0.05328318725029627\n",
      "Validation Loss for fold 6: 0.0586910347143809\n",
      "Validation Loss for fold 6: 0.055651236325502396\n",
      "Validation Loss for fold 6: 0.055288467556238174\n",
      "Validation Loss for fold 6: 0.0562419667840004\n",
      "Validation Loss for fold 6: 0.054104994982481\n",
      "Validation Loss for fold 6: 0.05835748836398125\n",
      "Validation Loss for fold 6: 0.058779891580343246\n",
      "Validation Loss for fold 6: 0.05535362040003141\n",
      "Validation Loss for fold 6: 0.05978580688436826\n",
      "Validation Loss for fold 6: 0.0560577300687631\n",
      "Validation Loss for fold 6: 0.0595466581483682\n",
      "Validation Loss for fold 6: 0.052805850903193154\n",
      "Validation Loss for fold 6: 0.05783432722091675\n",
      "Validation Loss for fold 6: 0.055121476451555886\n",
      "Validation Loss for fold 6: 0.052325901885827385\n",
      "Validation Loss for fold 6: 0.05406452218691508\n",
      "Validation Loss for fold 6: 0.05642032250761986\n",
      "Validation Loss for fold 6: 0.05426176761587461\n",
      "Validation Loss for fold 6: 0.05534771333138148\n",
      "Validation Loss for fold 6: 0.05312338595589002\n",
      "Validation Loss for fold 6: 0.05183360973993937\n",
      "Validation Loss for fold 6: 0.056185875087976456\n",
      "--------------------------------\n",
      "FOLD 7\n",
      "--------------------------------\n",
      "Validation Loss for fold 7: 0.5236359238624573\n",
      "Validation Loss for fold 7: 0.4044982393582662\n",
      "Validation Loss for fold 7: 0.31400742133458454\n",
      "Validation Loss for fold 7: 0.25287628670533496\n",
      "Validation Loss for fold 7: 0.1823994666337967\n",
      "Validation Loss for fold 7: 0.16115208466847739\n",
      "Validation Loss for fold 7: 0.161114235719045\n",
      "Validation Loss for fold 7: 0.12743530670801798\n",
      "Validation Loss for fold 7: 0.11299541840950648\n",
      "Validation Loss for fold 7: 0.10813514143228531\n",
      "Validation Loss for fold 7: 0.10695682466030121\n",
      "Validation Loss for fold 7: 0.09886676073074341\n",
      "Validation Loss for fold 7: 0.10901611546675365\n",
      "Validation Loss for fold 7: 0.1058349460363388\n",
      "Validation Loss for fold 7: 0.09409122665723164\n",
      "Validation Loss for fold 7: 0.10184503843386968\n",
      "Validation Loss for fold 7: 0.09759449462095897\n",
      "Validation Loss for fold 7: 0.09504836549361546\n",
      "Validation Loss for fold 7: 0.09021283189455669\n",
      "Validation Loss for fold 7: 0.089659350613753\n",
      "Validation Loss for fold 7: 0.08497331788142522\n",
      "Validation Loss for fold 7: 0.084425854186217\n",
      "Validation Loss for fold 7: 0.08102152496576309\n",
      "Validation Loss for fold 7: 0.08047952502965927\n",
      "Validation Loss for fold 7: 0.07701514661312103\n",
      "Validation Loss for fold 7: 0.07554931441942851\n",
      "Validation Loss for fold 7: 0.07869833459456761\n",
      "Validation Loss for fold 7: 0.0776946743329366\n",
      "Validation Loss for fold 7: 0.07708148658275604\n",
      "Validation Loss for fold 7: 0.08371537178754807\n",
      "Validation Loss for fold 7: 0.08220570037762324\n",
      "Validation Loss for fold 7: 0.08203248927990596\n",
      "Validation Loss for fold 7: 0.07968172430992126\n",
      "Validation Loss for fold 7: 0.0765934723118941\n",
      "Validation Loss for fold 7: 0.07483082513014476\n",
      "Validation Loss for fold 7: 0.0818068931500117\n",
      "Validation Loss for fold 7: 0.07304644336303075\n",
      "Validation Loss for fold 7: 0.07966644316911697\n",
      "Validation Loss for fold 7: 0.07447136690219243\n",
      "Validation Loss for fold 7: 0.08359875033299129\n",
      "Validation Loss for fold 7: 0.08634140218297641\n",
      "Validation Loss for fold 7: 0.07843355337778728\n",
      "Validation Loss for fold 7: 0.0630500049640735\n",
      "Validation Loss for fold 7: 0.07051205759247144\n",
      "Validation Loss for fold 7: 0.07693402220805486\n",
      "Validation Loss for fold 7: 0.07391935338576634\n",
      "Validation Loss for fold 7: 0.06815702219804128\n",
      "Validation Loss for fold 7: 0.06833327437440555\n",
      "Validation Loss for fold 7: 0.0720754365126292\n",
      "Validation Loss for fold 7: 0.06905822455883026\n",
      "Validation Loss for fold 7: 0.07296984394391377\n",
      "Validation Loss for fold 7: 0.06715958192944527\n",
      "Validation Loss for fold 7: 0.07410165419181187\n",
      "Validation Loss for fold 7: 0.06842816869417827\n",
      "Validation Loss for fold 7: 0.06536458805203438\n",
      "Validation Loss for fold 7: 0.07071550314625104\n",
      "Validation Loss for fold 7: 0.0663205124437809\n",
      "Validation Loss for fold 7: 0.06807230909665425\n",
      "Validation Loss for fold 7: 0.06700502087672551\n",
      "Validation Loss for fold 7: 0.07091377178827922\n",
      "Validation Loss for fold 7: 0.0665598710378011\n",
      "Validation Loss for fold 7: 0.06748818854490916\n",
      "Validation Loss for fold 7: 0.0673100749651591\n",
      "Validation Loss for fold 7: 0.06663139661153157\n",
      "Validation Loss for fold 7: 0.06627989560365677\n",
      "Validation Loss for fold 7: 0.06504092986385028\n",
      "Validation Loss for fold 7: 0.06672166287899017\n",
      "Validation Loss for fold 7: 0.07346201439698537\n",
      "Validation Loss for fold 7: 0.06471591939528783\n",
      "Validation Loss for fold 7: 0.06042713299393654\n",
      "Validation Loss for fold 7: 0.07104676216840744\n",
      "Validation Loss for fold 7: 0.0592474490404129\n",
      "Validation Loss for fold 7: 0.06196911260485649\n",
      "Validation Loss for fold 7: 0.0753942330678304\n",
      "Validation Loss for fold 7: 0.07141607130567233\n",
      "Validation Loss for fold 7: 0.06574023266633351\n",
      "Validation Loss for fold 7: 0.07348861545324326\n",
      "Validation Loss for fold 7: 0.061594525973002114\n",
      "Validation Loss for fold 7: 0.06295573463042577\n",
      "Validation Loss for fold 7: 0.06693029403686523\n",
      "Validation Loss for fold 7: 0.07110235343376796\n",
      "Validation Loss for fold 7: 0.0667734183371067\n",
      "Validation Loss for fold 7: 0.05949929480751356\n",
      "Validation Loss for fold 7: 0.06139296914140383\n",
      "Validation Loss for fold 7: 0.0645432323217392\n",
      "Validation Loss for fold 7: 0.06169957419236501\n",
      "Validation Loss for fold 7: 0.061027211447556816\n",
      "Validation Loss for fold 7: 0.06519962350527446\n",
      "Validation Loss for fold 7: 0.06257344782352448\n",
      "Validation Loss for fold 7: 0.056712827334801354\n",
      "Validation Loss for fold 7: 0.05994605148832003\n",
      "Validation Loss for fold 7: 0.06102821230888367\n",
      "Validation Loss for fold 7: 0.06346227849523227\n",
      "Validation Loss for fold 7: 0.06569008529186249\n",
      "Validation Loss for fold 7: 0.05750665565331777\n",
      "Validation Loss for fold 7: 0.06140168383717537\n",
      "Validation Loss for fold 7: 0.056916482746601105\n",
      "Validation Loss for fold 7: 0.05897484223047892\n",
      "Validation Loss for fold 7: 0.060763328025738396\n",
      "Validation Loss for fold 7: 0.06591366852323215\n",
      "Validation Loss for fold 7: 0.063172847032547\n",
      "Validation Loss for fold 7: 0.06337326765060425\n",
      "Validation Loss for fold 7: 0.061069811383883156\n",
      "Validation Loss for fold 7: 0.05920834590991338\n",
      "Validation Loss for fold 7: 0.05744127805034319\n",
      "Validation Loss for fold 7: 0.06246391807993253\n",
      "Validation Loss for fold 7: 0.05934815978010496\n",
      "Validation Loss for fold 7: 0.052682473013798393\n",
      "Validation Loss for fold 7: 0.06335209806760152\n",
      "Validation Loss for fold 7: 0.05868558585643768\n",
      "Validation Loss for fold 7: 0.05951440085967382\n",
      "Validation Loss for fold 7: 0.05693739031751951\n",
      "Validation Loss for fold 7: 0.06200222671031952\n",
      "Validation Loss for fold 7: 0.06017835810780525\n",
      "Validation Loss for fold 7: 0.0561695322394371\n",
      "Validation Loss for fold 7: 0.056800976395606995\n",
      "Validation Loss for fold 7: 0.05603127429882685\n",
      "Validation Loss for fold 7: 0.05504632617036501\n",
      "Validation Loss for fold 7: 0.05543601140379906\n",
      "Validation Loss for fold 7: 0.05695245290795962\n",
      "Validation Loss for fold 7: 0.06063417966167132\n",
      "Validation Loss for fold 7: 0.059472158551216125\n",
      "Validation Loss for fold 7: 0.058282202730576195\n",
      "Validation Loss for fold 7: 0.05683248986800512\n",
      "Validation Loss for fold 7: 0.061913141359885536\n",
      "Validation Loss for fold 7: 0.05907434597611427\n",
      "Validation Loss for fold 7: 0.059125433365503945\n",
      "Validation Loss for fold 7: 0.05410866936047872\n",
      "Validation Loss for fold 7: 0.05443411196271578\n",
      "Validation Loss for fold 7: 0.058028953770796456\n",
      "Validation Loss for fold 7: 0.05711315448085467\n",
      "Validation Loss for fold 7: 0.053450816621383034\n",
      "Validation Loss for fold 7: 0.05849570160110792\n",
      "Validation Loss for fold 7: 0.05710418646534284\n",
      "Validation Loss for fold 7: 0.05762983982761701\n",
      "Validation Loss for fold 7: 0.05871305118004481\n",
      "Validation Loss for fold 7: 0.05016083704928557\n",
      "Validation Loss for fold 7: 0.05387668932477633\n",
      "Validation Loss for fold 7: 0.05655636638402939\n",
      "Validation Loss for fold 7: 0.056437345842520394\n",
      "Validation Loss for fold 7: 0.06219413752357165\n",
      "Validation Loss for fold 7: 0.06015559285879135\n",
      "Validation Loss for fold 7: 0.05637670929233233\n",
      "Validation Loss for fold 7: 0.053883244593938194\n",
      "Validation Loss for fold 7: 0.059005593260129295\n",
      "Validation Loss for fold 7: 0.05245093380411466\n",
      "Validation Loss for fold 7: 0.051432772229115166\n",
      "Validation Loss for fold 7: 0.05512382586797079\n",
      "Validation Loss for fold 7: 0.05339872340361277\n",
      "Validation Loss for fold 7: 0.056257374584674835\n",
      "Validation Loss for fold 7: 0.06269779925545056\n",
      "Validation Loss for fold 7: 0.05434996634721756\n",
      "Validation Loss for fold 7: 0.06162951389948527\n",
      "Validation Loss for fold 7: 0.05196036770939827\n",
      "Validation Loss for fold 7: 0.058396175503730774\n",
      "Validation Loss for fold 7: 0.05136042584975561\n",
      "Validation Loss for fold 7: 0.052199289202690125\n",
      "Validation Loss for fold 7: 0.05127775793274244\n",
      "Validation Loss for fold 7: 0.05758442605535189\n",
      "Validation Loss for fold 7: 0.05457392459114393\n",
      "Validation Loss for fold 7: 0.05500641341010729\n",
      "Validation Loss for fold 7: 0.05218968912959099\n",
      "Validation Loss for fold 7: 0.05357277517517408\n",
      "Validation Loss for fold 7: 0.052977606654167175\n",
      "Validation Loss for fold 7: 0.060776290794213615\n",
      "Validation Loss for fold 7: 0.05674425388375918\n",
      "Validation Loss for fold 7: 0.0519485188027223\n",
      "Validation Loss for fold 7: 0.0541996123890082\n",
      "Validation Loss for fold 7: 0.05293460562825203\n",
      "Validation Loss for fold 7: 0.055123688032229744\n",
      "Validation Loss for fold 7: 0.051139313727617264\n",
      "Validation Loss for fold 7: 0.04875098789731661\n",
      "Validation Loss for fold 7: 0.05140817165374756\n",
      "Validation Loss for fold 7: 0.05611533671617508\n",
      "Validation Loss for fold 7: 0.0562377559641997\n",
      "Validation Loss for fold 7: 0.05003382513920466\n",
      "Validation Loss for fold 7: 0.05802618836363157\n",
      "Validation Loss for fold 7: 0.053723654399315514\n",
      "Validation Loss for fold 7: 0.05326647808154424\n",
      "Validation Loss for fold 7: 0.0518624372780323\n",
      "Validation Loss for fold 7: 0.054256138702233635\n",
      "Validation Loss for fold 7: 0.05460895970463753\n",
      "Validation Loss for fold 7: 0.05370258912444115\n",
      "Validation Loss for fold 7: 0.06000145524740219\n",
      "Validation Loss for fold 7: 0.054343290627002716\n",
      "Validation Loss for fold 7: 0.04906722903251648\n",
      "Validation Loss for fold 7: 0.05477563043435415\n",
      "Validation Loss for fold 7: 0.05414446691672007\n",
      "Validation Loss for fold 7: 0.05561847984790802\n",
      "Validation Loss for fold 7: 0.05296552429596583\n",
      "Validation Loss for fold 7: 0.05402084191640218\n",
      "Validation Loss for fold 7: 0.05072629824280739\n",
      "Validation Loss for fold 7: 0.05463282763957977\n",
      "Validation Loss for fold 7: 0.05046446373065313\n",
      "Validation Loss for fold 7: 0.057721915344397225\n",
      "Validation Loss for fold 7: 0.05416215335329374\n",
      "Validation Loss for fold 7: 0.056873925030231476\n",
      "Validation Loss for fold 7: 0.056349011758963265\n",
      "Validation Loss for fold 7: 0.05398424342274666\n",
      "Validation Loss for fold 7: 0.05336814001202583\n",
      "Validation Loss for fold 7: 0.050368876506884895\n",
      "Validation Loss for fold 7: 0.05209497238198916\n",
      "Validation Loss for fold 7: 0.04807573432723681\n",
      "Validation Loss for fold 7: 0.04964741691946983\n",
      "Validation Loss for fold 7: 0.05581407621502876\n",
      "Validation Loss for fold 7: 0.058005748937527336\n",
      "Validation Loss for fold 7: 0.04990621656179428\n",
      "Validation Loss for fold 7: 0.05687123785416285\n",
      "Validation Loss for fold 7: 0.0523564467827479\n",
      "Validation Loss for fold 7: 0.054354146122932434\n",
      "Validation Loss for fold 7: 0.0526608278354009\n",
      "Validation Loss for fold 7: 0.05534239734212557\n",
      "Validation Loss for fold 7: 0.058921828866004944\n",
      "Validation Loss for fold 7: 0.05358686794837316\n",
      "Validation Loss for fold 7: 0.05440061663587888\n",
      "Validation Loss for fold 7: 0.05215869223078092\n",
      "Validation Loss for fold 7: 0.055631037801504135\n",
      "Validation Loss for fold 7: 0.0487201102077961\n",
      "Validation Loss for fold 7: 0.04829001178344091\n",
      "Validation Loss for fold 7: 0.057924654334783554\n",
      "Validation Loss for fold 7: 0.05336294944087664\n",
      "Validation Loss for fold 7: 0.05044091989596685\n",
      "Validation Loss for fold 7: 0.04933886478344599\n",
      "Validation Loss for fold 7: 0.05582339564959208\n",
      "Validation Loss for fold 7: 0.05441926668087641\n",
      "Validation Loss for fold 7: 0.05523081496357918\n",
      "Validation Loss for fold 7: 0.05373208224773407\n",
      "Validation Loss for fold 7: 0.044695066288113594\n",
      "Validation Loss for fold 7: 0.04809156432747841\n",
      "Validation Loss for fold 7: 0.04989879454175631\n",
      "Validation Loss for fold 7: 0.052900644640127815\n",
      "Validation Loss for fold 7: 0.05310577650864919\n",
      "Validation Loss for fold 7: 0.05148539940516154\n",
      "Validation Loss for fold 7: 0.056243510295947395\n",
      "Validation Loss for fold 7: 0.05325463662544886\n",
      "Validation Loss for fold 7: 0.050443133960167565\n",
      "Validation Loss for fold 7: 0.04837968572974205\n",
      "Validation Loss for fold 7: 0.050483815371990204\n",
      "Validation Loss for fold 7: 0.05166908477743467\n",
      "Validation Loss for fold 7: 0.05212125927209854\n",
      "Validation Loss for fold 7: 0.05028073862195015\n",
      "Validation Loss for fold 7: 0.05144980922341347\n",
      "Validation Loss for fold 7: 0.047311960409084954\n",
      "Validation Loss for fold 7: 0.052429801474014916\n",
      "Validation Loss for fold 7: 0.053355185935894646\n",
      "Validation Loss for fold 7: 0.049989874164263405\n",
      "Validation Loss for fold 7: 0.05248586585124334\n",
      "Validation Loss for fold 7: 0.04901708414157232\n",
      "Validation Loss for fold 7: 0.04995208606123924\n",
      "Validation Loss for fold 7: 0.05372310181458791\n",
      "Validation Loss for fold 7: 0.05248783901333809\n",
      "Validation Loss for fold 7: 0.05373570819695791\n",
      "Validation Loss for fold 7: 0.0501813106238842\n",
      "Validation Loss for fold 7: 0.052391309291124344\n",
      "Validation Loss for fold 7: 0.052181227753559746\n",
      "Validation Loss for fold 7: 0.050181987384955086\n",
      "Validation Loss for fold 7: 0.05390853310624758\n",
      "Validation Loss for fold 7: 0.05048138896624247\n",
      "Validation Loss for fold 7: 0.047846910854180656\n",
      "Validation Loss for fold 7: 0.05401330689589182\n",
      "Validation Loss for fold 7: 0.04832272604107857\n",
      "Validation Loss for fold 7: 0.049787864089012146\n",
      "Validation Loss for fold 7: 0.05233785634239515\n",
      "Validation Loss for fold 7: 0.048682608952124916\n",
      "Validation Loss for fold 7: 0.04982438683509827\n",
      "Validation Loss for fold 7: 0.048695772886276245\n",
      "Validation Loss for fold 7: 0.0491756796836853\n",
      "Validation Loss for fold 7: 0.0522121029595534\n",
      "Validation Loss for fold 7: 0.055881526321172714\n",
      "Validation Loss for fold 7: 0.04976889491081238\n",
      "Validation Loss for fold 7: 0.054323192685842514\n",
      "Validation Loss for fold 7: 0.04692461217443148\n",
      "Validation Loss for fold 7: 0.0510189396639665\n",
      "Validation Loss for fold 7: 0.04761621728539467\n",
      "Validation Loss for fold 7: 0.050487201660871506\n",
      "Validation Loss for fold 7: 0.05041877552866936\n",
      "Validation Loss for fold 7: 0.04634595041473707\n",
      "Validation Loss for fold 7: 0.056789917250474296\n",
      "Validation Loss for fold 7: 0.04952238624294599\n",
      "Validation Loss for fold 7: 0.05103835463523865\n",
      "Validation Loss for fold 7: 0.045688292011618614\n",
      "Validation Loss for fold 7: 0.0548847441871961\n",
      "Validation Loss for fold 7: 0.04852483173211416\n",
      "Validation Loss for fold 7: 0.05101007968187332\n",
      "Validation Loss for fold 7: 0.05081211651364962\n",
      "Validation Loss for fold 7: 0.05413899446527163\n",
      "Validation Loss for fold 7: 0.05255426342288653\n",
      "Validation Loss for fold 7: 0.051032708336909614\n",
      "Validation Loss for fold 7: 0.048454608768224716\n",
      "Validation Loss for fold 7: 0.047464908411105476\n",
      "Validation Loss for fold 7: 0.04811195656657219\n",
      "Validation Loss for fold 7: 0.04826823249459267\n",
      "Validation Loss for fold 7: 0.04931984841823578\n",
      "Validation Loss for fold 7: 0.04604556908210119\n",
      "Validation Loss for fold 7: 0.04910304273168246\n",
      "Validation Loss for fold 7: 0.04836192106207212\n",
      "Validation Loss for fold 7: 0.04936888441443443\n",
      "Validation Loss for fold 7: 0.04517204314470291\n",
      "Validation Loss for fold 7: 0.04877941682934761\n",
      "Validation Loss for fold 7: 0.05107728640238444\n",
      "--------------------------------\n",
      "FOLD 8\n",
      "--------------------------------\n",
      "Validation Loss for fold 8: 0.35432496666908264\n",
      "Validation Loss for fold 8: 0.3005478084087372\n",
      "Validation Loss for fold 8: 0.2334259251753489\n",
      "Validation Loss for fold 8: 0.1890937089920044\n",
      "Validation Loss for fold 8: 0.18075764179229736\n",
      "Validation Loss for fold 8: 0.17516022423903146\n",
      "Validation Loss for fold 8: 0.15308422346909842\n",
      "Validation Loss for fold 8: 0.14187520990769067\n",
      "Validation Loss for fold 8: 0.14307264238595963\n",
      "Validation Loss for fold 8: 0.12906591594219208\n",
      "Validation Loss for fold 8: 0.12107477088769276\n",
      "Validation Loss for fold 8: 0.09968067457278569\n",
      "Validation Loss for fold 8: 0.10192634165287018\n",
      "Validation Loss for fold 8: 0.10618128130833308\n",
      "Validation Loss for fold 8: 0.10087214410305023\n",
      "Validation Loss for fold 8: 0.10545339435338974\n",
      "Validation Loss for fold 8: 0.09804845849672954\n",
      "Validation Loss for fold 8: 0.10103033234675725\n",
      "Validation Loss for fold 8: 0.1108301430940628\n",
      "Validation Loss for fold 8: 0.09357581287622452\n",
      "Validation Loss for fold 8: 0.09709219634532928\n",
      "Validation Loss for fold 8: 0.0962390477458636\n",
      "Validation Loss for fold 8: 0.10615475972493489\n",
      "Validation Loss for fold 8: 0.0970831960439682\n",
      "Validation Loss for fold 8: 0.09460240850845973\n",
      "Validation Loss for fold 8: 0.09377525001764297\n",
      "Validation Loss for fold 8: 0.08826238910357158\n",
      "Validation Loss for fold 8: 0.10349657386541367\n",
      "Validation Loss for fold 8: 0.08742200583219528\n",
      "Validation Loss for fold 8: 0.08606649190187454\n",
      "Validation Loss for fold 8: 0.08705407629410426\n",
      "Validation Loss for fold 8: 0.08983924736579259\n",
      "Validation Loss for fold 8: 0.08783098806937535\n",
      "Validation Loss for fold 8: 0.08156723529100418\n",
      "Validation Loss for fold 8: 0.08489988247553508\n",
      "Validation Loss for fold 8: 0.08871627102295558\n",
      "Validation Loss for fold 8: 0.08606404066085815\n",
      "Validation Loss for fold 8: 0.08290770649909973\n",
      "Validation Loss for fold 8: 0.08167168498039246\n",
      "Validation Loss for fold 8: 0.08493176847696304\n",
      "Validation Loss for fold 8: 0.08871540427207947\n",
      "Validation Loss for fold 8: 0.08021669338146846\n",
      "Validation Loss for fold 8: 0.0782683789730072\n",
      "Validation Loss for fold 8: 0.08628466725349426\n",
      "Validation Loss for fold 8: 0.08528063694636027\n",
      "Validation Loss for fold 8: 0.08451218654712041\n",
      "Validation Loss for fold 8: 0.08686905602614085\n",
      "Validation Loss for fold 8: 0.07723213483889897\n",
      "Validation Loss for fold 8: 0.08472522844870885\n",
      "Validation Loss for fold 8: 0.07943155119816463\n",
      "Validation Loss for fold 8: 0.07832114398479462\n",
      "Validation Loss for fold 8: 0.08220274746417999\n",
      "Validation Loss for fold 8: 0.07410900170604388\n",
      "Validation Loss for fold 8: 0.07455539455016454\n",
      "Validation Loss for fold 8: 0.08048479507366817\n",
      "Validation Loss for fold 8: 0.07086536784966786\n",
      "Validation Loss for fold 8: 0.07738531629244487\n",
      "Validation Loss for fold 8: 0.07948466142018636\n",
      "Validation Loss for fold 8: 0.07480089863141377\n",
      "Validation Loss for fold 8: 0.07617641737063725\n",
      "Validation Loss for fold 8: 0.07729101181030273\n",
      "Validation Loss for fold 8: 0.07294051597515742\n",
      "Validation Loss for fold 8: 0.07121809323628743\n",
      "Validation Loss for fold 8: 0.07204324379563332\n",
      "Validation Loss for fold 8: 0.0630337583522002\n",
      "Validation Loss for fold 8: 0.07257275531689326\n",
      "Validation Loss for fold 8: 0.07806443174680074\n",
      "Validation Loss for fold 8: 0.07749974230925243\n",
      "Validation Loss for fold 8: 0.070790596306324\n",
      "Validation Loss for fold 8: 0.06837120652198792\n",
      "Validation Loss for fold 8: 0.06853179881970088\n",
      "Validation Loss for fold 8: 0.0709339939057827\n",
      "Validation Loss for fold 8: 0.0689296027024587\n",
      "Validation Loss for fold 8: 0.063355952501297\n",
      "Validation Loss for fold 8: 0.06620065495371819\n",
      "Validation Loss for fold 8: 0.07063156863053639\n",
      "Validation Loss for fold 8: 0.07098423689603806\n",
      "Validation Loss for fold 8: 0.06662844245632489\n",
      "Validation Loss for fold 8: 0.06489373246828715\n",
      "Validation Loss for fold 8: 0.06894871840874355\n",
      "Validation Loss for fold 8: 0.07684835294882457\n",
      "Validation Loss for fold 8: 0.07059232890605927\n",
      "Validation Loss for fold 8: 0.0692150816321373\n",
      "Validation Loss for fold 8: 0.06274362156788509\n",
      "Validation Loss for fold 8: 0.06474525481462479\n",
      "Validation Loss for fold 8: 0.07204517722129822\n",
      "Validation Loss for fold 8: 0.06434476127227147\n",
      "Validation Loss for fold 8: 0.06146847208340963\n",
      "Validation Loss for fold 8: 0.0658116986354192\n",
      "Validation Loss for fold 8: 0.06539793312549591\n",
      "Validation Loss for fold 8: 0.06187263876199722\n",
      "Validation Loss for fold 8: 0.07033314431707065\n",
      "Validation Loss for fold 8: 0.06690136591593425\n",
      "Validation Loss for fold 8: 0.07602268705765407\n",
      "Validation Loss for fold 8: 0.06202159325281779\n",
      "Validation Loss for fold 8: 0.06626621137062709\n",
      "Validation Loss for fold 8: 0.06640280410647392\n",
      "Validation Loss for fold 8: 0.06700516864657402\n",
      "Validation Loss for fold 8: 0.06128511826197306\n",
      "Validation Loss for fold 8: 0.06595878427227338\n",
      "Validation Loss for fold 8: 0.06607841451962788\n",
      "Validation Loss for fold 8: 0.06876272211472194\n",
      "Validation Loss for fold 8: 0.06260744233926137\n",
      "Validation Loss for fold 8: 0.07030465205510457\n",
      "Validation Loss for fold 8: 0.06541155775388081\n",
      "Validation Loss for fold 8: 0.06213572372992834\n",
      "Validation Loss for fold 8: 0.061021129290262856\n",
      "Validation Loss for fold 8: 0.06775693347056706\n",
      "Validation Loss for fold 8: 0.06287760535875957\n",
      "Validation Loss for fold 8: 0.07200953861077626\n",
      "Validation Loss for fold 8: 0.066030186911424\n",
      "Validation Loss for fold 8: 0.06897664442658424\n",
      "Validation Loss for fold 8: 0.06720476845900218\n",
      "Validation Loss for fold 8: 0.06178789337476095\n",
      "Validation Loss for fold 8: 0.062338619182507195\n",
      "Validation Loss for fold 8: 0.06267949690421422\n",
      "Validation Loss for fold 8: 0.0640324130654335\n",
      "Validation Loss for fold 8: 0.0679260604083538\n",
      "Validation Loss for fold 8: 0.06600903595487277\n",
      "Validation Loss for fold 8: 0.06024496257305145\n",
      "Validation Loss for fold 8: 0.0617139699558417\n",
      "Validation Loss for fold 8: 0.06263949473698933\n",
      "Validation Loss for fold 8: 0.06339963028828303\n",
      "Validation Loss for fold 8: 0.061832270274559654\n",
      "Validation Loss for fold 8: 0.06281496708591779\n",
      "Validation Loss for fold 8: 0.06321934113899867\n",
      "Validation Loss for fold 8: 0.05938201521833738\n",
      "Validation Loss for fold 8: 0.05955727770924568\n",
      "Validation Loss for fold 8: 0.06080144643783569\n",
      "Validation Loss for fold 8: 0.05590887243549029\n",
      "Validation Loss for fold 8: 0.06673712655901909\n",
      "Validation Loss for fold 8: 0.06238142525156339\n",
      "Validation Loss for fold 8: 0.05743110179901123\n",
      "Validation Loss for fold 8: 0.05775142585237821\n",
      "Validation Loss for fold 8: 0.06392031659682591\n",
      "Validation Loss for fold 8: 0.06289700667063396\n",
      "Validation Loss for fold 8: 0.05967076743642489\n",
      "Validation Loss for fold 8: 0.06061491991082827\n",
      "Validation Loss for fold 8: 0.06088678787151972\n",
      "Validation Loss for fold 8: 0.06560890624920528\n",
      "Validation Loss for fold 8: 0.06728499506910642\n",
      "Validation Loss for fold 8: 0.06229372570912043\n",
      "Validation Loss for fold 8: 0.061293562253316246\n",
      "Validation Loss for fold 8: 0.06306016320983569\n",
      "Validation Loss for fold 8: 0.06503026187419891\n",
      "Validation Loss for fold 8: 0.05856318150957426\n",
      "Validation Loss for fold 8: 0.06096332396070162\n",
      "Validation Loss for fold 8: 0.06058640778064728\n",
      "Validation Loss for fold 8: 0.05505281314253807\n",
      "Validation Loss for fold 8: 0.06274878109494846\n",
      "Validation Loss for fold 8: 0.06025539959470431\n",
      "Validation Loss for fold 8: 0.05948580801486969\n",
      "Validation Loss for fold 8: 0.05560422564546267\n",
      "Validation Loss for fold 8: 0.05630273620287577\n",
      "Validation Loss for fold 8: 0.06152648106217384\n",
      "Validation Loss for fold 8: 0.06047051027417183\n",
      "Validation Loss for fold 8: 0.058288026601076126\n",
      "Validation Loss for fold 8: 0.0542155976096789\n",
      "Validation Loss for fold 8: 0.05972784385085106\n",
      "Validation Loss for fold 8: 0.05547988166411718\n",
      "Validation Loss for fold 8: 0.05741754422585169\n",
      "Validation Loss for fold 8: 0.06417858848969142\n",
      "Validation Loss for fold 8: 0.05536171173055967\n",
      "Validation Loss for fold 8: 0.05881833905975024\n",
      "Validation Loss for fold 8: 0.058979280292987823\n",
      "Validation Loss for fold 8: 0.055947678784529366\n",
      "Validation Loss for fold 8: 0.05829862256844839\n",
      "Validation Loss for fold 8: 0.05321308225393295\n",
      "Validation Loss for fold 8: 0.05663411815961202\n",
      "Validation Loss for fold 8: 0.054451623310645424\n",
      "Validation Loss for fold 8: 0.06302729621529579\n",
      "Validation Loss for fold 8: 0.06059106687704722\n",
      "Validation Loss for fold 8: 0.059702648470799126\n",
      "Validation Loss for fold 8: 0.05421761547525724\n",
      "Validation Loss for fold 8: 0.061927335957686104\n",
      "Validation Loss for fold 8: 0.056518107652664185\n",
      "Validation Loss for fold 8: 0.05749897907177607\n",
      "Validation Loss for fold 8: 0.05534758046269417\n",
      "Validation Loss for fold 8: 0.057031276325384773\n",
      "Validation Loss for fold 8: 0.05588176722327868\n",
      "Validation Loss for fold 8: 0.05426530043284098\n",
      "Validation Loss for fold 8: 0.05705191691716512\n",
      "Validation Loss for fold 8: 0.05528755113482475\n",
      "Validation Loss for fold 8: 0.0540319432814916\n",
      "Validation Loss for fold 8: 0.05994021147489548\n",
      "Validation Loss for fold 8: 0.06204977259039879\n",
      "Validation Loss for fold 8: 0.05893410990635554\n",
      "Validation Loss for fold 8: 0.055275668700536094\n",
      "Validation Loss for fold 8: 0.05563949172695478\n",
      "Validation Loss for fold 8: 0.057508339484532676\n",
      "Validation Loss for fold 8: 0.05976920326550802\n",
      "Validation Loss for fold 8: 0.05625752111275991\n",
      "Validation Loss for fold 8: 0.05893872554103533\n",
      "Validation Loss for fold 8: 0.059843435883522034\n",
      "Validation Loss for fold 8: 0.05615651234984398\n",
      "Validation Loss for fold 8: 0.058021711806456246\n",
      "Validation Loss for fold 8: 0.054934402306874595\n",
      "Validation Loss for fold 8: 0.057735116531451545\n",
      "Validation Loss for fold 8: 0.051356781274080276\n",
      "Validation Loss for fold 8: 0.05325393875439962\n",
      "Validation Loss for fold 8: 0.059769408156474434\n",
      "Validation Loss for fold 8: 0.05677500739693642\n",
      "Validation Loss for fold 8: 0.058229354520638786\n",
      "Validation Loss for fold 8: 0.05666442339619001\n",
      "Validation Loss for fold 8: 0.05947456260522207\n",
      "Validation Loss for fold 8: 0.057441744953393936\n",
      "Validation Loss for fold 8: 0.056700605899095535\n",
      "Validation Loss for fold 8: 0.05774230634172758\n",
      "Validation Loss for fold 8: 0.05241259808341662\n",
      "Validation Loss for fold 8: 0.054115451872348785\n",
      "Validation Loss for fold 8: 0.05568059782187144\n",
      "Validation Loss for fold 8: 0.054316457360982895\n",
      "Validation Loss for fold 8: 0.05739367504914602\n",
      "Validation Loss for fold 8: 0.05967418228586515\n",
      "Validation Loss for fold 8: 0.05753691246112188\n",
      "Validation Loss for fold 8: 0.057363361120224\n",
      "Validation Loss for fold 8: 0.05577624340852102\n",
      "Validation Loss for fold 8: 0.05145463471611341\n",
      "Validation Loss for fold 8: 0.052797142416238785\n",
      "Validation Loss for fold 8: 0.06331469615300496\n",
      "Validation Loss for fold 8: 0.05534419293204943\n",
      "Validation Loss for fold 8: 0.05434534947077433\n",
      "Validation Loss for fold 8: 0.06093655154109001\n",
      "Validation Loss for fold 8: 0.06379842261473338\n",
      "Validation Loss for fold 8: 0.05781015877922376\n",
      "Validation Loss for fold 8: 0.05481958016753197\n",
      "Validation Loss for fold 8: 0.05770628899335861\n",
      "Validation Loss for fold 8: 0.05411026875178019\n",
      "Validation Loss for fold 8: 0.05083182454109192\n",
      "Validation Loss for fold 8: 0.055614810436964035\n",
      "Validation Loss for fold 8: 0.057428911328315735\n",
      "Validation Loss for fold 8: 0.0539844793577989\n",
      "Validation Loss for fold 8: 0.052953362464904785\n",
      "Validation Loss for fold 8: 0.05280774459242821\n",
      "Validation Loss for fold 8: 0.05482612053553263\n",
      "Validation Loss for fold 8: 0.05579663813114166\n",
      "Validation Loss for fold 8: 0.05686930567026138\n",
      "Validation Loss for fold 8: 0.05236903329690298\n",
      "Validation Loss for fold 8: 0.053356790294249855\n",
      "Validation Loss for fold 8: 0.05404107893506686\n",
      "Validation Loss for fold 8: 0.053155013670523964\n",
      "Validation Loss for fold 8: 0.0591228703657786\n",
      "Validation Loss for fold 8: 0.0535630447169145\n",
      "Validation Loss for fold 8: 0.05784692366917928\n",
      "Validation Loss for fold 8: 0.0482931062579155\n",
      "Validation Loss for fold 8: 0.05278406168023745\n",
      "Validation Loss for fold 8: 0.05718207980195681\n",
      "Validation Loss for fold 8: 0.0585520106057326\n",
      "Validation Loss for fold 8: 0.055912911891937256\n",
      "Validation Loss for fold 8: 0.05168774724006653\n",
      "Validation Loss for fold 8: 0.04981339226166407\n",
      "Validation Loss for fold 8: 0.05982835839192072\n",
      "Validation Loss for fold 8: 0.05781711389621099\n",
      "Validation Loss for fold 8: 0.06392969439427058\n",
      "Validation Loss for fold 8: 0.053555277486642204\n",
      "Validation Loss for fold 8: 0.05287254353364309\n",
      "Validation Loss for fold 8: 0.05105587715903918\n",
      "Validation Loss for fold 8: 0.05414936815698942\n",
      "Validation Loss for fold 8: 0.0537040742735068\n",
      "Validation Loss for fold 8: 0.05154978111386299\n",
      "Validation Loss for fold 8: 0.053180038928985596\n",
      "Validation Loss for fold 8: 0.06019556522369385\n",
      "Validation Loss for fold 8: 0.05656251062949499\n",
      "Validation Loss for fold 8: 0.05689075216650963\n",
      "Validation Loss for fold 8: 0.056038605670134224\n",
      "Validation Loss for fold 8: 0.053235633919636406\n",
      "Validation Loss for fold 8: 0.05038724715511004\n",
      "Validation Loss for fold 8: 0.053792670369148254\n",
      "Validation Loss for fold 8: 0.048733167350292206\n",
      "Validation Loss for fold 8: 0.057473318030436836\n",
      "Validation Loss for fold 8: 0.0601582999030749\n",
      "Validation Loss for fold 8: 0.05597279220819473\n",
      "Validation Loss for fold 8: 0.06139679873983065\n",
      "Validation Loss for fold 8: 0.05307313551505407\n",
      "Validation Loss for fold 8: 0.05282551174362501\n",
      "Validation Loss for fold 8: 0.04968229060371717\n",
      "Validation Loss for fold 8: 0.0539555586874485\n",
      "Validation Loss for fold 8: 0.059690615783135094\n",
      "Validation Loss for fold 8: 0.057429615408182144\n",
      "Validation Loss for fold 8: 0.05918580790360769\n",
      "Validation Loss for fold 8: 0.05534562220176061\n",
      "Validation Loss for fold 8: 0.055168950309356056\n",
      "Validation Loss for fold 8: 0.05454396332303683\n",
      "Validation Loss for fold 8: 0.059226016203562416\n",
      "Validation Loss for fold 8: 0.05160628135005633\n",
      "Validation Loss for fold 8: 0.05026144037644068\n",
      "Validation Loss for fold 8: 0.05228337521354357\n",
      "Validation Loss for fold 8: 0.056832866122325264\n",
      "Validation Loss for fold 8: 0.05254250640670458\n",
      "Validation Loss for fold 8: 0.05434960499405861\n",
      "Validation Loss for fold 8: 0.05539227152864138\n",
      "Validation Loss for fold 8: 0.05617279062668482\n",
      "Validation Loss for fold 8: 0.05990053589145342\n",
      "Validation Loss for fold 8: 0.05944616595904032\n",
      "Validation Loss for fold 8: 0.05344318225979805\n",
      "Validation Loss for fold 8: 0.05606487890084585\n",
      "Validation Loss for fold 8: 0.05650006358822187\n",
      "Validation Loss for fold 8: 0.0582876019179821\n",
      "Validation Loss for fold 8: 0.05495743826031685\n",
      "Validation Loss for fold 8: 0.05694040283560753\n",
      "--------------------------------\n",
      "FOLD 9\n",
      "--------------------------------\n",
      "Validation Loss for fold 9: 0.154192715883255\n",
      "Validation Loss for fold 9: 0.1421502654751142\n",
      "Validation Loss for fold 9: 0.13336543242136636\n",
      "Validation Loss for fold 9: 0.1235181987285614\n",
      "Validation Loss for fold 9: 0.1063581258058548\n",
      "Validation Loss for fold 9: 0.10475696126619975\n",
      "Validation Loss for fold 9: 0.11620597789684932\n",
      "Validation Loss for fold 9: 0.10450195769468944\n",
      "Validation Loss for fold 9: 0.10428905735413234\n",
      "Validation Loss for fold 9: 0.10541402051846187\n",
      "Validation Loss for fold 9: 0.10721499472856522\n",
      "Validation Loss for fold 9: 0.09843963384628296\n",
      "Validation Loss for fold 9: 0.10726571828126907\n",
      "Validation Loss for fold 9: 0.10683665424585342\n",
      "Validation Loss for fold 9: 0.10375302284955978\n",
      "Validation Loss for fold 9: 0.09091921150684357\n",
      "Validation Loss for fold 9: 0.09462899218002956\n",
      "Validation Loss for fold 9: 0.09560755888621013\n",
      "Validation Loss for fold 9: 0.10107164333264033\n",
      "Validation Loss for fold 9: 0.09338992337385814\n",
      "Validation Loss for fold 9: 0.08967642982800801\n",
      "Validation Loss for fold 9: 0.09193133066097896\n",
      "Validation Loss for fold 9: 0.08994641651709874\n",
      "Validation Loss for fold 9: 0.09217699120442073\n",
      "Validation Loss for fold 9: 0.09320462743441264\n",
      "Validation Loss for fold 9: 0.0859023854136467\n",
      "Validation Loss for fold 9: 0.08516383295257886\n",
      "Validation Loss for fold 9: 0.08947183440128963\n",
      "Validation Loss for fold 9: 0.08731069415807724\n",
      "Validation Loss for fold 9: 0.08957696706056595\n",
      "Validation Loss for fold 9: 0.08774383614460628\n",
      "Validation Loss for fold 9: 0.09139267851909001\n",
      "Validation Loss for fold 9: 0.09457420061031978\n",
      "Validation Loss for fold 9: 0.09261423101027806\n",
      "Validation Loss for fold 9: 0.08476561804612477\n",
      "Validation Loss for fold 9: 0.0893117015560468\n",
      "Validation Loss for fold 9: 0.09389503051837285\n",
      "Validation Loss for fold 9: 0.09119658917188644\n",
      "Validation Loss for fold 9: 0.09350492308537166\n",
      "Validation Loss for fold 9: 0.0866966222723325\n",
      "Validation Loss for fold 9: 0.09519702444473903\n",
      "Validation Loss for fold 9: 0.08897243440151215\n",
      "Validation Loss for fold 9: 0.08677914241949718\n",
      "Validation Loss for fold 9: 0.08138735095659892\n",
      "Validation Loss for fold 9: 0.08290660132964452\n",
      "Validation Loss for fold 9: 0.09231024980545044\n",
      "Validation Loss for fold 9: 0.08731057743231456\n",
      "Validation Loss for fold 9: 0.07728078216314316\n",
      "Validation Loss for fold 9: 0.0898238296310107\n",
      "Validation Loss for fold 9: 0.08107887953519821\n",
      "Validation Loss for fold 9: 0.08297829826672871\n",
      "Validation Loss for fold 9: 0.08662332097689311\n",
      "Validation Loss for fold 9: 0.08004305760065715\n",
      "Validation Loss for fold 9: 0.08288975059986115\n",
      "Validation Loss for fold 9: 0.08668157209952672\n",
      "Validation Loss for fold 9: 0.08385655283927917\n",
      "Validation Loss for fold 9: 0.0983273983001709\n",
      "Validation Loss for fold 9: 0.08418806145588557\n",
      "Validation Loss for fold 9: 0.07932973653078079\n",
      "Validation Loss for fold 9: 0.07733028382062912\n",
      "Validation Loss for fold 9: 0.08510707567135493\n",
      "Validation Loss for fold 9: 0.07777948801716168\n",
      "Validation Loss for fold 9: 0.08712827414274216\n",
      "Validation Loss for fold 9: 0.07530224074920018\n",
      "Validation Loss for fold 9: 0.0806830922762553\n",
      "Validation Loss for fold 9: 0.08242262899875641\n",
      "Validation Loss for fold 9: 0.08718060205380122\n",
      "Validation Loss for fold 9: 0.07892519980669022\n",
      "Validation Loss for fold 9: 0.08315938959519069\n",
      "Validation Loss for fold 9: 0.08498762299617131\n",
      "Validation Loss for fold 9: 0.0824839174747467\n",
      "Validation Loss for fold 9: 0.08708710968494415\n",
      "Validation Loss for fold 9: 0.07850106060504913\n",
      "Validation Loss for fold 9: 0.07802931219339371\n",
      "Validation Loss for fold 9: 0.07504893342653911\n",
      "Validation Loss for fold 9: 0.06890742604931195\n",
      "Validation Loss for fold 9: 0.07907122125228246\n",
      "Validation Loss for fold 9: 0.08750602354605992\n",
      "Validation Loss for fold 9: 0.06886750956376393\n",
      "Validation Loss for fold 9: 0.07394327595829964\n",
      "Validation Loss for fold 9: 0.07848372062047322\n",
      "Validation Loss for fold 9: 0.07722719013690948\n",
      "Validation Loss for fold 9: 0.07484028240044911\n",
      "Validation Loss for fold 9: 0.06841354941328366\n",
      "Validation Loss for fold 9: 0.08170866966247559\n",
      "Validation Loss for fold 9: 0.07967814058065414\n",
      "Validation Loss for fold 9: 0.07201486453413963\n",
      "Validation Loss for fold 9: 0.07851756115754445\n",
      "Validation Loss for fold 9: 0.07056550184885661\n",
      "Validation Loss for fold 9: 0.06796898817022641\n",
      "Validation Loss for fold 9: 0.07410319894552231\n",
      "Validation Loss for fold 9: 0.0806848481297493\n",
      "Validation Loss for fold 9: 0.07249398653705914\n",
      "Validation Loss for fold 9: 0.07665422310431798\n",
      "Validation Loss for fold 9: 0.07640627523263295\n",
      "Validation Loss for fold 9: 0.0750136598944664\n",
      "Validation Loss for fold 9: 0.07219962279001872\n",
      "Validation Loss for fold 9: 0.07741229732831319\n",
      "Validation Loss for fold 9: 0.07327877481778462\n",
      "Validation Loss for fold 9: 0.07546703269084294\n",
      "Validation Loss for fold 9: 0.0716104085246722\n",
      "Validation Loss for fold 9: 0.07266088078419368\n",
      "Validation Loss for fold 9: 0.07075659434000652\n",
      "Validation Loss for fold 9: 0.07387632379929225\n",
      "Validation Loss for fold 9: 0.0714941422144572\n",
      "Validation Loss for fold 9: 0.07047139108181\n",
      "Validation Loss for fold 9: 0.0692504992087682\n",
      "Validation Loss for fold 9: 0.0721537359058857\n",
      "Validation Loss for fold 9: 0.07089853783448537\n",
      "Validation Loss for fold 9: 0.07094548890988032\n",
      "Validation Loss for fold 9: 0.0718396430214246\n",
      "Validation Loss for fold 9: 0.0676649237672488\n",
      "Validation Loss for fold 9: 0.06706976145505905\n",
      "Validation Loss for fold 9: 0.06431517004966736\n",
      "Validation Loss for fold 9: 0.07295599083105724\n",
      "Validation Loss for fold 9: 0.06991804142793019\n",
      "Validation Loss for fold 9: 0.06294615566730499\n",
      "Validation Loss for fold 9: 0.07033593580126762\n",
      "Validation Loss for fold 9: 0.06798248117168744\n",
      "Validation Loss for fold 9: 0.07731662069757779\n",
      "Validation Loss for fold 9: 0.07357869297266006\n",
      "Validation Loss for fold 9: 0.06725525980194409\n",
      "Validation Loss for fold 9: 0.06832909335692723\n",
      "Validation Loss for fold 9: 0.07737879951794942\n",
      "Validation Loss for fold 9: 0.07504881297548611\n",
      "Validation Loss for fold 9: 0.07049716512362163\n",
      "Validation Loss for fold 9: 0.06197996437549591\n",
      "Validation Loss for fold 9: 0.0652373805642128\n",
      "Validation Loss for fold 9: 0.06871415426333745\n",
      "Validation Loss for fold 9: 0.06817580635348956\n",
      "Validation Loss for fold 9: 0.06660110379258792\n",
      "Validation Loss for fold 9: 0.07364685709277789\n",
      "Validation Loss for fold 9: 0.06406014536817868\n",
      "Validation Loss for fold 9: 0.06978048384189606\n",
      "Validation Loss for fold 9: 0.06456862638394038\n",
      "Validation Loss for fold 9: 0.07718064015110333\n",
      "Validation Loss for fold 9: 0.07417598366737366\n",
      "Validation Loss for fold 9: 0.0677751066784064\n",
      "Validation Loss for fold 9: 0.06974317257603009\n",
      "Validation Loss for fold 9: 0.07042647525668144\n",
      "Validation Loss for fold 9: 0.07204840332269669\n",
      "Validation Loss for fold 9: 0.07035317520300548\n",
      "Validation Loss for fold 9: 0.07121500248710315\n",
      "Validation Loss for fold 9: 0.06909022231896718\n",
      "Validation Loss for fold 9: 0.06684597084919612\n",
      "Validation Loss for fold 9: 0.07487989962100983\n",
      "Validation Loss for fold 9: 0.06850311408440272\n",
      "Validation Loss for fold 9: 0.06626579413811366\n",
      "Validation Loss for fold 9: 0.06875815242528915\n",
      "Validation Loss for fold 9: 0.07222713902592659\n",
      "Validation Loss for fold 9: 0.07171079516410828\n",
      "Validation Loss for fold 9: 0.061649554719527565\n",
      "Validation Loss for fold 9: 0.06825934226314227\n",
      "Validation Loss for fold 9: 0.06306181599696477\n",
      "Validation Loss for fold 9: 0.06517460073033969\n",
      "Validation Loss for fold 9: 0.0632109505434831\n",
      "Validation Loss for fold 9: 0.07034899791081746\n",
      "Validation Loss for fold 9: 0.06369508430361748\n",
      "Validation Loss for fold 9: 0.0661197304725647\n",
      "Validation Loss for fold 9: 0.06691424051920573\n",
      "Validation Loss for fold 9: 0.06316739941636722\n",
      "Validation Loss for fold 9: 0.06507297356923421\n",
      "Validation Loss for fold 9: 0.0640718253950278\n",
      "Validation Loss for fold 9: 0.06908931583166122\n",
      "Validation Loss for fold 9: 0.06343139832218488\n",
      "Validation Loss for fold 9: 0.06941978385051091\n",
      "Validation Loss for fold 9: 0.07009285191694896\n",
      "Validation Loss for fold 9: 0.07484010979533195\n",
      "Validation Loss for fold 9: 0.06111622229218483\n",
      "Validation Loss for fold 9: 0.06977221245567004\n",
      "Validation Loss for fold 9: 0.0686515395840009\n",
      "Validation Loss for fold 9: 0.0625260757903258\n",
      "Validation Loss for fold 9: 0.06776195267836253\n",
      "Validation Loss for fold 9: 0.06921499719222386\n",
      "Validation Loss for fold 9: 0.06332461535930634\n",
      "Validation Loss for fold 9: 0.06916312625010808\n",
      "Validation Loss for fold 9: 0.0643250159919262\n",
      "Validation Loss for fold 9: 0.0719397949675719\n",
      "Validation Loss for fold 9: 0.06564209113518397\n",
      "Validation Loss for fold 9: 0.06479995201031367\n",
      "Validation Loss for fold 9: 0.06601971512039502\n",
      "Validation Loss for fold 9: 0.06963373596469562\n",
      "Validation Loss for fold 9: 0.06499600410461426\n",
      "Validation Loss for fold 9: 0.07141285389661789\n",
      "Validation Loss for fold 9: 0.06503205746412277\n",
      "Validation Loss for fold 9: 0.061638771245876946\n",
      "Validation Loss for fold 9: 0.0691498468319575\n",
      "Validation Loss for fold 9: 0.058520992596944175\n",
      "Validation Loss for fold 9: 0.06616118798653285\n",
      "Validation Loss for fold 9: 0.06385632480184238\n",
      "Validation Loss for fold 9: 0.0673562337954839\n",
      "Validation Loss for fold 9: 0.05933858578403791\n",
      "Validation Loss for fold 9: 0.06883622581760089\n",
      "Validation Loss for fold 9: 0.07029044379790624\n",
      "Validation Loss for fold 9: 0.06410693128903706\n",
      "Validation Loss for fold 9: 0.06760519246260326\n",
      "Validation Loss for fold 9: 0.06360264619191487\n",
      "Validation Loss for fold 9: 0.06385631362597148\n",
      "Validation Loss for fold 9: 0.06317768866817157\n",
      "Validation Loss for fold 9: 0.059862248599529266\n",
      "Validation Loss for fold 9: 0.06717937563856442\n",
      "Validation Loss for fold 9: 0.06234548365076383\n",
      "Validation Loss for fold 9: 0.062149246533711754\n",
      "Validation Loss for fold 9: 0.06609343489011128\n",
      "Validation Loss for fold 9: 0.06289665897687276\n",
      "Validation Loss for fold 9: 0.06330249831080437\n",
      "Validation Loss for fold 9: 0.06455062453945477\n",
      "Validation Loss for fold 9: 0.06721921389301617\n",
      "Validation Loss for fold 9: 0.05895350128412247\n",
      "Validation Loss for fold 9: 0.06308756892879804\n",
      "Validation Loss for fold 9: 0.06316592047611873\n",
      "Validation Loss for fold 9: 0.06390402962764104\n",
      "Validation Loss for fold 9: 0.05950728182991346\n",
      "Validation Loss for fold 9: 0.06420012935996056\n",
      "Validation Loss for fold 9: 0.06575142095486324\n",
      "Validation Loss for fold 9: 0.0621882900595665\n",
      "Validation Loss for fold 9: 0.06126581753293673\n",
      "Validation Loss for fold 9: 0.061879719297091164\n",
      "Validation Loss for fold 9: 0.0636994019150734\n",
      "Validation Loss for fold 9: 0.061011914163827896\n",
      "Validation Loss for fold 9: 0.05689003566900889\n",
      "Validation Loss for fold 9: 0.06285107632478078\n",
      "Validation Loss for fold 9: 0.07006108264128368\n",
      "Validation Loss for fold 9: 0.057426308592160545\n",
      "Validation Loss for fold 9: 0.06416322787602742\n",
      "Validation Loss for fold 9: 0.06334813684225082\n",
      "Validation Loss for fold 9: 0.06583196918169658\n",
      "Validation Loss for fold 9: 0.06666212156414986\n",
      "Validation Loss for fold 9: 0.05939604714512825\n",
      "Validation Loss for fold 9: 0.06389657407999039\n",
      "Validation Loss for fold 9: 0.05714405203859011\n",
      "Validation Loss for fold 9: 0.06132359802722931\n",
      "Validation Loss for fold 9: 0.05878931159774462\n",
      "Validation Loss for fold 9: 0.06771384924650192\n",
      "Validation Loss for fold 9: 0.060557773957649864\n",
      "Validation Loss for fold 9: 0.061070699244737625\n",
      "Validation Loss for fold 9: 0.06113471960028013\n",
      "Validation Loss for fold 9: 0.057833376030127205\n",
      "Validation Loss for fold 9: 0.06662306686242421\n",
      "Validation Loss for fold 9: 0.06567647183934848\n",
      "Validation Loss for fold 9: 0.061963338404893875\n",
      "Validation Loss for fold 9: 0.06551005442937215\n",
      "Validation Loss for fold 9: 0.06386994197964668\n",
      "Validation Loss for fold 9: 0.0593686414261659\n",
      "Validation Loss for fold 9: 0.06459195787707965\n",
      "Validation Loss for fold 9: 0.06307010849316914\n",
      "Validation Loss for fold 9: 0.06265719483296077\n",
      "Validation Loss for fold 9: 0.06186140328645706\n",
      "Validation Loss for fold 9: 0.06875495115915935\n",
      "Validation Loss for fold 9: 0.06556881094972293\n",
      "Validation Loss for fold 9: 0.06103453288475672\n",
      "Validation Loss for fold 9: 0.06207543363173803\n",
      "Validation Loss for fold 9: 0.06521564225355785\n",
      "Validation Loss for fold 9: 0.0684459147353967\n",
      "Validation Loss for fold 9: 0.061910523722569145\n",
      "Validation Loss for fold 9: 0.057874796291192375\n",
      "Validation Loss for fold 9: 0.060603378961483635\n",
      "Validation Loss for fold 9: 0.0717506210009257\n",
      "Validation Loss for fold 9: 0.06556221842765808\n",
      "Validation Loss for fold 9: 0.06265697628259659\n",
      "Validation Loss for fold 9: 0.05747248108188311\n",
      "Validation Loss for fold 9: 0.0609859103957812\n",
      "Validation Loss for fold 9: 0.06247234965364138\n",
      "Validation Loss for fold 9: 0.06086697926123937\n",
      "Validation Loss for fold 9: 0.06451919302344322\n",
      "Validation Loss for fold 9: 0.06246949980656306\n",
      "Validation Loss for fold 9: 0.06709862872958183\n",
      "Validation Loss for fold 9: 0.06509664903084438\n",
      "Validation Loss for fold 9: 0.06919469560186069\n",
      "Validation Loss for fold 9: 0.06101024275024732\n",
      "Validation Loss for fold 9: 0.06540440022945404\n",
      "Validation Loss for fold 9: 0.0633653961122036\n",
      "Validation Loss for fold 9: 0.06297768279910088\n",
      "Validation Loss for fold 9: 0.06105666980147362\n",
      "Validation Loss for fold 9: 0.06685945143302281\n",
      "Validation Loss for fold 9: 0.05958725387851397\n",
      "Validation Loss for fold 9: 0.06154407188296318\n",
      "Validation Loss for fold 9: 0.06200860689083735\n",
      "Validation Loss for fold 9: 0.062459068993727364\n",
      "Validation Loss for fold 9: 0.05978977556029955\n",
      "Validation Loss for fold 9: 0.056258336951335274\n",
      "Validation Loss for fold 9: 0.060572415590286255\n",
      "Validation Loss for fold 9: 0.06102599576115608\n",
      "Validation Loss for fold 9: 0.0647289405266444\n",
      "Validation Loss for fold 9: 0.06149329369266828\n",
      "Validation Loss for fold 9: 0.06971072529753049\n",
      "Validation Loss for fold 9: 0.060699354857206345\n",
      "Validation Loss for fold 9: 0.061447313676277794\n",
      "Validation Loss for fold 9: 0.05766943842172623\n",
      "Validation Loss for fold 9: 0.059091098606586456\n",
      "Validation Loss for fold 9: 0.06765526160597801\n",
      "Validation Loss for fold 9: 0.05667625864346822\n",
      "Validation Loss for fold 9: 0.06405361617604892\n",
      "Validation Loss for fold 9: 0.062331028282642365\n",
      "Validation Loss for fold 9: 0.057734500616788864\n",
      "Validation Loss for fold 9: 0.06453050673007965\n",
      "Validation Loss for fold 9: 0.060062142709891\n",
      "Validation Loss for fold 9: 0.059038358430067696\n",
      "Validation Loss for fold 9: 0.061010073870420456\n",
      "Validation Loss for fold 9: 0.06388923774162929\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Avg Validation Loss</td><td>▆▃▂▂▄▂▁▁▇▂▂▂▇▃▂▁▄▂▂▁█▅▃▂▅▄▂▁▃▁▁▁▅▃▂▁▅▃▂▂</td></tr><tr><td>Avg. Training Loss</td><td>▃▂▁▁▄▂▁▁█▁▁▁▄▂▁▁▅▂▁▁▃▂▂▁▃▂▁▁▂▁▁▁▃▂▁▁▅▁▁▁</td></tr><tr><td>Epoch</td><td>▂▃▅▇▁▃▅▇▁▄▅▇▂▃▆▇▁▄▅▇▂▃▅█▁▃▆▇▂▄▅█▂▃▆▇▁▄▅█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Avg Validation Loss</td><td>0.06389</td></tr><tr><td>Avg. Training Loss</td><td>0.03575</td></tr><tr><td>Epoch</td><td>299</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">happy-armadillo-44</strong> at: <a href='https://wandb.ai/jcrich/collaborative%20filter%20model/runs/ow39d2po' target=\"_blank\">https://wandb.ai/jcrich/collaborative%20filter%20model/runs/ow39d2po</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240312_162719-ow39d2po/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Correct variable names and logic\n",
    "# Set the best validation loss to infinity at the start\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for fold, (train_ids, val_ids) in enumerate(kfold.split(train_ds)):\n",
    "    print(f'FOLD {fold}')\n",
    "    print('--------------------------------')\n",
    "\n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "    val_subsampler = torch.utils.data.SubsetRandomSampler(val_ids)\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=train_subsampler)\n",
    "    valloader = torch.utils.data.DataLoader(validation_ds, batch_size=BATCH_SIZE, sampler=val_subsampler)\n",
    "\n",
    "    model = MLPCollaborativeFilter(num_users + 1, num_movies + 1, embedding_dim=FEATURES)\n",
    "    optimiser = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE,weight_decay=L2_REGULARIZATION)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for user_indices, item_indices, ratings in trainloader:\n",
    "            optimiser.zero_grad()\n",
    "            outputs = model(user_indices, item_indices).squeeze()\n",
    "            loss = criterion(outputs, ratings)\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Average training loss over all batches\n",
    "        train_loss /= len(trainloader)\n",
    "        # Log training loss for the current epoch\n",
    "        wandb.log({\"Avg. Training Loss\": train_loss, \"Epoch\": epoch})\n",
    "        mlflow.log_metric(\"Avg. Training Loss\", train_loss, step=epoch)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for user_indices, item_indices, ratings in valloader:\n",
    "                outputs = model(user_indices, item_indices).squeeze()\n",
    "                loss = criterion(outputs, ratings)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        avg_validation_loss = val_loss / len(valloader)\n",
    "        print(f'Validation Loss for fold {fold}: {avg_validation_loss}')\n",
    "        \n",
    "        if avg_validation_loss < best_val_loss:\n",
    "            best_val_loss = avg_validation_loss\n",
    "            mlflow.log_metric(\"Best Validation Loss\", best_val_loss, step=epoch)\n",
    "            mlflow.pytorch.log_model(model, \"model\")\n",
    "            # Save model state\n",
    "            torch.save(model.state_dict(), 'best_model_state.pth')\n",
    "            # If you also want to save the optimizer state along with the model:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimiser_state_dict': optimiser.state_dict(),\n",
    "                'loss': avg_validation_loss,\n",
    "            }, 'best_col_model_checkpoint.pth')\n",
    "            # Log the model checkpoint as an artifact\n",
    "            mlflow.log_artifact('best_col_model_checkpoint.pth')\n",
    "        \n",
    "        # Log validation loss for the current epoch\n",
    "        wandb.log({\"Avg Validation Loss\": avg_validation_loss, \"Epoch\": epoch})\n",
    "        mlflow.log_metric(\"Avg Validation Loss\", avg_validation_loss, step=epoch)\n",
    "    \n",
    "    print('--------------------------------')\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9ede2c71-6995-4fd5-a657-e15996945cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06178586923500948\n"
     ]
    }
   ],
   "source": [
    "#test data\n",
    "model.eval()  # Switch to evaluation mode\n",
    "test_loss = 0\n",
    "correct_predictions = 0  # Fixed variable name for clarity\n",
    "total_contexts = 0\n",
    "\n",
    "with torch.no_grad():  # Disable gradient computation\n",
    "    for user_ids, movie_ids, ratings in test_data_loader:\n",
    "        predictions = model(user_ids,movie_ids)  # Generate predictions\n",
    "        # Calculate and accumulate loss\n",
    "        loss = criterion(predictions, ratings)\n",
    "        test_loss += loss.item()\n",
    "        \n",
    "        # # Reshape predictions to match [batch_size, context_size, vocab_size]\n",
    "        # predictions = predictions.view(-1, context_size, VOCAB_SIZE)\n",
    "        \n",
    "        # # Get top prediction for each context position\n",
    "        # top_predictions = predictions.argmax(dim=2)\n",
    "        \n",
    "        # # Calculate correct predictions\n",
    "        # correct_preds = (top_predictions == context).float().sum()\n",
    "        # correct_predictions += correct_preds.item()  # Accumulate correct predictions\n",
    "        \n",
    "        # total_contexts += context.numel()  # Total number of context word positions evaluated\n",
    "\n",
    "# Calculate final metrics\n",
    "test_loss /= len(test_data_loader)  # Average loss per batch\n",
    "# print('correct predictions = ',correct_predictions)\n",
    "# print('out of  = ',total_contexts)\n",
    "# accuracy = correct_predictions / total_contexts  # Compute accuracy\n",
    "\n",
    "# print(f'Test set: Average loss: {test_loss:.4f}, Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Evaluate the model\n",
    "# (This would involve using a separate validation set or performing cross-validation)\n",
    "print(test_loss)\n",
    "\n",
    "mlflow.log_metric('Post training test loss',test_loss)\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff24efc-8629-4242-8cce-6311cb1f9192",
   "metadata": {},
   "source": [
    "# Batch Training - on all trainable data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "08519bcf-2bf1-4d39-9f42-8b68b14135ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18391"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6dc2a29b-3b26-47ac-a881-2cf4ef13b777",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ds = ColFDataset(data,encoder)\n",
    "all_data_loader = DataLoader(train_ds,batch_size=BATCH_SIZE,shuffle=False,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0d1d344e-f48f-4ee2-ab94-b470f2159b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model =  MLPCollaborativeFilter(num_users + 1, num_movies + 1, embedding_dim=FEATURES)\n",
    "optimiser = optim.SGD(final_model.parameters(), lr=LEARNING_RATE,weight_decay=L2_REGULARIZATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "31c8ce29-174d-4687-9476-a996d5a9ccfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optional: Set MLflow experiment\n",
    "mlflow.set_experiment(\"Finetuned Collaborative Filter\")\n",
    "# Log model parameters (example)\n",
    "mlflow.start_run()\n",
    "mlflow.log_param(\"epochs\", EPOCHS)\n",
    "mlflow.log_param(\"optimizer\", type(optimiser).__name__)\n",
    "mlflow.log_param(\"learning rate\", LEARNING_RATE)\n",
    "mlflow.log_param(\"batch size\", BATCH_SIZE)\n",
    "mlflow.log_param(\"L2 regularization\", L2_REGULARIZATION)\n",
    "mlflow.log_param(\"KFOLDS\", num_folds)\n",
    "mlflow.log_param(\"drop out\", 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b8e36fba-65b3-40c4-ad25-0e6b7b004449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.16.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/james/Desktop/torchex/movie-users/wandb/run-20240312_163231-6ve6fb46</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jcrich/collaborative%20filter%20model/runs/6ve6fb46' target=\"_blank\">eager-flower-45</a></strong> to <a href='https://wandb.ai/jcrich/collaborative%20filter%20model' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jcrich/collaborative%20filter%20model' target=\"_blank\">https://wandb.ai/jcrich/collaborative%20filter%20model</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jcrich/collaborative%20filter%20model/runs/6ve6fb46' target=\"_blank\">https://wandb.ai/jcrich/collaborative%20filter%20model/runs/6ve6fb46</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/jcrich/collaborative%20filter%20model/runs/6ve6fb46?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f38c037c550>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    entity=\"jcrich\",\n",
    "    project=\"collaborative filter model\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"architecture\": \"collaborative filter\",\n",
    "    \"dataset\": \"letterboxd\",\n",
    "    \"epochs\": EPOCHS,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e1b550-c0e4-41e8-b28a-e56c79d42e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|█████████████████████████████████| 22/22 [00:00<00:00, 236.72it/s]\n",
      "Epoch 1: 100%|█████████████████████████████████| 22/22 [00:00<00:00, 275.40it/s]\n",
      "Epoch 2: 100%|█████████████████████████████████| 22/22 [00:00<00:00, 291.27it/s]\n",
      "Epoch 3: 100%|█████████████████████████████████| 22/22 [00:00<00:00, 289.25it/s]\n",
      "Epoch 4: 100%|█████████████████████████████████| 22/22 [00:00<00:00, 311.86it/s]\n",
      "Epoch 5: 100%|█████████████████████████████████| 22/22 [00:00<00:00, 309.81it/s]\n",
      "Epoch 6: 100%|█████████████████████████████████| 22/22 [00:00<00:00, 295.05it/s]\n",
      "Epoch 7: 100%|█████████████████████████████████| 22/22 [00:00<00:00, 253.15it/s]\n",
      "Epoch 8: 100%|█████████████████████████████████| 22/22 [00:00<00:00, 187.24it/s]\n",
      "Epoch 9: 100%|█████████████████████████████████| 22/22 [00:00<00:00, 261.31it/s]\n",
      "Epoch 10: 100%|████████████████████████████████| 22/22 [00:00<00:00, 262.06it/s]\n",
      "Epoch 11: 100%|████████████████████████████████| 22/22 [00:00<00:00, 247.40it/s]\n",
      "Epoch 12: 100%|████████████████████████████████| 22/22 [00:00<00:00, 247.76it/s]\n",
      "Epoch 13: 100%|████████████████████████████████| 22/22 [00:00<00:00, 272.41it/s]\n",
      "Epoch 14: 100%|████████████████████████████████| 22/22 [00:00<00:00, 267.41it/s]\n",
      "Epoch 15: 100%|████████████████████████████████| 22/22 [00:00<00:00, 273.99it/s]\n",
      "Epoch 16: 100%|████████████████████████████████| 22/22 [00:00<00:00, 265.35it/s]\n",
      "Epoch 17: 100%|████████████████████████████████| 22/22 [00:00<00:00, 255.16it/s]\n",
      "Epoch 18: 100%|████████████████████████████████| 22/22 [00:00<00:00, 235.93it/s]\n",
      "Epoch 19: 100%|████████████████████████████████| 22/22 [00:00<00:00, 235.31it/s]\n",
      "Epoch 20: 100%|████████████████████████████████| 22/22 [00:00<00:00, 258.81it/s]\n",
      "Epoch 21: 100%|████████████████████████████████| 22/22 [00:00<00:00, 236.88it/s]\n",
      "Epoch 22: 100%|████████████████████████████████| 22/22 [00:00<00:00, 241.58it/s]\n",
      "Epoch 23: 100%|████████████████████████████████| 22/22 [00:00<00:00, 256.85it/s]\n",
      "Epoch 24: 100%|████████████████████████████████| 22/22 [00:00<00:00, 226.89it/s]\n",
      "Epoch 25: 100%|████████████████████████████████| 22/22 [00:00<00:00, 235.60it/s]\n",
      "Epoch 26: 100%|████████████████████████████████| 22/22 [00:00<00:00, 250.32it/s]\n",
      "Epoch 27: 100%|████████████████████████████████| 22/22 [00:00<00:00, 244.63it/s]\n",
      "Epoch 28: 100%|████████████████████████████████| 22/22 [00:00<00:00, 243.58it/s]\n",
      "Epoch 29: 100%|████████████████████████████████| 22/22 [00:00<00:00, 239.60it/s]\n",
      "Epoch 30: 100%|████████████████████████████████| 22/22 [00:00<00:00, 226.74it/s]\n",
      "Epoch 31: 100%|████████████████████████████████| 22/22 [00:00<00:00, 227.82it/s]\n",
      "Epoch 32: 100%|████████████████████████████████| 22/22 [00:00<00:00, 243.22it/s]\n",
      "Epoch 33: 100%|████████████████████████████████| 22/22 [00:00<00:00, 238.89it/s]\n",
      "Epoch 34: 100%|████████████████████████████████| 22/22 [00:00<00:00, 231.90it/s]\n",
      "Epoch 35: 100%|████████████████████████████████| 22/22 [00:00<00:00, 276.27it/s]\n",
      "Epoch 36: 100%|████████████████████████████████| 22/22 [00:00<00:00, 268.02it/s]\n",
      "Epoch 37: 100%|████████████████████████████████| 22/22 [00:00<00:00, 277.71it/s]\n",
      "Epoch 38: 100%|████████████████████████████████| 22/22 [00:00<00:00, 276.74it/s]\n",
      "Epoch 39: 100%|████████████████████████████████| 22/22 [00:00<00:00, 282.48it/s]\n",
      "Epoch 40: 100%|████████████████████████████████| 22/22 [00:00<00:00, 278.16it/s]\n",
      "Epoch 41: 100%|████████████████████████████████| 22/22 [00:00<00:00, 269.64it/s]\n",
      "Epoch 42: 100%|████████████████████████████████| 22/22 [00:00<00:00, 297.35it/s]\n",
      "Epoch 43: 100%|████████████████████████████████| 22/22 [00:00<00:00, 273.05it/s]\n",
      "Epoch 44: 100%|████████████████████████████████| 22/22 [00:00<00:00, 211.84it/s]\n",
      "Epoch 45: 100%|████████████████████████████████| 22/22 [00:00<00:00, 219.62it/s]\n",
      "Epoch 46: 100%|████████████████████████████████| 22/22 [00:00<00:00, 209.49it/s]\n",
      "Epoch 47: 100%|████████████████████████████████| 22/22 [00:00<00:00, 182.71it/s]\n",
      "Epoch 48: 100%|████████████████████████████████| 22/22 [00:00<00:00, 186.17it/s]\n",
      "Epoch 49: 100%|████████████████████████████████| 22/22 [00:00<00:00, 215.92it/s]\n",
      "Epoch 50: 100%|████████████████████████████████| 22/22 [00:00<00:00, 218.62it/s]\n",
      "Epoch 51: 100%|████████████████████████████████| 22/22 [00:00<00:00, 222.93it/s]\n",
      "Epoch 52: 100%|████████████████████████████████| 22/22 [00:00<00:00, 244.11it/s]\n",
      "Epoch 53: 100%|████████████████████████████████| 22/22 [00:00<00:00, 231.82it/s]\n",
      "Epoch 54: 100%|████████████████████████████████| 22/22 [00:00<00:00, 209.51it/s]\n",
      "Epoch 55: 100%|████████████████████████████████| 22/22 [00:00<00:00, 235.39it/s]\n",
      "Epoch 56: 100%|████████████████████████████████| 22/22 [00:00<00:00, 229.39it/s]\n",
      "Epoch 57: 100%|████████████████████████████████| 22/22 [00:00<00:00, 245.53it/s]\n",
      "Epoch 58: 100%|████████████████████████████████| 22/22 [00:00<00:00, 260.84it/s]\n",
      "Epoch 59: 100%|████████████████████████████████| 22/22 [00:00<00:00, 222.18it/s]\n",
      "Epoch 60: 100%|████████████████████████████████| 22/22 [00:00<00:00, 216.17it/s]\n",
      "Epoch 61: 100%|████████████████████████████████| 22/22 [00:00<00:00, 229.80it/s]\n",
      "Epoch 62: 100%|████████████████████████████████| 22/22 [00:00<00:00, 253.51it/s]\n",
      "Epoch 63: 100%|████████████████████████████████| 22/22 [00:00<00:00, 259.35it/s]\n",
      "Epoch 64: 100%|████████████████████████████████| 22/22 [00:00<00:00, 269.83it/s]\n",
      "Epoch 65: 100%|████████████████████████████████| 22/22 [00:00<00:00, 263.21it/s]\n",
      "Epoch 66: 100%|████████████████████████████████| 22/22 [00:00<00:00, 273.70it/s]\n",
      "Epoch 67: 100%|████████████████████████████████| 22/22 [00:00<00:00, 262.43it/s]\n",
      "Epoch 68: 100%|████████████████████████████████| 22/22 [00:00<00:00, 291.29it/s]\n",
      "Epoch 69: 100%|████████████████████████████████| 22/22 [00:00<00:00, 248.58it/s]\n",
      "Epoch 70: 100%|████████████████████████████████| 22/22 [00:00<00:00, 228.76it/s]\n",
      "Epoch 71: 100%|████████████████████████████████| 22/22 [00:00<00:00, 244.57it/s]\n",
      "Epoch 72: 100%|████████████████████████████████| 22/22 [00:00<00:00, 256.81it/s]\n",
      "Epoch 73: 100%|████████████████████████████████| 22/22 [00:00<00:00, 241.50it/s]\n",
      "Epoch 74: 100%|████████████████████████████████| 22/22 [00:00<00:00, 246.79it/s]\n",
      "Epoch 75: 100%|████████████████████████████████| 22/22 [00:00<00:00, 272.80it/s]\n",
      "Epoch 76: 100%|████████████████████████████████| 22/22 [00:00<00:00, 265.85it/s]\n",
      "Epoch 77: 100%|████████████████████████████████| 22/22 [00:00<00:00, 256.11it/s]\n",
      "Epoch 78: 100%|████████████████████████████████| 22/22 [00:00<00:00, 239.66it/s]\n",
      "Epoch 79: 100%|████████████████████████████████| 22/22 [00:00<00:00, 223.24it/s]\n",
      "Epoch 80: 100%|████████████████████████████████| 22/22 [00:00<00:00, 205.44it/s]\n",
      "Epoch 81: 100%|████████████████████████████████| 22/22 [00:00<00:00, 259.82it/s]\n",
      "Epoch 82: 100%|████████████████████████████████| 22/22 [00:00<00:00, 269.72it/s]\n",
      "Epoch 83: 100%|████████████████████████████████| 22/22 [00:00<00:00, 290.68it/s]\n",
      "Epoch 84: 100%|████████████████████████████████| 22/22 [00:00<00:00, 262.51it/s]\n",
      "Epoch 85: 100%|████████████████████████████████| 22/22 [00:00<00:00, 244.04it/s]\n",
      "Epoch 86: 100%|████████████████████████████████| 22/22 [00:00<00:00, 262.14it/s]\n",
      "Epoch 87: 100%|████████████████████████████████| 22/22 [00:00<00:00, 288.42it/s]\n",
      "Epoch 88: 100%|████████████████████████████████| 22/22 [00:00<00:00, 278.50it/s]\n",
      "Epoch 89: 100%|████████████████████████████████| 22/22 [00:00<00:00, 280.92it/s]\n",
      "Epoch 90: 100%|████████████████████████████████| 22/22 [00:00<00:00, 271.40it/s]\n",
      "Epoch 91: 100%|████████████████████████████████| 22/22 [00:00<00:00, 251.93it/s]\n",
      "Epoch 92: 100%|████████████████████████████████| 22/22 [00:00<00:00, 267.41it/s]\n",
      "Epoch 93: 100%|████████████████████████████████| 22/22 [00:00<00:00, 297.71it/s]\n",
      "Epoch 94: 100%|████████████████████████████████| 22/22 [00:00<00:00, 238.29it/s]\n",
      "Epoch 95: 100%|████████████████████████████████| 22/22 [00:00<00:00, 260.02it/s]\n",
      "Epoch 96: 100%|████████████████████████████████| 22/22 [00:00<00:00, 252.80it/s]\n",
      "Epoch 97: 100%|████████████████████████████████| 22/22 [00:00<00:00, 273.22it/s]\n",
      "Epoch 98: 100%|████████████████████████████████| 22/22 [00:00<00:00, 277.14it/s]\n",
      "Epoch 99: 100%|████████████████████████████████| 22/22 [00:00<00:00, 271.53it/s]\n",
      "Epoch 100: 100%|███████████████████████████████| 22/22 [00:00<00:00, 252.81it/s]\n",
      "Epoch 101: 100%|███████████████████████████████| 22/22 [00:00<00:00, 257.68it/s]\n",
      "Epoch 102: 100%|███████████████████████████████| 22/22 [00:00<00:00, 266.42it/s]\n",
      "Epoch 103: 100%|███████████████████████████████| 22/22 [00:00<00:00, 269.11it/s]\n",
      "Epoch 104: 100%|███████████████████████████████| 22/22 [00:00<00:00, 263.54it/s]\n",
      "Epoch 105: 100%|███████████████████████████████| 22/22 [00:00<00:00, 274.13it/s]\n",
      "Epoch 106: 100%|███████████████████████████████| 22/22 [00:00<00:00, 261.99it/s]\n",
      "Epoch 107: 100%|███████████████████████████████| 22/22 [00:00<00:00, 273.40it/s]\n",
      "Epoch 108: 100%|███████████████████████████████| 22/22 [00:00<00:00, 275.07it/s]\n",
      "Epoch 109: 100%|███████████████████████████████| 22/22 [00:00<00:00, 277.01it/s]\n",
      "Epoch 110: 100%|███████████████████████████████| 22/22 [00:00<00:00, 297.59it/s]\n",
      "Epoch 111: 100%|███████████████████████████████| 22/22 [00:00<00:00, 277.94it/s]\n",
      "Epoch 112: 100%|███████████████████████████████| 22/22 [00:00<00:00, 298.54it/s]\n",
      "Epoch 113: 100%|███████████████████████████████| 22/22 [00:00<00:00, 286.30it/s]\n",
      "Epoch 114: 100%|███████████████████████████████| 22/22 [00:00<00:00, 295.70it/s]\n",
      "Epoch 115: 100%|███████████████████████████████| 22/22 [00:00<00:00, 289.73it/s]\n",
      "Epoch 116: 100%|███████████████████████████████| 22/22 [00:00<00:00, 293.73it/s]\n",
      "Epoch 117: 100%|███████████████████████████████| 22/22 [00:00<00:00, 295.36it/s]\n",
      "Epoch 118: 100%|███████████████████████████████| 22/22 [00:00<00:00, 298.17it/s]\n",
      "Epoch 119: 100%|███████████████████████████████| 22/22 [00:00<00:00, 274.17it/s]\n",
      "Epoch 120: 100%|███████████████████████████████| 22/22 [00:00<00:00, 251.21it/s]\n",
      "Epoch 121: 100%|███████████████████████████████| 22/22 [00:00<00:00, 257.98it/s]\n",
      "Epoch 122: 100%|███████████████████████████████| 22/22 [00:00<00:00, 248.94it/s]\n",
      "Epoch 123: 100%|███████████████████████████████| 22/22 [00:00<00:00, 274.46it/s]\n",
      "Epoch 124: 100%|███████████████████████████████| 22/22 [00:00<00:00, 269.10it/s]\n",
      "Epoch 125: 100%|███████████████████████████████| 22/22 [00:00<00:00, 265.97it/s]\n",
      "Epoch 126: 100%|███████████████████████████████| 22/22 [00:00<00:00, 245.14it/s]\n",
      "Epoch 127: 100%|███████████████████████████████| 22/22 [00:00<00:00, 284.32it/s]\n",
      "Epoch 128: 100%|███████████████████████████████| 22/22 [00:00<00:00, 300.42it/s]\n",
      "Epoch 129: 100%|███████████████████████████████| 22/22 [00:00<00:00, 296.78it/s]\n",
      "Epoch 130: 100%|███████████████████████████████| 22/22 [00:00<00:00, 279.22it/s]\n",
      "Epoch 131: 100%|███████████████████████████████| 22/22 [00:00<00:00, 292.43it/s]\n",
      "Epoch 132: 100%|███████████████████████████████| 22/22 [00:00<00:00, 264.00it/s]\n",
      "Epoch 133: 100%|███████████████████████████████| 22/22 [00:00<00:00, 252.27it/s]\n",
      "Epoch 134: 100%|███████████████████████████████| 22/22 [00:00<00:00, 261.07it/s]\n",
      "Epoch 135: 100%|███████████████████████████████| 22/22 [00:00<00:00, 269.61it/s]\n",
      "Epoch 136: 100%|███████████████████████████████| 22/22 [00:00<00:00, 283.53it/s]\n",
      "Epoch 137: 100%|███████████████████████████████| 22/22 [00:00<00:00, 260.77it/s]\n",
      "Epoch 138:   0%|                                         | 0/22 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Initialize lists to keep track of losses and epochs\n",
    "# Initialize MLflow run\n",
    "# Initialize MLflow run\n",
    "# Initialize MLflow run\n",
    "\n",
    "# Ensure this is the model you intend to train\n",
    "model = final_model\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()  # Correct model reference for training mode\n",
    "    total_epoch_loss = 0\n",
    "    for batch_idx, (user_ids, movie_ids, ratings) in tqdm(enumerate(all_data_loader), total=len(all_data_loader), desc=f'Epoch {epoch}'):\n",
    "\n",
    "        optimiser.zero_grad()\n",
    "        predictions = model(user_ids, movie_ids)  # Ensure consistent model reference\n",
    "        loss = criterion(predictions, ratings)\n",
    "\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        \n",
    "        batch_loss = loss.item()\n",
    "        total_epoch_loss += batch_loss\n",
    "        \n",
    "    avg_epoch_loss = total_epoch_loss / len(all_data_loader)\n",
    "    \n",
    "    # Logging\n",
    "    wandb.log({\"Avg. Training Loss\": avg_epoch_loss, \"Epoch\": epoch})\n",
    "    mlflow.log_metric(\"Avg. Training Loss\", avg_epoch_loss, step=epoch)\n",
    "\n",
    "# Finish the Weights & Biases run\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d87936-8980-4124-b32c-cc1a4813102b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfed358-d01f-4c10-a71a-04d55bbb4989",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': final_model.state_dict(),\n",
    "    'optimiser_state_dict': optimiser.state_dict(),\n",
    "    'Avg. training loss': avg_epoch_loss,\n",
    "}, 'final_col_model_checkpoint.pth')\n",
    "mlflow.log_artifact('final_col_model_checkpoint.pth')\n",
    "mlflow.pytorch.log_model(final_model, \"model\")\n",
    "# Save model state\n",
    "torch.save(final_model.state_dict(), 'best_model_state.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a5e18c-653c-4d10-9bac-b3669a38c95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test data\n",
    "final_model.eval()  # Switch to evaluation mode\n",
    "test_loss = 0\n",
    "correct_predictions = 0  # Fixed variable name for clarity\n",
    "total_contexts = 0\n",
    "\n",
    "with torch.no_grad():  # Disable gradient computation\n",
    "    for user_ids, movie_ids, ratings in test_data_loader:\n",
    "        predictions = final_model(user_ids,movie_ids)  # Generate predictions\n",
    "        # Calculate and accumulate loss\n",
    "        loss = criterion(predictions, ratings)\n",
    "        test_loss += loss.item()\n",
    "        \n",
    "        # # Reshape predictions to match [batch_size, context_size, vocab_size]\n",
    "        # predictions = predictions.view(-1, context_size, VOCAB_SIZE)\n",
    "        \n",
    "        # # Get top prediction for each context position\n",
    "        # top_predictions = predictions.argmax(dim=2)\n",
    "        \n",
    "        # # Calculate correct predictions\n",
    "        # correct_preds = (top_predictions == context).float().sum()\n",
    "        # correct_predictions += correct_preds.item()  # Accumulate correct predictions\n",
    "        \n",
    "        # total_contexts += context.numel()  # Total number of context word positions evaluated\n",
    "\n",
    "# Calculate final metrics\n",
    "test_loss /= len(test_data_loader)  # Average loss per batch\n",
    "# print('correct predictions = ',correct_predictions)\n",
    "# print('out of  = ',total_contexts)\n",
    "# accuracy = correct_predictions / total_contexts  # Compute accuracy\n",
    "\n",
    "# print(f'Test set: Average loss: {test_loss:.4f}, Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Evaluate the model\n",
    "# (This would involve using a separate validation set or performing cross-validation)\n",
    "print(test_loss)\n",
    "\n",
    "mlflow.log_metric('Post training test loss',test_loss)\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10373ba-5557-46bf-aa37-8f4234b43def",
   "metadata": {},
   "source": [
    "## Preference ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377c97ca-a1c8-4a79-ad7f-f844c2b6ab59",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_user_id = 140440102666832"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb9de61-95a2-4c4d-bae9-78f17254013c",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_uid = encoder.encode(target_user_id,encoder.user_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb44d161-b86c-48e5-a2a8-228ee34d61fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_features(uid):\n",
    "\n",
    "    for i in range(0,len(all_ds)):\n",
    "\n",
    "        u,m,r = all_ds[i]\n",
    "        # print(u)\n",
    "        if u == uid:\n",
    "            yield [m,r]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca61297-1afc-439e-9a3a-81bff33d384f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ratings(e_uid,e_mid,model):\n",
    "    e_uid_tensor = torch.tensor(e_uid, dtype=torch.int64).unsqueeze(0)\n",
    "    movie_eid_tensor = torch.tensor(e_mid,dtype=torch.int64).unsqueeze(0)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        user_rating = model(e_uid_tensor,movie_eid_tensor)\n",
    "        return movie_eid_tensor,user_rating\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3780e9b-1c32-4f2b-a45e-14bdb9d35b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get user and features\n",
    "seen_features = set(tuple(fe) for fe in get_user_features(e_uid)) # Get user features if needed\n",
    "seen_movies = set(sf[0].item() for sf in seen_features)\n",
    "all_movies = set([num for num in range(30)])\n",
    "unseen_movies = all_movies - seen_movies\n",
    "unseen_features = {tuple(predict_ratings(e_uid,um,model)) for um in unseen_movies}\n",
    "all_features = seen_features | unseen_features\n",
    "\n",
    "sorted_set = sorted(all_features, key=lambda x: x[1])\n",
    "top_n = 10\n",
    "recommendations = list(filter(lambda x: x[0].item() in unseen_movies,sorted_set))\n",
    "n_recommendations = list(map(lambda x: x[0],recommendations[-1:top_n*-1:-1]))\n",
    "n_recommendations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3470ac7f-df21-40aa-94aa-32aa93606c9a",
   "metadata": {},
   "source": [
    "# Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9289eb5-57c3-42bd-ba7f-7c519590d8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all users, with all movie ratings.\n",
    "def generate_all_ratings(model,num_users,num_movies):\n",
    "    #build matrix\n",
    "\n",
    "    #data already encoded....\n",
    "    all_data = [['dummy',mnm,unm,'dummy','0/10'] for unm in range(0,num_users) for mnm in range(0,num_movies)]\n",
    "    # print(all_data)\n",
    "    # print([em for em in all_data])\n",
    "    fake_encoder = Encoder([did[2] for did in all_data],[did[1] for did in all_data])\n",
    "    all_ds = CFDataset(all_data,fake_encoder)\n",
    "    \n",
    "    data_loader = DataLoader(all_ds, batch_size=1)\n",
    "    for uid,mid,ra in data_loader:\n",
    "        # print(uid,mid)\n",
    "        prediction = model(uid,mid)\n",
    "        yield (uid.item(),mid.item(),prediction.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788b3ed2-c265-4b06-a5b9-53fa0c63b904",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ratings_tensor(data, num_users, num_movies):\n",
    "    # Initialize an empty tensor to hold the ratings\n",
    "    ratings_tensor = torch.zeros(num_users, num_movies)\n",
    "    \n",
    "    # Iterate over the data and fill the tensor\n",
    "    for entry in data:\n",
    "        user_id, movie_id, rating = entry\n",
    "        # Convert rating to float\n",
    "        rating = float(rating)\n",
    "        ratings_tensor[user_id, movie_id] = rating\n",
    "    \n",
    "    return ratings_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd12391-a846-475e-be55-b2516a0ef7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Generate Predictions\n",
    "predictions = [ra for ra in generate_all_ratings(model,num_users,num_movies)]\n",
    "# print(predictions)\n",
    "ratings_tensor = create_ratings_tensor(predictions,num_users,num_movies)\n",
    "print(ratings_tensor)\n",
    "# predictions[0]\n",
    "\n",
    "# ratings_tensor.shape\n",
    "\n",
    "# ratings_tensor[0]\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Assuming predicted_ratings is your tensor of predicted ratings\n",
    "# predicted_ratings.shape should be (num_users, num_movies)\n",
    "\n",
    "# # Calculate cosine similarity\n",
    "# user_similarities = F.cosine_similarity(ratings_tensor, ratings_tensor, dim=1)\n",
    "\n",
    "# user_similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d0c474-3481-489e-b494-047c64b14925",
   "metadata": {},
   "source": [
    "## COSINE SIMILARITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cae1de-bc62-460e-b2db-f046961f4493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Normalize predicted ratings\n",
    "normalized_ratings = F.normalize(ratings_tensor, p=2, dim=1)\n",
    "\n",
    "# Step 2: Calculate cosine similarity\n",
    "user_similarities = torch.matmul(normalized_ratings, normalized_ratings.T)\n",
    "\n",
    "# Set diagonal elements to a large negative value to exclude self-similarity\n",
    "user_similarities.fill_diagonal_(-float('inf'))\n",
    "\n",
    "# You can optionally convert the similarities tensor to a numpy array for easier manipulation\n",
    "user_similarities_np = user_similarities.numpy()\n",
    "\n",
    "# Print or use the user similarities tensor\n",
    "print(user_similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb89e2b-202f-405a-a732-26cf09959925",
   "metadata": {},
   "source": [
    "### Matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e402f0-b39d-4599-9617-5fcf23d00a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_matches = {}\n",
    "for i in range(len(user_similarities)):\n",
    "    # Sort similarities for the current user i\n",
    "    ranked_users = torch.argsort(torch.tensor(user_similarities[i]), descending=True)\n",
    "    # Exclude self from top matches (optional)\n",
    "    top_matches[i] = ranked_users[ranked_users != i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d560016-14da-4170-be81-43e27b094043",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebc2399-8831-4bef-b722-b9405c776b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.decode(71,encoder.idx_to_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cb1dfe-e30f-407f-accb-367b5bce66a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#decoded\n",
    "\n",
    "decoded_matches = {encoder.decode(k,encoder.idx_to_user):[encoder.decode(ve,encoder.idx_to_user) for ve in v.tolist()] for k,v in top_matches.items()}\n",
    "decoded_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73632e60-b177-479e-8e78-c6eea82ea863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Select Top Matches\n",
    "N = 5  # Number of top matches to select\n",
    "for user, similar_users in decoded_matches.items():\n",
    "    decoded_matches[user] = similar_users[:N]\n",
    "\n",
    "decoded_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7f3715-96e0-44a4-9a49-eee4e94bfaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#decode...."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c7c0fa-b24f-4418-b5ca-e1713254be27",
   "metadata": {},
   "source": [
    "## FINALLY FIND THE MATCHES...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd884b67-4541-453a-8bdf-87adf9d4ca39",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_user = 140440102666832\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0929f29-14e7-4cfa-acc9-9a741b7a5187",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_matches[target_user]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ed138f-43a0-4e9c-a66d-c7030de6e662",
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbour = 140440115331424"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a6a7ef-f019-4273-bc7c-c63dae3a104d",
   "metadata": {},
   "source": [
    "## Now see how they relate to the database..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b89fe2d-944d-43bc-b2a2-81af64bd6ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = db.get_table_values('Reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df7b55f-d93f-4a8d-b4c3-f0b1ea9ddf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2b3f3b-a7f8-48a6-b772-23c522f5ab85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reviews_info(user_id,reviews):\n",
    "    # print(db.get_table_values('Users'))\n",
    "    info = {}\n",
    "    info[user_id] = list(filter(lambda x :x[0] == user_id,db.get_table_values('Users')))[0][1]\n",
    "\n",
    "    \n",
    "    #TODO finish work...\n",
    "    mov_tab = db.get_table_values('Movies')\n",
    "    \n",
    "    for review in reviews:\n",
    "        mov_id = review[1]\n",
    "        info[mov_id] = {'title':list(filter(lambda x :x[0] == mov_id,mov_tab))[0][1],'rating':review[4]}\n",
    "        \n",
    "    return info\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef46e616-1299-4659-8e09-f6d3fdf44b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89593c73-41e3-4a6e-8e9c-61fd41b372dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_reviews = list(filter(lambda rv: rv[2] == target_user,reviews))\n",
    "neighbour_reviews = list(filter(lambda rv: rv[2] == neighbour,reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fd7bda-4f06-41de-89d2-7e84ce851863",
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_info = reviews_info(target_user,target_reviews)\n",
    "neigh_info = reviews_info(neighbour,neighbour_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cf7940-a7bf-4443-87b6-abd8d7d47bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78c8ccb-97d3-439a-ac8d-28ebe5e8ba91",
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6864fda6-f897-4c0a-8cf8-405e8cadd208",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae878490-881c-4345-9040-934a38615e40",
   "metadata": {},
   "source": [
    "# SAVE_MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef1a3d5-a607-4a92-82a3-237a07f15ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653f0e2d-f90d-4bc4-b72c-2dfda1b2f542",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "menv",
   "language": "python",
   "name": "menv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
