{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a573191-c50c-46a7-be39-8d9e9ad5ae8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from db.ipynb\n",
      "best_col_model_checkpoint.pth\t\tmain.py\n",
      "best_contentfiltermodel_checkpoint.pth\tmenv\n",
      "best_model_checkpoint.pth\t\tmlruns\n",
      "best_model_state.pth\t\t\tmodels\n",
      "cf_model.py\t\t\t\tmovie.py\n",
      "config.py\t\t\t\tpath_to_your_database.db\n",
      "dataset.py\t\t\t\t__pycache__\n",
      "db.ipynb\t\t\t\tq.py\n",
      "Dockerfile\t\t\t\tREADME.md\n",
      "done.txt\t\t\t\trequirements.txt\n",
      "encoder.pkl\t\t\t\tserver\n",
      "fdstests.ipynb\t\t\t\ttest.db\n",
      "filemanager.py\t\t\t\ttestsources.ipynb\n",
      "holocenemodels\t\t\t\ttodo.txt\n",
      "indie_letterboxd.db\t\t\ttrain_colfilter.ipynb\n",
      "indie_letterboxd_v2.db\t\t\ttrain_confilter.ipynb\n",
      "letterboxd\t\t\t\tuser.py\n",
      "letterboxd.db\t\t\t\twandb\n",
      "main.ipynb\t\t\t\tweb_spider.py\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from cf_model import MLPCollaborativeFilter\n",
    "from dataset import ColFDataset, Encoder, split_data\n",
    "import import_ipynb\n",
    "import db\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4967b732-0b9e-4747-a84f-07e3e8f4dd71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users: 2400\n",
      "\n",
      "Movies: 771\n",
      "\n",
      "Reviews: 18504\n"
     ]
    }
   ],
   "source": [
    "DB_NAME = 'indie_letterboxd_v2'\n",
    "db.display_size(DB_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28be6c6f-cf28-486e-9aa5-78b8ff780a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(DB_NAME+'.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# SQL query to fetch user name and ID, movie name and ID, and rating from reviews\n",
    "query = '''\n",
    "    SELECT \n",
    "        u.user_id,\n",
    "        u.name AS user_name, \n",
    "        m.movie_id,\n",
    "        m.title AS movie_title, \n",
    "        r.rating\n",
    "    FROM \n",
    "        Reviews r\n",
    "    JOIN Users u ON r.user_id = u.user_id\n",
    "    JOIN Movies m ON r.movie_id = m.movie_id\n",
    "    '''\n",
    "\n",
    "try:\n",
    "    cursor.execute(query)\n",
    "    reviews = cursor.fetchall()\n",
    "    \n",
    "    # for review in reviews:\n",
    "    #     print(\"User ID:\", review[0], \"| User Name:\", review[1], \"| Movie ID:\", review[2], \"| Movie Title:\", review[3], \"| Rating:\", review[4])\n",
    "except sqlite3.Error as e:\n",
    "    print(\"An error occurred:\", e)\n",
    "finally:\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8802ab01-9d57-4fdf-8341-215f3e0212ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Sr(a). Felix Rosado', 'Saloum', 0.8), ('Фрацил Джогов', 'Love Dad', 0.24), ('Jožefa Mlakar', 'Prisoners Daughter', 0.8), ('Monica Wheeler', 'Alam', 0.6), ('Hunar De', 'The Burnt Orange Heresy', 0.2), ('Dr Wayne Rowe', 'The World Is Family', 0.9), ('Сафонова Зинаида Викторовна', 'Mountain Cat', 0.4), ('Saulītis, Ernests', 'State Funeral', 0.71), ('Lizz Latier-van Es', 'Coming Home Again', 0.4), ('Samuel Oliver', 'My Life as a Comedian', 0.8)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(db.display_size())\n",
    "#TODO change shuffle and not shuffle\n",
    "\n",
    "# Assuming 'reviews' is a list of tuples and you've already created 'data'\n",
    "data = [tuple([did[1], did[3], did[-1]]) for did in reviews]\n",
    "\n",
    "# Shuffle 'data' in place with random.shuffle()\n",
    "random.shuffle(data)\n",
    "\n",
    "# Now 'data' is shuffled, and you can work with it\n",
    "print(data[:10])\n",
    "#found invalid values earlier.\n",
    "\n",
    "data = [pre for pre in data if pre[-1] > 0]\n",
    "data.count(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a8d4986-83f4-4fbc-8bba-429cda6a865e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test data is holdout for Kfolds cross data validation evaluation.\n",
    "train_data, test_data = split_data(data)\n",
    "train_data, validation_data = split_data(train_data,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2eccd133-b107-4dfa-9b16-b07bc58b375b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13241"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2438744-302d-4f6b-9a63-d49b2eadbd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_users_data = [ud[0] for ud in train_data]\n",
    "train_movies_data = [md[1] for md in train_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1b2649-c1fb-48bd-ab55-030b640199e7",
   "metadata": {},
   "source": [
    "## after splitting, only use the training data --- for encoding!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfbc8886-5861-4526-9488-ed53e502a2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(users=train_users_data,movies=train_movies_data)###<---- important\n",
    "import pickle\n",
    "\n",
    "# Assuming 'encoder' is your encoder object\n",
    "with open('encoder.pkl', 'wb') as file:\n",
    "    pickle.dump(encoder, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "319e8a9c-44a3-44d4-99c7-ad3b042220e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ColFDataset(train_data,encoder)\n",
    "test_ds = ColFDataset(test_data,encoder)\n",
    "validation_ds = ColFDataset(validation_data,encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af64661d-4749-45e3-a3a9-dcfdb27b43ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1471"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds.movie_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a97f9cb-f0f1-45db-b175-4293529fa52d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.movie_ids.count(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae3212ab-ac1f-4efa-b29a-a509247f08d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13241"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validation_ds.movie_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75750116-45b4-4af9-be6e-d3b828a05311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7167"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_ds.user_ids.count(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2a65e0d-f096-4312-b02e-2ccbbb1a38af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1869"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_ds.movie_ids.count(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cbc125dc-8fca-445a-8f90-ea504b004900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3679"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_ds.movie_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83a82ced-863d-4bd4-b00d-a99a24047c89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "546"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds.movie_ids.count(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75616c10-0c39-460b-a33d-9507b9043ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "train_data_loader = DataLoader(train_ds,batch_size=BATCH_SIZE,shuffle=False,drop_last=True)\n",
    "validation_data_loader = DataLoader(validation_ds,batch_size=BATCH_SIZE,shuffle=False,drop_last=True)\n",
    "test_data_loader = DataLoader(test_ds,batch_size=BATCH_SIZE,shuffle=False,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36edabff-0087-49fd-9cbf-c22e06c27b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users = len(encoder.vocab_to_idx['users'])\n",
    "num_movies = len(encoder.vocab_to_idx['movies'])\n",
    "FEATURES=700\n",
    "model = MLPCollaborativeFilter(num_users + 1, num_movies + 1, embedding_dim=FEATURES)\n",
    "\n",
    "\n",
    "LEARNING_RATE = 0.001\n",
    "criterion = nn.MSELoss()\n",
    "#weight decay L2 regularization\n",
    "# optimiser = optim.SGD(model.parameters(), lr=0.001,weight_decay=1e-5)\\\n",
    "L2_REGULARIZATION=0.1\n",
    "optimiser = optim.SGD(model.parameters(), lr=LEARNING_RATE,weight_decay=L2_REGULARIZATION)\n",
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "157cd204-76fe-473a-9e57-a7adda48b185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18391\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1471"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(data))\n",
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0aea2d-2597-4a50-9b6d-d65231ed49eb",
   "metadata": {},
   "source": [
    "# EDA "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadfc0f4-b4df-4835-85b1-ebae07f11741",
   "metadata": {},
   "source": [
    "    Examine the data for patterns, anomalies, or characteristics.\n",
    "    Check for data quality issues such as missing values, outliers, or incorrect data types. <--- either predropped or imputed.\n",
    "    Get a sense of the distributions of your variables and the relationships between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1228065-7071-4b93-b460-a4204e6fbbda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  user                    movie  ratings\n",
      "0  Sr(a). Felix Rosado                   Saloum     0.80\n",
      "1        Фрацил Джогов                 Love Dad     0.24\n",
      "2        Jožefa Mlakar       Prisoners Daughter     0.80\n",
      "3       Monica Wheeler                     Alam     0.60\n",
      "4             Hunar De  The Burnt Orange Heresy     0.20\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 18391 entries, 0 to 18390\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   user     18391 non-null  object \n",
      " 1   movie    18391 non-null  object \n",
      " 2   ratings  18391 non-null  float64\n",
      "dtypes: float64(1), object(2)\n",
      "memory usage: 431.2+ KB\n",
      "None\n",
      "            ratings\n",
      "count  18391.000000\n",
      "mean       0.674751\n",
      "std        0.210540\n",
      "min        0.020000\n",
      "25%        0.540000\n",
      "50%        0.700000\n",
      "75%        0.800000\n",
      "max        1.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "column_names = ['user', 'movie','ratings']\n",
    "\n",
    "df = pd.DataFrame(data, columns=column_names)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(df.head())\n",
    "\n",
    "# Get a summary of the dataset\n",
    "print(df.info())\n",
    "\n",
    "# Generate descriptive statistics\n",
    "print(df.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c02bbd90-8966-44b8-9ddd-59d31ce3d82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user       0\n",
      "movie      0\n",
      "ratings    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "51da5313-6bec-42a9-835e-2c535197d382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x1500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 250x250 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Histograms for numeric features\n",
    "df.hist(bins=50, figsize=(20,15))\n",
    "plt.show()\n",
    "\n",
    "# Box plots for distributions and spotting outliers\n",
    "sns.boxplot(data=df)\n",
    "plt.show()\n",
    "\n",
    "# Pairplot to visualize pairwise relationships between features\n",
    "sns.pairplot(df)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6b05fd44-31ed-4c75-b0b2-5558d525bd0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>movie</th>\n",
       "      <th>ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [user, movie, ratings]\n",
       "Index: []"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['ratings'] == -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5fe39638-e29b-49ca-bf75-4d8d44ace1c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     6\n",
       "1     8\n",
       "2    18\n",
       "3     4\n",
       "4    23\n",
       "Name: title_length, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['title_length'] = df['movie'].str.len()\n",
    "df['title_length'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6c4fe575-c792-46de-9fac-c4e280e4889f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "df['title_length']\n",
    "# Correlation matrix\n",
    "\n",
    "corr_matrix = df[['title_length','ratings']].corr()\n",
    "\n",
    "# Heatmap of the correlation matrix\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\")\n",
    "plt.show()\n",
    "\n",
    "# Specifically look at correlations with the target variable if defined\n",
    "# target_corr = corr_matrix[\"YourTargetVariable\"].sort_values(ascending=False)\n",
    "# print(target_corr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dc364699-eaa9-4bd5-9e46-359b19c0271f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26\n",
      "                            user                    movie  ratings  \\\n",
      "99               მზია ქევხიშვილი         Dear Evan Hansen     0.10   \n",
      "130          Пейолина Пенджакова                 Daliland     0.10   \n",
      "241             Гаврило Ґжицький                 Synonyms     0.10   \n",
      "298                 Arne Hansson         The Moneychanger     0.09   \n",
      "312    Dr. Feketené Orbán Margit           Spring Blossom     0.10   \n",
      "...                          ...                      ...      ...   \n",
      "18174                        윤영미          The Hummingbird     0.10   \n",
      "18213        ललित बालासुब्रमणियम                Baby Ruby     0.10   \n",
      "18270            सत्यव्रता रिमाल               Good Madam     0.10   \n",
      "18369             ლიკა გურგენიძე                   Zalava     0.10   \n",
      "18378              Vilém Růžička  Hate to Love Nickelback     0.10   \n",
      "\n",
      "       title_length  \n",
      "99               16  \n",
      "130               8  \n",
      "241               8  \n",
      "298              16  \n",
      "312              14  \n",
      "...             ...  \n",
      "18174            15  \n",
      "18213             9  \n",
      "18270            10  \n",
      "18369             6  \n",
      "18378            23  \n",
      "\n",
      "[300 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Example of finding outliers for a 'feature_name'\n",
    "Q1 = df['ratings'].quantile(0.25)\n",
    "Q3 = df['ratings'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "print(IQR)\n",
    "outlier_condition = ((df['ratings'] < (Q1 - 1.5 * IQR)) | (df['ratings'] > (Q3 + 1.5 * IQR)))\n",
    "outliers = df[outlier_condition]\n",
    "print(outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5dbea2-ea83-4f1a-b206-60a993873cfe",
   "metadata": {},
   "source": [
    "## MLFLOw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ae300159-0298-450a-b512-ddf65ad3d207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///home/james/Desktop/torchex/movie-users/mlruns/827152334394031969', creation_time=1710099404696, experiment_id='827152334394031969', last_update_time=1710099404696, lifecycle_stage='active', name='Finetuned Collaborative Filter', tags={}>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# Optional: Set MLflow experiment\n",
    "mlflow.set_experiment(\"Finetuned Collaborative Filter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5bebaeb2-02a1-47d8-8f20-28d488b69201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Log model parameters (example)\n",
    "mlflow.start_run()\n",
    "mlflow.log_param(\"epochs\", EPOCHS)\n",
    "mlflow.log_param(\"optimizer\", type(optimiser).__name__)\n",
    "mlflow.log_param(\"learning rate\", LEARNING_RATE)\n",
    "mlflow.log_param(\"batch size\", BATCH_SIZE)\n",
    "mlflow.log_param(\"L2 regularization\", L2_REGULARIZATION)\n",
    "mlflow.log_param(\"drop out\", 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623dc331-6d00-41db-b670-fe7c76a9d10b",
   "metadata": {},
   "source": [
    "# PRETEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1f13953d-a5bc-4fd2-8d48-3c323be56cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8345452013768648\n"
     ]
    }
   ],
   "source": [
    "#test data\n",
    "model.eval()  # Switch to evaluation mode\n",
    "test_loss = 0\n",
    "correct_predictions = 0  # Fixed variable name for clarity\n",
    "total_contexts = 0\n",
    "\n",
    "with torch.no_grad():  # Disable gradient computation\n",
    "    for user_ids, movie_ids, ratings in test_data_loader:\n",
    "        predictions = model(user_ids,movie_ids)  # Generate predictions\n",
    "        # Calculate and accumulate loss\n",
    "        loss = criterion(predictions, ratings)\n",
    "        test_loss += loss.item()\n",
    "        \n",
    "\n",
    "test_loss /= len(test_data_loader)  # Average loss per batch\n",
    "\n",
    "print(test_loss)\n",
    "\n",
    "mlflow.log_metric('pretraining test loss',test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39e0f67-b1d4-4a01-826a-4e37d9fb5127",
   "metadata": {},
   "source": [
    "## wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "000ffd1c-94ac-4276-bede-a4b7f186794f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjcrich\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/james/Desktop/torchex/movie-users/wandb/run-20240311_221135-qj9da64k</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jcrich/collaborative%20filter%20model/runs/qj9da64k' target=\"_blank\">rich-hill-33</a></strong> to <a href='https://wandb.ai/jcrich/collaborative%20filter%20model' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jcrich/collaborative%20filter%20model' target=\"_blank\">https://wandb.ai/jcrich/collaborative%20filter%20model</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jcrich/collaborative%20filter%20model/runs/qj9da64k' target=\"_blank\">https://wandb.ai/jcrich/collaborative%20filter%20model/runs/qj9da64k</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/jcrich/collaborative%20filter%20model/runs/qj9da64k?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fbf6cc3b5b0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "import random\n",
    "\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    entity=\"jcrich\",\n",
    "    project=\"collaborative filter model\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"architecture\": \"collaborative filter\",\n",
    "    \"dataset\": \"imdb\",\n",
    "    \"epochs\": EPOCHS,\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4ef218b7-74b1-4769-9175-cfc7a9fa2f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 226.77it/s]\n",
      "Epoch 1: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 191.39it/s]\n",
      "Epoch 2: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 347.32it/s]\n",
      "Epoch 3: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 282.05it/s]\n",
      "Epoch 4: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 288.07it/s]\n",
      "Epoch 5: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 320.43it/s]\n",
      "Epoch 6: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 318.18it/s]\n",
      "Epoch 7: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 328.79it/s]\n",
      "Epoch 8: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 350.45it/s]\n",
      "Epoch 9: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 281.45it/s]\n",
      "Epoch 10: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 357.75it/s]\n",
      "Epoch 11: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 344.90it/s]\n",
      "Epoch 12: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 297.07it/s]\n",
      "Epoch 13: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 319.98it/s]\n",
      "Epoch 14: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 363.79it/s]\n",
      "Epoch 15: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 335.08it/s]\n",
      "Epoch 16: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 310.61it/s]\n",
      "Epoch 17: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 243.44it/s]\n",
      "Epoch 18: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 342.98it/s]\n",
      "Epoch 19: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 320.72it/s]\n",
      "Epoch 20: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 338.55it/s]\n",
      "Epoch 21: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 344.04it/s]\n",
      "Epoch 22: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 376.97it/s]\n",
      "Epoch 23: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 315.67it/s]\n",
      "Epoch 24: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 318.26it/s]\n",
      "Epoch 25: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 314.46it/s]\n",
      "Epoch 26: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 340.30it/s]\n",
      "Epoch 27: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 291.15it/s]\n",
      "Epoch 28: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 287.08it/s]\n",
      "Epoch 29: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 157.72it/s]\n",
      "Epoch 30: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 276.27it/s]\n",
      "Epoch 31: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 240.07it/s]\n",
      "Epoch 32: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 297.18it/s]\n",
      "Epoch 33: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 277.45it/s]\n",
      "Epoch 34: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 271.17it/s]\n",
      "Epoch 35: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 316.48it/s]\n",
      "Epoch 36: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 285.57it/s]\n",
      "Epoch 37: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 287.90it/s]\n",
      "Epoch 38: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 274.98it/s]\n",
      "Epoch 39: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 261.62it/s]\n",
      "Epoch 40: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 331.95it/s]\n",
      "Epoch 41: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 284.15it/s]\n",
      "Epoch 42: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 319.64it/s]\n",
      "Epoch 43: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 319.01it/s]\n",
      "Epoch 44: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 315.49it/s]\n",
      "Epoch 45: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 277.57it/s]\n",
      "Epoch 46: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 287.61it/s]\n",
      "Epoch 47: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 293.78it/s]\n",
      "Epoch 48: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 278.26it/s]\n",
      "Epoch 49: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 338.42it/s]\n",
      "Epoch 50: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 339.02it/s]\n",
      "Epoch 51: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 236.01it/s]\n",
      "Epoch 52: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 361.57it/s]\n",
      "Epoch 53: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 173.13it/s]\n",
      "Epoch 54: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 223.02it/s]\n",
      "Epoch 55: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 231.41it/s]\n",
      "Epoch 56: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 324.93it/s]\n",
      "Epoch 57: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 309.31it/s]\n",
      "Epoch 58: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 299.08it/s]\n",
      "Epoch 59: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 355.59it/s]\n",
      "Epoch 60: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 328.59it/s]\n",
      "Epoch 61: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 273.56it/s]\n",
      "Epoch 62: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 342.96it/s]\n",
      "Epoch 63: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 309.91it/s]\n",
      "Epoch 64: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 295.34it/s]\n",
      "Epoch 65: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 244.73it/s]\n",
      "Epoch 66: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 201.77it/s]\n",
      "Epoch 67: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 322.40it/s]\n",
      "Epoch 68: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 318.20it/s]\n",
      "Epoch 69: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 283.18it/s]\n",
      "Epoch 70: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 306.06it/s]\n",
      "Epoch 71: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 259.18it/s]\n",
      "Epoch 72: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 247.02it/s]\n",
      "Epoch 73: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 272.60it/s]\n",
      "Epoch 74: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 252.43it/s]\n",
      "Epoch 75: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 278.54it/s]\n",
      "Epoch 76: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 291.54it/s]\n",
      "Epoch 77: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 303.09it/s]\n",
      "Epoch 78: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 279.90it/s]\n",
      "Epoch 79: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 305.56it/s]\n",
      "Epoch 80: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 328.42it/s]\n",
      "Epoch 81: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 325.86it/s]\n",
      "Epoch 82: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 350.00it/s]\n",
      "Epoch 83: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 335.12it/s]\n",
      "Epoch 84: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 285.03it/s]\n",
      "Epoch 85: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 255.80it/s]\n",
      "Epoch 86: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 301.23it/s]\n",
      "Epoch 87: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 320.46it/s]\n",
      "Epoch 88: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 331.24it/s]\n",
      "Epoch 89: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 333.17it/s]\n",
      "Epoch 90: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 320.04it/s]\n",
      "Epoch 91: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 326.76it/s]\n",
      "Epoch 92: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 318.53it/s]\n",
      "Epoch 93: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 254.69it/s]\n",
      "Epoch 94: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 279.53it/s]\n",
      "Epoch 95: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 301.87it/s]\n",
      "Epoch 96: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 314.06it/s]\n",
      "Epoch 97: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 272.62it/s]\n",
      "Epoch 98: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 307.98it/s]\n",
      "Epoch 99: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 241.79it/s]\n",
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>Training Loss</td><td>█▅▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Validation Loss</td><td>█▅▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>99</td></tr><tr><td>Training Loss</td><td>0.0598</td></tr><tr><td>Validation Loss</td><td>0.09444</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">rich-hill-33</strong> at: <a href='https://wandb.ai/jcrich/collaborative%20filter%20model/runs/qj9da64k' target=\"_blank\">https://wandb.ai/jcrich/collaborative%20filter%20model/runs/qj9da64k</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240311_221135-qj9da64k/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize lists to keep track of losses and epochs\n",
    "# Initialize MLflow run\n",
    "# Initialize MLflow run\n",
    "# Initialize MLflow run\n",
    "\n",
    "train_losses = []\n",
    "validation_losses = []\n",
    "epochs = []\n",
    "\n",
    "# Set the best validation loss to infinity at the start\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_epoch_loss = 0\n",
    "    for batch_idx, (user_ids, movie_ids, ratings) in tqdm(enumerate(train_data_loader), total=len(train_data_loader), desc=f'Epoch {epoch}'):\n",
    "\n",
    "        optimiser.zero_grad()\n",
    "        predictions = model(user_ids, movie_ids)\n",
    "        loss = criterion(predictions, ratings)\n",
    "\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        \n",
    "        batch_loss = loss.item()\n",
    "        total_epoch_loss += batch_loss\n",
    "        \n",
    "    # After all batches, calculate average loss for the epoch\n",
    "    avg_epoch_loss = total_epoch_loss / len(train_data_loader)\n",
    "    train_losses.append(avg_epoch_loss)\n",
    "    epochs.append(epoch)\n",
    "    \n",
    "    # Log training loss for the current epoch\n",
    "    wandb.log({\"Training Loss\": avg_epoch_loss, \"Epoch\": epoch})\n",
    "    mlflow.log_metric(\"Training Loss\", avg_epoch_loss, step=epoch)\n",
    "\n",
    "    # Start validation phase\n",
    "    model.eval()\n",
    "    total_validation_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (user_ids, movie_ids, ratings) in enumerate(validation_data_loader):\n",
    "            predictions = model(user_ids, movie_ids)\n",
    "            loss = criterion(predictions, ratings)\n",
    "            total_validation_loss += loss.item()\n",
    "    \n",
    "    # Calculate and log validation loss after the epoch\n",
    "    avg_validation_loss = total_validation_loss / len(validation_data_loader)\n",
    "    validation_losses.append(avg_validation_loss)\n",
    "\n",
    "    # Save the model if this epoch has the best validation loss so far\n",
    "    if avg_validation_loss < best_val_loss:\n",
    "        best_val_loss = avg_validation_loss\n",
    "        mlflow.log_metric(\"Best Validation Loss\", best_val_loss, step=epoch)\n",
    "        mlflow.pytorch.log_model(model, \"model\")\n",
    "        # Save model state\n",
    "        torch.save(model.state_dict(), 'best_model_state.pth')\n",
    "        # If you also want to save the optimizer state along with the model:\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimiser_state_dict': optimiser.state_dict(),\n",
    "            'loss': avg_validation_loss,\n",
    "        }, 'best_col_model_checkpoint.pth')\n",
    "        # Log the model checkpoint as an artifact\n",
    "        mlflow.log_artifact('best_col_model_checkpoint.pth')\n",
    "    \n",
    "    # Log validation loss for the current epoch\n",
    "    wandb.log({\"Validation Loss\": avg_validation_loss, \"Epoch\": epoch})\n",
    "    mlflow.log_metric(\"Validation Loss\", avg_validation_loss, step=epoch)\n",
    "    \n",
    "# Finish the Weights & Biases run\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9ede2c71-6995-4fd5-a657-e15996945cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09750499393333469\n"
     ]
    }
   ],
   "source": [
    "#test data\n",
    "model.eval()  # Switch to evaluation mode\n",
    "test_loss = 0\n",
    "correct_predictions = 0  # Fixed variable name for clarity\n",
    "total_contexts = 0\n",
    "\n",
    "with torch.no_grad():  # Disable gradient computation\n",
    "    for user_ids, movie_ids, ratings in test_data_loader:\n",
    "        predictions = model(user_ids,movie_ids)  # Generate predictions\n",
    "        # Calculate and accumulate loss\n",
    "        loss = criterion(predictions, ratings)\n",
    "        test_loss += loss.item()\n",
    "        \n",
    "        # # Reshape predictions to match [batch_size, context_size, vocab_size]\n",
    "        # predictions = predictions.view(-1, context_size, VOCAB_SIZE)\n",
    "        \n",
    "        # # Get top prediction for each context position\n",
    "        # top_predictions = predictions.argmax(dim=2)\n",
    "        \n",
    "        # # Calculate correct predictions\n",
    "        # correct_preds = (top_predictions == context).float().sum()\n",
    "        # correct_predictions += correct_preds.item()  # Accumulate correct predictions\n",
    "        \n",
    "        # total_contexts += context.numel()  # Total number of context word positions evaluated\n",
    "\n",
    "# Calculate final metrics\n",
    "test_loss /= len(test_data_loader)  # Average loss per batch\n",
    "# print('correct predictions = ',correct_predictions)\n",
    "# print('out of  = ',total_contexts)\n",
    "# accuracy = correct_predictions / total_contexts  # Compute accuracy\n",
    "\n",
    "# print(f'Test set: Average loss: {test_loss:.4f}, Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Evaluate the model\n",
    "# (This would involve using a separate validation set or performing cross-validation)\n",
    "print(test_loss)\n",
    "\n",
    "mlflow.log_metric('Post training test loss',test_loss)\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff24efc-8629-4242-8cce-6311cb1f9192",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8631385b-7cfe-427b-b2db-427241171649",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPLit\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Configuration\n",
    "num_folds = 10\n",
    "data_size = len(test_ds)  # Assuming 'dataset' is a PyTorch Dataset\n",
    "\n",
    "# Define the K-fold Cross Validator\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "31c8ce29-174d-4687-9476-a996d5a9ccfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optional: Set MLflow experiment\n",
    "mlflow.set_experiment(\"Finetuned Collaborative Filter\")\n",
    "# Log model parameters (example)\n",
    "mlflow.start_run()\n",
    "mlflow.log_param(\"epochs\", EPOCHS)\n",
    "mlflow.log_param(\"optimizer\", type(optimiser).__name__)\n",
    "mlflow.log_param(\"learning rate\", LEARNING_RATE)\n",
    "mlflow.log_param(\"batch size\", BATCH_SIZE)\n",
    "mlflow.log_param(\"L2 regularization\", L2_REGULARIZATION)\n",
    "mlflow.log_param(\"KFOLDS\", num_folds)\n",
    "mlflow.log_param(\"drop out\", 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b8e36fba-65b3-40c4-ad25-0e6b7b004449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.16.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/james/Desktop/torchex/movie-users/wandb/run-20240311_221428-qoalaxpy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jcrich/collaborative%20filter%20model/runs/qoalaxpy' target=\"_blank\">sage-waterfall-34</a></strong> to <a href='https://wandb.ai/jcrich/collaborative%20filter%20model' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jcrich/collaborative%20filter%20model' target=\"_blank\">https://wandb.ai/jcrich/collaborative%20filter%20model</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jcrich/collaborative%20filter%20model/runs/qoalaxpy' target=\"_blank\">https://wandb.ai/jcrich/collaborative%20filter%20model/runs/qoalaxpy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/jcrich/collaborative%20filter%20model/runs/qoalaxpy?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fbf42365e70>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    entity=\"jcrich\",\n",
    "    project=\"collaborative filter model\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"architecture\": \"collaborative filter\",\n",
    "    \"dataset\": \"letterboxd\",\n",
    "    \"epochs\": EPOCHS,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8b0830cd-43f1-454c-8e81-72e5d73549fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0\n",
      "--------------------------------\n",
      "Validation Loss for fold 0: 0.40004058678944904\n",
      "Validation Loss for fold 0: 0.3405859172344208\n",
      "Validation Loss for fold 0: 0.2523006349802017\n",
      "Validation Loss for fold 0: 0.19710467755794525\n",
      "Validation Loss for fold 0: 0.17236324648062387\n",
      "Validation Loss for fold 0: 0.18156878650188446\n",
      "Validation Loss for fold 0: 0.1494556019703547\n",
      "Validation Loss for fold 0: 0.14232215782006583\n",
      "Validation Loss for fold 0: 0.12940147270758948\n",
      "Validation Loss for fold 0: 0.1328766867518425\n",
      "Validation Loss for fold 0: 0.11800055454174678\n",
      "Validation Loss for fold 0: 0.11020211627086003\n",
      "Validation Loss for fold 0: 0.10912800580263138\n",
      "Validation Loss for fold 0: 0.10899850477774937\n",
      "Validation Loss for fold 0: 0.1046971728404363\n",
      "Validation Loss for fold 0: 0.11421542366345723\n",
      "Validation Loss for fold 0: 0.10135728617509206\n",
      "Validation Loss for fold 0: 0.09762210647265117\n",
      "Validation Loss for fold 0: 0.10419445981582005\n",
      "Validation Loss for fold 0: 0.1007133200764656\n",
      "Validation Loss for fold 0: 0.08899580190579097\n",
      "Validation Loss for fold 0: 0.10338961829741795\n",
      "Validation Loss for fold 0: 0.09615454574426015\n",
      "Validation Loss for fold 0: 0.09956108282009761\n",
      "Validation Loss for fold 0: 0.09147925923268001\n",
      "Validation Loss for fold 0: 0.09406893452008565\n",
      "Validation Loss for fold 0: 0.0951269119977951\n",
      "Validation Loss for fold 0: 0.09909753253062566\n",
      "Validation Loss for fold 0: 0.0867918332417806\n",
      "Validation Loss for fold 0: 0.09063327312469482\n",
      "Validation Loss for fold 0: 0.08596678823232651\n",
      "Validation Loss for fold 0: 0.08006745080153148\n",
      "Validation Loss for fold 0: 0.08382769177357356\n",
      "Validation Loss for fold 0: 0.09040863811969757\n",
      "Validation Loss for fold 0: 0.0831243246793747\n",
      "Validation Loss for fold 0: 0.08006356656551361\n",
      "Validation Loss for fold 0: 0.07841306552290916\n",
      "Validation Loss for fold 0: 0.08383367210626602\n",
      "Validation Loss for fold 0: 0.08961676061153412\n",
      "Validation Loss for fold 0: 0.08151192218065262\n",
      "Validation Loss for fold 0: 0.08321217944224675\n",
      "Validation Loss for fold 0: 0.0802933896581332\n",
      "Validation Loss for fold 0: 0.08187379936377208\n",
      "Validation Loss for fold 0: 0.07891201724608739\n",
      "Validation Loss for fold 0: 0.07870906094710033\n",
      "Validation Loss for fold 0: 0.07384126509229343\n",
      "Validation Loss for fold 0: 0.08660399417082469\n",
      "Validation Loss for fold 0: 0.08777070045471191\n",
      "Validation Loss for fold 0: 0.07109983637928963\n",
      "Validation Loss for fold 0: 0.082614965736866\n",
      "Validation Loss for fold 0: 0.08199499795834224\n",
      "Validation Loss for fold 0: 0.07593721896409988\n",
      "Validation Loss for fold 0: 0.07569479445616405\n",
      "Validation Loss for fold 0: 0.0804206704099973\n",
      "Validation Loss for fold 0: 0.07588233053684235\n",
      "Validation Loss for fold 0: 0.07882135609785716\n",
      "Validation Loss for fold 0: 0.07563282549381256\n",
      "Validation Loss for fold 0: 0.0664745420217514\n",
      "Validation Loss for fold 0: 0.0703739399711291\n",
      "Validation Loss for fold 0: 0.07261369129021962\n",
      "Validation Loss for fold 0: 0.07516742994387944\n",
      "Validation Loss for fold 0: 0.06901479636629422\n",
      "Validation Loss for fold 0: 0.07059687872727712\n",
      "Validation Loss for fold 0: 0.0733090192079544\n",
      "Validation Loss for fold 0: 0.07170218725999196\n",
      "Validation Loss for fold 0: 0.07674896096189816\n",
      "Validation Loss for fold 0: 0.06769139443834622\n",
      "Validation Loss for fold 0: 0.070797232290109\n",
      "Validation Loss for fold 0: 0.06980129579703014\n",
      "Validation Loss for fold 0: 0.06573322042822838\n",
      "Validation Loss for fold 0: 0.06680475920438766\n",
      "Validation Loss for fold 0: 0.07004865383108456\n",
      "Validation Loss for fold 0: 0.06677345186471939\n",
      "Validation Loss for fold 0: 0.06902672350406647\n",
      "Validation Loss for fold 0: 0.07091784725586574\n",
      "Validation Loss for fold 0: 0.069070170323054\n",
      "Validation Loss for fold 0: 0.06149250393112501\n",
      "Validation Loss for fold 0: 0.05828159675002098\n",
      "Validation Loss for fold 0: 0.06515669698516528\n",
      "Validation Loss for fold 0: 0.0709163174033165\n",
      "Validation Loss for fold 0: 0.06353060528635979\n",
      "Validation Loss for fold 0: 0.06689856946468353\n",
      "Validation Loss for fold 0: 0.06486580272515614\n",
      "Validation Loss for fold 0: 0.06374543905258179\n",
      "Validation Loss for fold 0: 0.06499384840329488\n",
      "Validation Loss for fold 0: 0.06658828631043434\n",
      "Validation Loss for fold 0: 0.05818859115242958\n",
      "Validation Loss for fold 0: 0.06583161900440852\n",
      "Validation Loss for fold 0: 0.07152313987414043\n",
      "Validation Loss for fold 0: 0.0644659089545409\n",
      "Validation Loss for fold 0: 0.060108013451099396\n",
      "Validation Loss for fold 0: 0.05678878600398699\n",
      "Validation Loss for fold 0: 0.06220446899533272\n",
      "Validation Loss for fold 0: 0.06230149045586586\n",
      "Validation Loss for fold 0: 0.06612609202663104\n",
      "Validation Loss for fold 0: 0.06523856148123741\n",
      "Validation Loss for fold 0: 0.06355303277571996\n",
      "Validation Loss for fold 0: 0.07061976566910744\n",
      "Validation Loss for fold 0: 0.054758613308270775\n",
      "Validation Loss for fold 0: 0.06616035476326942\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Validation Loss for fold 1: 0.44452271858851117\n",
      "Validation Loss for fold 1: 0.4096940755844116\n",
      "Validation Loss for fold 1: 0.38448312878608704\n",
      "Validation Loss for fold 1: 0.301165630420049\n",
      "Validation Loss for fold 1: 0.2823963661988576\n",
      "Validation Loss for fold 1: 0.2655285447835922\n",
      "Validation Loss for fold 1: 0.23780547082424164\n",
      "Validation Loss for fold 1: 0.23141702016194662\n",
      "Validation Loss for fold 1: 0.2153445283571879\n",
      "Validation Loss for fold 1: 0.2013364632924398\n",
      "Validation Loss for fold 1: 0.18469765782356262\n",
      "Validation Loss for fold 1: 0.18474740286668143\n",
      "Validation Loss for fold 1: 0.17807199557622275\n",
      "Validation Loss for fold 1: 0.161181112130483\n",
      "Validation Loss for fold 1: 0.16933163503805795\n",
      "Validation Loss for fold 1: 0.1831007202466329\n",
      "Validation Loss for fold 1: 0.16581999758879343\n",
      "Validation Loss for fold 1: 0.15410979092121124\n",
      "Validation Loss for fold 1: 0.1480882614850998\n",
      "Validation Loss for fold 1: 0.14202553033828735\n",
      "Validation Loss for fold 1: 0.1433348978559176\n",
      "Validation Loss for fold 1: 0.12029457588990529\n",
      "Validation Loss for fold 1: 0.12405741959810257\n",
      "Validation Loss for fold 1: 0.13317442685365677\n",
      "Validation Loss for fold 1: 0.12550687293211618\n",
      "Validation Loss for fold 1: 0.1208804299434026\n",
      "Validation Loss for fold 1: 0.11619913081328075\n",
      "Validation Loss for fold 1: 0.12623310337464014\n",
      "Validation Loss for fold 1: 0.11741331964731216\n",
      "Validation Loss for fold 1: 0.11509684721628825\n",
      "Validation Loss for fold 1: 0.10768392930428188\n",
      "Validation Loss for fold 1: 0.10701412210861842\n",
      "Validation Loss for fold 1: 0.11258043597141902\n",
      "Validation Loss for fold 1: 0.09626744811733563\n",
      "Validation Loss for fold 1: 0.10513107726971309\n",
      "Validation Loss for fold 1: 0.10816954572995503\n",
      "Validation Loss for fold 1: 0.10869894921779633\n",
      "Validation Loss for fold 1: 0.10295184701681137\n",
      "Validation Loss for fold 1: 0.10148939738670985\n",
      "Validation Loss for fold 1: 0.09310683111349742\n",
      "Validation Loss for fold 1: 0.09385750939448674\n",
      "Validation Loss for fold 1: 0.09897624204556148\n",
      "Validation Loss for fold 1: 0.09540151556332906\n",
      "Validation Loss for fold 1: 0.0907902717590332\n",
      "Validation Loss for fold 1: 0.08432638024290402\n",
      "Validation Loss for fold 1: 0.09246848026911418\n",
      "Validation Loss for fold 1: 0.09164645771185558\n",
      "Validation Loss for fold 1: 0.08873194456100464\n",
      "Validation Loss for fold 1: 0.08555023123820622\n",
      "Validation Loss for fold 1: 0.08779859046141307\n",
      "Validation Loss for fold 1: 0.08137140050530434\n",
      "Validation Loss for fold 1: 0.07916780437032382\n",
      "Validation Loss for fold 1: 0.08361699680487315\n",
      "Validation Loss for fold 1: 0.09300542871157329\n",
      "Validation Loss for fold 1: 0.08520887543757756\n",
      "Validation Loss for fold 1: 0.07840243726968765\n",
      "Validation Loss for fold 1: 0.08456550041834514\n",
      "Validation Loss for fold 1: 0.08031893024841945\n",
      "Validation Loss for fold 1: 0.08627938727537791\n",
      "Validation Loss for fold 1: 0.07756278042991956\n",
      "Validation Loss for fold 1: 0.08104993899663289\n",
      "Validation Loss for fold 1: 0.0904655506213506\n",
      "Validation Loss for fold 1: 0.07613804563879967\n",
      "Validation Loss for fold 1: 0.07523639748493831\n",
      "Validation Loss for fold 1: 0.0779402107000351\n",
      "Validation Loss for fold 1: 0.07771040499210358\n",
      "Validation Loss for fold 1: 0.08202241112788518\n",
      "Validation Loss for fold 1: 0.07923527310291927\n",
      "Validation Loss for fold 1: 0.07563068717718124\n",
      "Validation Loss for fold 1: 0.06871765355269115\n",
      "Validation Loss for fold 1: 0.0785686622063319\n",
      "Validation Loss for fold 1: 0.07168782378236453\n",
      "Validation Loss for fold 1: 0.08217155188322067\n",
      "Validation Loss for fold 1: 0.075304480890433\n",
      "Validation Loss for fold 1: 0.07649533823132515\n",
      "Validation Loss for fold 1: 0.07660459727048874\n",
      "Validation Loss for fold 1: 0.07291436195373535\n",
      "Validation Loss for fold 1: 0.07079595824082692\n",
      "Validation Loss for fold 1: 0.07178587218125661\n",
      "Validation Loss for fold 1: 0.07672414183616638\n",
      "Validation Loss for fold 1: 0.07573158293962479\n",
      "Validation Loss for fold 1: 0.07492047796646754\n",
      "Validation Loss for fold 1: 0.06958010296026866\n",
      "Validation Loss for fold 1: 0.07624814162651698\n",
      "Validation Loss for fold 1: 0.06766688947876294\n",
      "Validation Loss for fold 1: 0.07086131721735\n",
      "Validation Loss for fold 1: 0.07219489912192027\n",
      "Validation Loss for fold 1: 0.0698552926381429\n",
      "Validation Loss for fold 1: 0.06461711103717487\n",
      "Validation Loss for fold 1: 0.0737968347966671\n",
      "Validation Loss for fold 1: 0.06977924704551697\n",
      "Validation Loss for fold 1: 0.06124073142806689\n",
      "Validation Loss for fold 1: 0.06734047457575798\n",
      "Validation Loss for fold 1: 0.06749686102072398\n",
      "Validation Loss for fold 1: 0.07960302506883939\n",
      "Validation Loss for fold 1: 0.06385562444726627\n",
      "Validation Loss for fold 1: 0.06880827620625496\n",
      "Validation Loss for fold 1: 0.06632442027330399\n",
      "Validation Loss for fold 1: 0.06698019181688626\n",
      "Validation Loss for fold 1: 0.06771531576911609\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Validation Loss for fold 2: 0.5372051497300466\n",
      "Validation Loss for fold 2: 0.39200631777445477\n",
      "Validation Loss for fold 2: 0.2838432937860489\n",
      "Validation Loss for fold 2: 0.24631219108899435\n",
      "Validation Loss for fold 2: 0.21207253138224283\n",
      "Validation Loss for fold 2: 0.1862680564324061\n",
      "Validation Loss for fold 2: 0.17930781344572702\n",
      "Validation Loss for fold 2: 0.1628252093990644\n",
      "Validation Loss for fold 2: 0.14186276495456696\n",
      "Validation Loss for fold 2: 0.15046370526154837\n",
      "Validation Loss for fold 2: 0.1469054619471232\n",
      "Validation Loss for fold 2: 0.14551895608504614\n",
      "Validation Loss for fold 2: 0.12960636615753174\n",
      "Validation Loss for fold 2: 0.12048973888158798\n",
      "Validation Loss for fold 2: 0.12609416246414185\n",
      "Validation Loss for fold 2: 0.12800617267688116\n",
      "Validation Loss for fold 2: 0.1342299977938334\n",
      "Validation Loss for fold 2: 0.13122650980949402\n",
      "Validation Loss for fold 2: 0.12236213187376659\n",
      "Validation Loss for fold 2: 0.12755909065405527\n",
      "Validation Loss for fold 2: 0.13127715388933817\n",
      "Validation Loss for fold 2: 0.12288111696640651\n",
      "Validation Loss for fold 2: 0.1314347138007482\n",
      "Validation Loss for fold 2: 0.1144244372844696\n",
      "Validation Loss for fold 2: 0.12331688900788625\n",
      "Validation Loss for fold 2: 0.1286205599705378\n",
      "Validation Loss for fold 2: 0.10645135243733723\n",
      "Validation Loss for fold 2: 0.11340394119421641\n",
      "Validation Loss for fold 2: 0.11130802830060323\n",
      "Validation Loss for fold 2: 0.11521378656228383\n",
      "Validation Loss for fold 2: 0.1161256010333697\n",
      "Validation Loss for fold 2: 0.10627915461858113\n",
      "Validation Loss for fold 2: 0.1041449432571729\n",
      "Validation Loss for fold 2: 0.11544647067785263\n",
      "Validation Loss for fold 2: 0.11110991487900417\n",
      "Validation Loss for fold 2: 0.11036502321561177\n",
      "Validation Loss for fold 2: 0.12469537556171417\n",
      "Validation Loss for fold 2: 0.1068071077267329\n",
      "Validation Loss for fold 2: 0.1059640496969223\n",
      "Validation Loss for fold 2: 0.11290769279003143\n",
      "Validation Loss for fold 2: 0.10644898563623428\n",
      "Validation Loss for fold 2: 0.0985330839951833\n",
      "Validation Loss for fold 2: 0.11041756470998128\n",
      "Validation Loss for fold 2: 0.09551676859458287\n",
      "Validation Loss for fold 2: 0.09710093090931575\n",
      "Validation Loss for fold 2: 0.11035110056400299\n",
      "Validation Loss for fold 2: 0.10963208725055058\n",
      "Validation Loss for fold 2: 0.10574241975943248\n",
      "Validation Loss for fold 2: 0.09699490418036778\n",
      "Validation Loss for fold 2: 0.10164167732000351\n",
      "Validation Loss for fold 2: 0.10961844275395076\n",
      "Validation Loss for fold 2: 0.10575473308563232\n",
      "Validation Loss for fold 2: 0.09963578234116237\n",
      "Validation Loss for fold 2: 0.10270149757464726\n",
      "Validation Loss for fold 2: 0.09178810318311055\n",
      "Validation Loss for fold 2: 0.09913509339094162\n",
      "Validation Loss for fold 2: 0.09629920870065689\n",
      "Validation Loss for fold 2: 0.09410758068164189\n",
      "Validation Loss for fold 2: 0.10776946693658829\n",
      "Validation Loss for fold 2: 0.0981927290558815\n",
      "Validation Loss for fold 2: 0.09812931219736735\n",
      "Validation Loss for fold 2: 0.09732569257418315\n",
      "Validation Loss for fold 2: 0.09572599083185196\n",
      "Validation Loss for fold 2: 0.09251345445712407\n",
      "Validation Loss for fold 2: 0.09514168401559193\n",
      "Validation Loss for fold 2: 0.09868161876996358\n",
      "Validation Loss for fold 2: 0.09020627041657765\n",
      "Validation Loss for fold 2: 0.08840579787890117\n",
      "Validation Loss for fold 2: 0.08682935684919357\n",
      "Validation Loss for fold 2: 0.09110374252001445\n",
      "Validation Loss for fold 2: 0.08602715531984965\n",
      "Validation Loss for fold 2: 0.09642713516950607\n",
      "Validation Loss for fold 2: 0.08909405022859573\n",
      "Validation Loss for fold 2: 0.08973326782385509\n",
      "Validation Loss for fold 2: 0.08944397419691086\n",
      "Validation Loss for fold 2: 0.08386057863632838\n",
      "Validation Loss for fold 2: 0.08933278918266296\n",
      "Validation Loss for fold 2: 0.08505549778540929\n",
      "Validation Loss for fold 2: 0.0910780131816864\n",
      "Validation Loss for fold 2: 0.09107526143391927\n",
      "Validation Loss for fold 2: 0.08804026991128922\n",
      "Validation Loss for fold 2: 0.0847651834289233\n",
      "Validation Loss for fold 2: 0.08719607690970103\n",
      "Validation Loss for fold 2: 0.08475847542285919\n",
      "Validation Loss for fold 2: 0.09274858236312866\n",
      "Validation Loss for fold 2: 0.08730013916889827\n",
      "Validation Loss for fold 2: 0.08540250609318416\n",
      "Validation Loss for fold 2: 0.085378497838974\n",
      "Validation Loss for fold 2: 0.087473563849926\n",
      "Validation Loss for fold 2: 0.08169521888097127\n",
      "Validation Loss for fold 2: 0.08879451205333073\n",
      "Validation Loss for fold 2: 0.08352644741535187\n",
      "Validation Loss for fold 2: 0.08440390229225159\n",
      "Validation Loss for fold 2: 0.08650567879279454\n",
      "Validation Loss for fold 2: 0.07959456990162532\n",
      "Validation Loss for fold 2: 0.07956935216983159\n",
      "Validation Loss for fold 2: 0.08557830502589543\n",
      "Validation Loss for fold 2: 0.07962997009356816\n",
      "Validation Loss for fold 2: 0.08293161292870839\n",
      "Validation Loss for fold 2: 0.07453505198160808\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "Validation Loss for fold 3: 0.3264773488044739\n",
      "Validation Loss for fold 3: 0.31192896763483685\n",
      "Validation Loss for fold 3: 0.2298002988100052\n",
      "Validation Loss for fold 3: 0.2236704578002294\n",
      "Validation Loss for fold 3: 0.1905445009469986\n",
      "Validation Loss for fold 3: 0.18540443976720175\n",
      "Validation Loss for fold 3: 0.16110273202260336\n",
      "Validation Loss for fold 3: 0.15450112024943033\n",
      "Validation Loss for fold 3: 0.1455028752485911\n",
      "Validation Loss for fold 3: 0.13661078612009683\n",
      "Validation Loss for fold 3: 0.14494375387827554\n",
      "Validation Loss for fold 3: 0.1346416026353836\n",
      "Validation Loss for fold 3: 0.1235427384575208\n",
      "Validation Loss for fold 3: 0.12394347290198009\n",
      "Validation Loss for fold 3: 0.11499051501353581\n",
      "Validation Loss for fold 3: 0.12268369893232982\n",
      "Validation Loss for fold 3: 0.11209672192732494\n",
      "Validation Loss for fold 3: 0.10713520894447963\n",
      "Validation Loss for fold 3: 0.11699160933494568\n",
      "Validation Loss for fold 3: 0.10307196031014125\n",
      "Validation Loss for fold 3: 0.10464881360530853\n",
      "Validation Loss for fold 3: 0.1082408328851064\n",
      "Validation Loss for fold 3: 0.10447198897600174\n",
      "Validation Loss for fold 3: 0.10495637605587642\n",
      "Validation Loss for fold 3: 0.09722138941287994\n",
      "Validation Loss for fold 3: 0.10109105954567592\n",
      "Validation Loss for fold 3: 0.08294324949383736\n",
      "Validation Loss for fold 3: 0.09417317310969035\n",
      "Validation Loss for fold 3: 0.10589835792779922\n",
      "Validation Loss for fold 3: 0.10020012905200322\n",
      "Validation Loss for fold 3: 0.0942038968205452\n",
      "Validation Loss for fold 3: 0.09500497579574585\n",
      "Validation Loss for fold 3: 0.09607807795206706\n",
      "Validation Loss for fold 3: 0.09538701921701431\n",
      "Validation Loss for fold 3: 0.08602090924978256\n",
      "Validation Loss for fold 3: 0.08851839850346248\n",
      "Validation Loss for fold 3: 0.0840286985039711\n",
      "Validation Loss for fold 3: 0.08943931013345718\n",
      "Validation Loss for fold 3: 0.08708599706490834\n",
      "Validation Loss for fold 3: 0.07921379432082176\n",
      "Validation Loss for fold 3: 0.0819530760248502\n",
      "Validation Loss for fold 3: 0.08964849015076955\n",
      "Validation Loss for fold 3: 0.08809217313925426\n",
      "Validation Loss for fold 3: 0.09553350880742073\n",
      "Validation Loss for fold 3: 0.092611163854599\n",
      "Validation Loss for fold 3: 0.07657291988531749\n",
      "Validation Loss for fold 3: 0.08118822673956554\n",
      "Validation Loss for fold 3: 0.07962437967459361\n",
      "Validation Loss for fold 3: 0.07321795572837193\n",
      "Validation Loss for fold 3: 0.0866862138112386\n",
      "Validation Loss for fold 3: 0.08308204760154088\n",
      "Validation Loss for fold 3: 0.08749208350976308\n",
      "Validation Loss for fold 3: 0.08128738651673\n",
      "Validation Loss for fold 3: 0.07869474217295647\n",
      "Validation Loss for fold 3: 0.0751393511891365\n",
      "Validation Loss for fold 3: 0.07607578486204147\n",
      "Validation Loss for fold 3: 0.08076213300228119\n",
      "Validation Loss for fold 3: 0.07211400071779887\n",
      "Validation Loss for fold 3: 0.08002043763796489\n",
      "Validation Loss for fold 3: 0.0742466722925504\n",
      "Validation Loss for fold 3: 0.07709229985872905\n",
      "Validation Loss for fold 3: 0.07194379220406215\n",
      "Validation Loss for fold 3: 0.07360421617825826\n",
      "Validation Loss for fold 3: 0.07444798201322556\n",
      "Validation Loss for fold 3: 0.07705885171890259\n",
      "Validation Loss for fold 3: 0.06849972034494083\n",
      "Validation Loss for fold 3: 0.06939681619405746\n",
      "Validation Loss for fold 3: 0.06371695920825005\n",
      "Validation Loss for fold 3: 0.07047507415215175\n",
      "Validation Loss for fold 3: 0.07593223700920741\n",
      "Validation Loss for fold 3: 0.07448155681292216\n",
      "Validation Loss for fold 3: 0.06903641546765964\n",
      "Validation Loss for fold 3: 0.07229126244783401\n",
      "Validation Loss for fold 3: 0.07290296256542206\n",
      "Validation Loss for fold 3: 0.07410865773757298\n",
      "Validation Loss for fold 3: 0.06574934472640355\n",
      "Validation Loss for fold 3: 0.07215471814076106\n",
      "Validation Loss for fold 3: 0.0781203384200732\n",
      "Validation Loss for fold 3: 0.07453662157058716\n",
      "Validation Loss for fold 3: 0.06638432790835698\n",
      "Validation Loss for fold 3: 0.06924434006214142\n",
      "Validation Loss for fold 3: 0.07136476039886475\n",
      "Validation Loss for fold 3: 0.07040914396444957\n",
      "Validation Loss for fold 3: 0.07709649701913197\n",
      "Validation Loss for fold 3: 0.07237051799893379\n",
      "Validation Loss for fold 3: 0.0624185415605704\n",
      "Validation Loss for fold 3: 0.07357580835620563\n",
      "Validation Loss for fold 3: 0.06239874909321467\n",
      "Validation Loss for fold 3: 0.06731171160936356\n",
      "Validation Loss for fold 3: 0.07123107214768727\n",
      "Validation Loss for fold 3: 0.05965572347243627\n",
      "Validation Loss for fold 3: 0.06433884302775066\n",
      "Validation Loss for fold 3: 0.06291487316290538\n",
      "Validation Loss for fold 3: 0.06622613469759624\n",
      "Validation Loss for fold 3: 0.06481010342637698\n",
      "Validation Loss for fold 3: 0.07144066070516904\n",
      "Validation Loss for fold 3: 0.06018774211406708\n",
      "Validation Loss for fold 3: 0.056463683024048805\n",
      "Validation Loss for fold 3: 0.06345908592144649\n",
      "Validation Loss for fold 3: 0.0665894386669\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "Validation Loss for fold 4: 0.7962493300437927\n",
      "Validation Loss for fold 4: 0.697428305943807\n",
      "Validation Loss for fold 4: 0.5398273766040802\n",
      "Validation Loss for fold 4: 0.44280679027239483\n",
      "Validation Loss for fold 4: 0.43430442611376446\n",
      "Validation Loss for fold 4: 0.36539597312609357\n",
      "Validation Loss for fold 4: 0.3358175953229268\n",
      "Validation Loss for fold 4: 0.2997806668281555\n",
      "Validation Loss for fold 4: 0.2963620622952779\n",
      "Validation Loss for fold 4: 0.2893210252126058\n",
      "Validation Loss for fold 4: 0.26462844510873157\n",
      "Validation Loss for fold 4: 0.25137090186278027\n",
      "Validation Loss for fold 4: 0.2536298930644989\n",
      "Validation Loss for fold 4: 0.25633951524893445\n",
      "Validation Loss for fold 4: 0.2546137521664302\n",
      "Validation Loss for fold 4: 0.22583822906017303\n",
      "Validation Loss for fold 4: 0.22581886251767477\n",
      "Validation Loss for fold 4: 0.22099535167217255\n",
      "Validation Loss for fold 4: 0.2285758157571157\n",
      "Validation Loss for fold 4: 0.21319941679636636\n",
      "Validation Loss for fold 4: 0.21361993749936423\n",
      "Validation Loss for fold 4: 0.1848052740097046\n",
      "Validation Loss for fold 4: 0.19311330715815225\n",
      "Validation Loss for fold 4: 0.18872908254464468\n",
      "Validation Loss for fold 4: 0.19706248243649802\n",
      "Validation Loss for fold 4: 0.17857280870278677\n",
      "Validation Loss for fold 4: 0.18213004370530447\n",
      "Validation Loss for fold 4: 0.17809502283732095\n",
      "Validation Loss for fold 4: 0.17829737067222595\n",
      "Validation Loss for fold 4: 0.19560501476128897\n",
      "Validation Loss for fold 4: 0.18400301039218903\n",
      "Validation Loss for fold 4: 0.17069779833157858\n",
      "Validation Loss for fold 4: 0.177351380387942\n",
      "Validation Loss for fold 4: 0.1729309062163035\n",
      "Validation Loss for fold 4: 0.16369856894016266\n",
      "Validation Loss for fold 4: 0.1681972543398539\n",
      "Validation Loss for fold 4: 0.1475997418165207\n",
      "Validation Loss for fold 4: 0.1580235262711843\n",
      "Validation Loss for fold 4: 0.1421719491481781\n",
      "Validation Loss for fold 4: 0.1473858878016472\n",
      "Validation Loss for fold 4: 0.15017116566499075\n",
      "Validation Loss for fold 4: 0.15180551509062448\n",
      "Validation Loss for fold 4: 0.15323888262112936\n",
      "Validation Loss for fold 4: 0.14285283784071603\n",
      "Validation Loss for fold 4: 0.14922201136747995\n",
      "Validation Loss for fold 4: 0.15002857148647308\n",
      "Validation Loss for fold 4: 0.15490551789601645\n",
      "Validation Loss for fold 4: 0.14129255712032318\n",
      "Validation Loss for fold 4: 0.13135193288326263\n",
      "Validation Loss for fold 4: 0.1399930715560913\n",
      "Validation Loss for fold 4: 0.15118918816248575\n",
      "Validation Loss for fold 4: 0.12794401248296103\n",
      "Validation Loss for fold 4: 0.13229641069968542\n",
      "Validation Loss for fold 4: 0.12803535411755243\n",
      "Validation Loss for fold 4: 0.137053944170475\n",
      "Validation Loss for fold 4: 0.14186045030752817\n",
      "Validation Loss for fold 4: 0.13584206501642862\n",
      "Validation Loss for fold 4: 0.1248583843310674\n",
      "Validation Loss for fold 4: 0.12975252668062845\n",
      "Validation Loss for fold 4: 0.12196071445941925\n",
      "Validation Loss for fold 4: 0.12536512315273285\n",
      "Validation Loss for fold 4: 0.12871060520410538\n",
      "Validation Loss for fold 4: 0.12491550296545029\n",
      "Validation Loss for fold 4: 0.12903838604688644\n",
      "Validation Loss for fold 4: 0.12960345298051834\n",
      "Validation Loss for fold 4: 0.11749395728111267\n",
      "Validation Loss for fold 4: 0.12551800906658173\n",
      "Validation Loss for fold 4: 0.11735657850901286\n",
      "Validation Loss for fold 4: 0.12208789587020874\n",
      "Validation Loss for fold 4: 0.11228278279304504\n",
      "Validation Loss for fold 4: 0.1072637215256691\n",
      "Validation Loss for fold 4: 0.1241371879975001\n",
      "Validation Loss for fold 4: 0.11924385031064351\n",
      "Validation Loss for fold 4: 0.11326393485069275\n",
      "Validation Loss for fold 4: 0.1146310493350029\n",
      "Validation Loss for fold 4: 0.11433007071415584\n",
      "Validation Loss for fold 4: 0.10909648487965266\n",
      "Validation Loss for fold 4: 0.11729802191257477\n",
      "Validation Loss for fold 4: 0.117764746149381\n",
      "Validation Loss for fold 4: 0.11702179660399754\n",
      "Validation Loss for fold 4: 0.10650115460157394\n",
      "Validation Loss for fold 4: 0.10496889054775238\n",
      "Validation Loss for fold 4: 0.11355590571959813\n",
      "Validation Loss for fold 4: 0.10495838522911072\n",
      "Validation Loss for fold 4: 0.1008296509583791\n",
      "Validation Loss for fold 4: 0.1054953212539355\n",
      "Validation Loss for fold 4: 0.10690261671940486\n",
      "Validation Loss for fold 4: 0.10611581305662791\n",
      "Validation Loss for fold 4: 0.09713989744583766\n",
      "Validation Loss for fold 4: 0.10050746550162633\n",
      "Validation Loss for fold 4: 0.10346372425556183\n",
      "Validation Loss for fold 4: 0.10270684709151585\n",
      "Validation Loss for fold 4: 0.10440108676751454\n",
      "Validation Loss for fold 4: 0.09649171680212021\n",
      "Validation Loss for fold 4: 0.09668026119470596\n",
      "Validation Loss for fold 4: 0.09532532344261806\n",
      "Validation Loss for fold 4: 0.10706344991922379\n",
      "Validation Loss for fold 4: 0.10119475920995076\n",
      "Validation Loss for fold 4: 0.09499514102935791\n",
      "Validation Loss for fold 4: 0.09183313449223836\n",
      "--------------------------------\n",
      "FOLD 5\n",
      "--------------------------------\n",
      "Validation Loss for fold 5: 0.1682387093702952\n",
      "Validation Loss for fold 5: 0.16043045123418173\n",
      "Validation Loss for fold 5: 0.14383003612359366\n",
      "Validation Loss for fold 5: 0.14919538299242655\n",
      "Validation Loss for fold 5: 0.12375396738449733\n",
      "Validation Loss for fold 5: 0.11966141064961751\n",
      "Validation Loss for fold 5: 0.11883022387822469\n",
      "Validation Loss for fold 5: 0.12513716518878937\n",
      "Validation Loss for fold 5: 0.1204412505030632\n",
      "Validation Loss for fold 5: 0.09885490809877713\n",
      "Validation Loss for fold 5: 0.10627282162507375\n",
      "Validation Loss for fold 5: 0.09867240488529205\n",
      "Validation Loss for fold 5: 0.107551209628582\n",
      "Validation Loss for fold 5: 0.09592836101849873\n",
      "Validation Loss for fold 5: 0.09772762407859166\n",
      "Validation Loss for fold 5: 0.10118394841750462\n",
      "Validation Loss for fold 5: 0.09757585823535919\n",
      "Validation Loss for fold 5: 0.10279047240813573\n",
      "Validation Loss for fold 5: 0.09288238982359569\n",
      "Validation Loss for fold 5: 0.09213300049304962\n",
      "Validation Loss for fold 5: 0.09982989480098088\n",
      "Validation Loss for fold 5: 0.09820478906234105\n",
      "Validation Loss for fold 5: 0.08529604971408844\n",
      "Validation Loss for fold 5: 0.08965374777714412\n",
      "Validation Loss for fold 5: 0.09771198034286499\n",
      "Validation Loss for fold 5: 0.09764802952607472\n",
      "Validation Loss for fold 5: 0.08650114387273788\n",
      "Validation Loss for fold 5: 0.08236977209647496\n",
      "Validation Loss for fold 5: 0.09208365281422932\n",
      "Validation Loss for fold 5: 0.08278446396191914\n",
      "Validation Loss for fold 5: 0.08788013458251953\n",
      "Validation Loss for fold 5: 0.08821915090084076\n",
      "Validation Loss for fold 5: 0.07850806539257367\n",
      "Validation Loss for fold 5: 0.08156204720338185\n",
      "Validation Loss for fold 5: 0.09533824274937312\n",
      "Validation Loss for fold 5: 0.08392575879891713\n",
      "Validation Loss for fold 5: 0.08935000747442245\n",
      "Validation Loss for fold 5: 0.09207760294278462\n",
      "Validation Loss for fold 5: 0.08409071465333302\n",
      "Validation Loss for fold 5: 0.08728513618310292\n",
      "Validation Loss for fold 5: 0.09322487562894821\n",
      "Validation Loss for fold 5: 0.08482499917348225\n",
      "Validation Loss for fold 5: 0.07361942281325658\n",
      "Validation Loss for fold 5: 0.08054555952548981\n",
      "Validation Loss for fold 5: 0.08418308198451996\n",
      "Validation Loss for fold 5: 0.07667719945311546\n",
      "Validation Loss for fold 5: 0.08056110888719559\n",
      "Validation Loss for fold 5: 0.07995123912890752\n",
      "Validation Loss for fold 5: 0.08067082862059276\n",
      "Validation Loss for fold 5: 0.07870460549990337\n",
      "Validation Loss for fold 5: 0.08385971436897914\n",
      "Validation Loss for fold 5: 0.07514482984940211\n",
      "Validation Loss for fold 5: 0.0826716348528862\n",
      "Validation Loss for fold 5: 0.08209418505430222\n",
      "Validation Loss for fold 5: 0.08077709625164668\n",
      "Validation Loss for fold 5: 0.08298684904972713\n",
      "Validation Loss for fold 5: 0.07403699929515521\n",
      "Validation Loss for fold 5: 0.07864754398663838\n",
      "Validation Loss for fold 5: 0.08144632975260417\n",
      "Validation Loss for fold 5: 0.08045837779839833\n",
      "Validation Loss for fold 5: 0.07790692895650864\n",
      "Validation Loss for fold 5: 0.07609319190184276\n",
      "Validation Loss for fold 5: 0.0735165923833847\n",
      "Validation Loss for fold 5: 0.07136400789022446\n",
      "Validation Loss for fold 5: 0.07065353045860927\n",
      "Validation Loss for fold 5: 0.07170711954434712\n",
      "Validation Loss for fold 5: 0.0724666491150856\n",
      "Validation Loss for fold 5: 0.08532050748666127\n",
      "Validation Loss for fold 5: 0.0704113319516182\n",
      "Validation Loss for fold 5: 0.06980959326028824\n",
      "Validation Loss for fold 5: 0.07715933894117673\n",
      "Validation Loss for fold 5: 0.07103593771656354\n",
      "Validation Loss for fold 5: 0.06506795063614845\n",
      "Validation Loss for fold 5: 0.07750063886245091\n",
      "Validation Loss for fold 5: 0.07286570966243744\n",
      "Validation Loss for fold 5: 0.07569548115134239\n",
      "Validation Loss for fold 5: 0.06511940682927768\n",
      "Validation Loss for fold 5: 0.07479426264762878\n",
      "Validation Loss for fold 5: 0.06539431462685268\n",
      "Validation Loss for fold 5: 0.07363694037000339\n",
      "Validation Loss for fold 5: 0.07032688707113266\n",
      "Validation Loss for fold 5: 0.06851815183957417\n",
      "Validation Loss for fold 5: 0.06873949120442073\n",
      "Validation Loss for fold 5: 0.06345655148228009\n",
      "Validation Loss for fold 5: 0.06724327926834424\n",
      "Validation Loss for fold 5: 0.06666265428066254\n",
      "Validation Loss for fold 5: 0.06460347771644592\n",
      "Validation Loss for fold 5: 0.06663444638252258\n",
      "Validation Loss for fold 5: 0.06254717086752255\n",
      "Validation Loss for fold 5: 0.06585223351915677\n",
      "Validation Loss for fold 5: 0.06693548709154129\n",
      "Validation Loss for fold 5: 0.06669524560372035\n",
      "Validation Loss for fold 5: 0.06335326035817464\n",
      "Validation Loss for fold 5: 0.07046471784512202\n",
      "Validation Loss for fold 5: 0.06857092678546906\n",
      "Validation Loss for fold 5: 0.06109861905376116\n",
      "Validation Loss for fold 5: 0.0658230980237325\n",
      "Validation Loss for fold 5: 0.060477033257484436\n",
      "Validation Loss for fold 5: 0.05947397028406461\n",
      "Validation Loss for fold 5: 0.0650607521335284\n",
      "--------------------------------\n",
      "FOLD 6\n",
      "--------------------------------\n",
      "Validation Loss for fold 6: 0.529888391494751\n",
      "Validation Loss for fold 6: 0.40373878677686054\n",
      "Validation Loss for fold 6: 0.32116414109865826\n",
      "Validation Loss for fold 6: 0.23911980291207632\n",
      "Validation Loss for fold 6: 0.20589027802149454\n",
      "Validation Loss for fold 6: 0.18061946829160055\n",
      "Validation Loss for fold 6: 0.1623800347248713\n",
      "Validation Loss for fold 6: 0.14545336365699768\n",
      "Validation Loss for fold 6: 0.13427474598089853\n",
      "Validation Loss for fold 6: 0.12949189792076746\n",
      "Validation Loss for fold 6: 0.12347294886906941\n",
      "Validation Loss for fold 6: 0.1368702749411265\n",
      "Validation Loss for fold 6: 0.12098516275485356\n",
      "Validation Loss for fold 6: 0.10929152369499207\n",
      "Validation Loss for fold 6: 0.10922946780920029\n",
      "Validation Loss for fold 6: 0.10110120475292206\n",
      "Validation Loss for fold 6: 0.10488291084766388\n",
      "Validation Loss for fold 6: 0.10470672448476155\n",
      "Validation Loss for fold 6: 0.09744558731714885\n",
      "Validation Loss for fold 6: 0.09643909831841786\n",
      "Validation Loss for fold 6: 0.10452307264010112\n",
      "Validation Loss for fold 6: 0.09982579449812572\n",
      "Validation Loss for fold 6: 0.0939769372344017\n",
      "Validation Loss for fold 6: 0.09280472248792648\n",
      "Validation Loss for fold 6: 0.08887147903442383\n",
      "Validation Loss for fold 6: 0.09584680447975795\n",
      "Validation Loss for fold 6: 0.09145711610714595\n",
      "Validation Loss for fold 6: 0.09224765251080196\n",
      "Validation Loss for fold 6: 0.08768072972695033\n",
      "Validation Loss for fold 6: 0.0942516749103864\n",
      "Validation Loss for fold 6: 0.092050405840079\n",
      "Validation Loss for fold 6: 0.08752838025490443\n",
      "Validation Loss for fold 6: 0.08047239606579144\n",
      "Validation Loss for fold 6: 0.09458146244287491\n",
      "Validation Loss for fold 6: 0.08143347750107448\n",
      "Validation Loss for fold 6: 0.0844147155682246\n",
      "Validation Loss for fold 6: 0.0805891032020251\n",
      "Validation Loss for fold 6: 0.08416229486465454\n",
      "Validation Loss for fold 6: 0.0830472062031428\n",
      "Validation Loss for fold 6: 0.08490894238154094\n",
      "Validation Loss for fold 6: 0.07832229634126027\n",
      "Validation Loss for fold 6: 0.07570574184258778\n",
      "Validation Loss for fold 6: 0.07730866223573685\n",
      "Validation Loss for fold 6: 0.07507617274920146\n",
      "Validation Loss for fold 6: 0.07691334436337154\n",
      "Validation Loss for fold 6: 0.0840321679910024\n",
      "Validation Loss for fold 6: 0.07860405246416728\n",
      "Validation Loss for fold 6: 0.07575352986653645\n",
      "Validation Loss for fold 6: 0.07983465492725372\n",
      "Validation Loss for fold 6: 0.07513654480377834\n",
      "Validation Loss for fold 6: 0.07060313721497853\n",
      "Validation Loss for fold 6: 0.07891685763994853\n",
      "Validation Loss for fold 6: 0.07108587274948756\n",
      "Validation Loss for fold 6: 0.07583107054233551\n",
      "Validation Loss for fold 6: 0.07363448043664296\n",
      "Validation Loss for fold 6: 0.07168359806140263\n",
      "Validation Loss for fold 6: 0.07878897339105606\n",
      "Validation Loss for fold 6: 0.07167957971493404\n",
      "Validation Loss for fold 6: 0.07000746577978134\n",
      "Validation Loss for fold 6: 0.06655944262941678\n",
      "Validation Loss for fold 6: 0.07164455701907475\n",
      "Validation Loss for fold 6: 0.06544915338357289\n",
      "Validation Loss for fold 6: 0.06479538107911746\n",
      "Validation Loss for fold 6: 0.0756445899605751\n",
      "Validation Loss for fold 6: 0.06658363342285156\n",
      "Validation Loss for fold 6: 0.07292724152406056\n",
      "Validation Loss for fold 6: 0.060543028016885124\n",
      "Validation Loss for fold 6: 0.06236595908800761\n",
      "Validation Loss for fold 6: 0.06565608580907185\n",
      "Validation Loss for fold 6: 0.07003507514794667\n",
      "Validation Loss for fold 6: 0.06425584355990092\n",
      "Validation Loss for fold 6: 0.06503906846046448\n",
      "Validation Loss for fold 6: 0.06587592512369156\n",
      "Validation Loss for fold 6: 0.06332585960626602\n",
      "Validation Loss for fold 6: 0.05995282903313637\n",
      "Validation Loss for fold 6: 0.06351107855637868\n",
      "Validation Loss for fold 6: 0.0670912005007267\n",
      "Validation Loss for fold 6: 0.06542124971747398\n",
      "Validation Loss for fold 6: 0.0546588363746802\n",
      "Validation Loss for fold 6: 0.06081890190641085\n",
      "Validation Loss for fold 6: 0.06173399090766907\n",
      "Validation Loss for fold 6: 0.059478415797154106\n",
      "Validation Loss for fold 6: 0.05952209606766701\n",
      "Validation Loss for fold 6: 0.05946619311968485\n",
      "Validation Loss for fold 6: 0.06363118439912796\n",
      "Validation Loss for fold 6: 0.06073256333669027\n",
      "Validation Loss for fold 6: 0.05989420786499977\n",
      "Validation Loss for fold 6: 0.0684540867805481\n",
      "Validation Loss for fold 6: 0.06335810820261638\n",
      "Validation Loss for fold 6: 0.05499256650606791\n",
      "Validation Loss for fold 6: 0.06292776887615521\n",
      "Validation Loss for fold 6: 0.06314057608445485\n",
      "Validation Loss for fold 6: 0.05583457027872404\n",
      "Validation Loss for fold 6: 0.05787619079152743\n",
      "Validation Loss for fold 6: 0.05833416680494944\n",
      "Validation Loss for fold 6: 0.06172459696729978\n",
      "Validation Loss for fold 6: 0.059397282699743904\n",
      "Validation Loss for fold 6: 0.058398857712745667\n",
      "Validation Loss for fold 6: 0.05487025032440821\n",
      "Validation Loss for fold 6: 0.04931126100321611\n",
      "--------------------------------\n",
      "FOLD 7\n",
      "--------------------------------\n",
      "Validation Loss for fold 7: 0.5659770369529724\n",
      "Validation Loss for fold 7: 0.3916485905647278\n",
      "Validation Loss for fold 7: 0.29348517457644147\n",
      "Validation Loss for fold 7: 0.23017974694569907\n",
      "Validation Loss for fold 7: 0.18591944873332977\n",
      "Validation Loss for fold 7: 0.16646470626195273\n",
      "Validation Loss for fold 7: 0.1388069142897924\n",
      "Validation Loss for fold 7: 0.14237338801225027\n",
      "Validation Loss for fold 7: 0.1166743462284406\n",
      "Validation Loss for fold 7: 0.11336268732945125\n",
      "Validation Loss for fold 7: 0.11239916582902272\n",
      "Validation Loss for fold 7: 0.10309960941473643\n",
      "Validation Loss for fold 7: 0.10040856649478276\n",
      "Validation Loss for fold 7: 0.11148546387751897\n",
      "Validation Loss for fold 7: 0.09233942379554112\n",
      "Validation Loss for fold 7: 0.1013430804014206\n",
      "Validation Loss for fold 7: 0.08726302658518155\n",
      "Validation Loss for fold 7: 0.09845002740621567\n",
      "Validation Loss for fold 7: 0.08935015151898067\n",
      "Validation Loss for fold 7: 0.09403215845425923\n",
      "Validation Loss for fold 7: 0.09490889062484105\n",
      "Validation Loss for fold 7: 0.07925098141034444\n",
      "Validation Loss for fold 7: 0.0845915178457896\n",
      "Validation Loss for fold 7: 0.08349833140770595\n",
      "Validation Loss for fold 7: 0.09015727291504542\n",
      "Validation Loss for fold 7: 0.07999280591805775\n",
      "Validation Loss for fold 7: 0.07892741511265437\n",
      "Validation Loss for fold 7: 0.07464215283592542\n",
      "Validation Loss for fold 7: 0.07599355280399323\n",
      "Validation Loss for fold 7: 0.08044215043385823\n",
      "Validation Loss for fold 7: 0.0803733691573143\n",
      "Validation Loss for fold 7: 0.07505406190951665\n",
      "Validation Loss for fold 7: 0.08192420999209087\n",
      "Validation Loss for fold 7: 0.07986301183700562\n",
      "Validation Loss for fold 7: 0.07507761071125667\n",
      "Validation Loss for fold 7: 0.07748496284087499\n",
      "Validation Loss for fold 7: 0.08071741461753845\n",
      "Validation Loss for fold 7: 0.07537538061539333\n",
      "Validation Loss for fold 7: 0.07410808404286702\n",
      "Validation Loss for fold 7: 0.07671797772248586\n",
      "Validation Loss for fold 7: 0.07462241500616074\n",
      "Validation Loss for fold 7: 0.07331282148758571\n",
      "Validation Loss for fold 7: 0.07947033147017162\n",
      "Validation Loss for fold 7: 0.07089199374119441\n",
      "Validation Loss for fold 7: 0.07249582310517629\n",
      "Validation Loss for fold 7: 0.06686125695705414\n",
      "Validation Loss for fold 7: 0.07063667600353558\n",
      "Validation Loss for fold 7: 0.07561761140823364\n",
      "Validation Loss for fold 7: 0.06851700196663539\n",
      "Validation Loss for fold 7: 0.07018827895323436\n",
      "Validation Loss for fold 7: 0.07251135508219402\n",
      "Validation Loss for fold 7: 0.07486136506001155\n",
      "Validation Loss for fold 7: 0.0660048412779967\n",
      "Validation Loss for fold 7: 0.07664483785629272\n",
      "Validation Loss for fold 7: 0.06994876265525818\n",
      "Validation Loss for fold 7: 0.06858305384715398\n",
      "Validation Loss for fold 7: 0.06533030420541763\n",
      "Validation Loss for fold 7: 0.06626191486914952\n",
      "Validation Loss for fold 7: 0.06430515646934509\n",
      "Validation Loss for fold 7: 0.06655396521091461\n",
      "Validation Loss for fold 7: 0.06722425669431686\n",
      "Validation Loss for fold 7: 0.06569692865014076\n",
      "Validation Loss for fold 7: 0.06614400818943977\n",
      "Validation Loss for fold 7: 0.06532133122285207\n",
      "Validation Loss for fold 7: 0.06735384215911229\n",
      "Validation Loss for fold 7: 0.060039229691028595\n",
      "Validation Loss for fold 7: 0.0694946435590585\n",
      "Validation Loss for fold 7: 0.06271510322888692\n",
      "Validation Loss for fold 7: 0.06336352104942004\n",
      "Validation Loss for fold 7: 0.06420765320460002\n",
      "Validation Loss for fold 7: 0.06295951828360558\n",
      "Validation Loss for fold 7: 0.06291662032405536\n",
      "Validation Loss for fold 7: 0.06903582811355591\n",
      "Validation Loss for fold 7: 0.06384316459298134\n",
      "Validation Loss for fold 7: 0.060075584799051285\n",
      "Validation Loss for fold 7: 0.06334385275840759\n",
      "Validation Loss for fold 7: 0.0610148049890995\n",
      "Validation Loss for fold 7: 0.06879950314760208\n",
      "Validation Loss for fold 7: 0.06028543412685394\n",
      "Validation Loss for fold 7: 0.06118210032582283\n",
      "Validation Loss for fold 7: 0.06485525767008464\n",
      "Validation Loss for fold 7: 0.0637684812148412\n",
      "Validation Loss for fold 7: 0.062323227524757385\n",
      "Validation Loss for fold 7: 0.061283692717552185\n",
      "Validation Loss for fold 7: 0.06103351588050524\n",
      "Validation Loss for fold 7: 0.05908590058485667\n",
      "Validation Loss for fold 7: 0.0664221631983916\n",
      "Validation Loss for fold 7: 0.05785492683450381\n",
      "Validation Loss for fold 7: 0.0660451240837574\n",
      "Validation Loss for fold 7: 0.05864739790558815\n",
      "Validation Loss for fold 7: 0.054219299306472145\n",
      "Validation Loss for fold 7: 0.05961575359106064\n",
      "Validation Loss for fold 7: 0.05669612934192022\n",
      "Validation Loss for fold 7: 0.05912728731830915\n",
      "Validation Loss for fold 7: 0.05945221334695816\n",
      "Validation Loss for fold 7: 0.05830075219273567\n",
      "Validation Loss for fold 7: 0.06055325145522753\n",
      "Validation Loss for fold 7: 0.05980599050720533\n",
      "Validation Loss for fold 7: 0.06523073588808377\n",
      "Validation Loss for fold 7: 0.06604904557267825\n",
      "--------------------------------\n",
      "FOLD 8\n",
      "--------------------------------\n",
      "Validation Loss for fold 8: 0.3809284567832947\n",
      "Validation Loss for fold 8: 0.2966587245464325\n",
      "Validation Loss for fold 8: 0.2482734719912211\n",
      "Validation Loss for fold 8: 0.2138306846221288\n",
      "Validation Loss for fold 8: 0.1951213926076889\n",
      "Validation Loss for fold 8: 0.16548248628775278\n",
      "Validation Loss for fold 8: 0.14777299265066782\n",
      "Validation Loss for fold 8: 0.16302921871344248\n",
      "Validation Loss for fold 8: 0.14509633680184683\n",
      "Validation Loss for fold 8: 0.12395906448364258\n",
      "Validation Loss for fold 8: 0.11029280970493953\n",
      "Validation Loss for fold 8: 0.12925081451733908\n",
      "Validation Loss for fold 8: 0.13105999181667963\n",
      "Validation Loss for fold 8: 0.1190892035762469\n",
      "Validation Loss for fold 8: 0.12819675852855048\n",
      "Validation Loss for fold 8: 0.12480881313482921\n",
      "Validation Loss for fold 8: 0.11919095863898595\n",
      "Validation Loss for fold 8: 0.12533697485923767\n",
      "Validation Loss for fold 8: 0.11184047659238179\n",
      "Validation Loss for fold 8: 0.10509484509627025\n",
      "Validation Loss for fold 8: 0.121828888853391\n",
      "Validation Loss for fold 8: 0.11328530311584473\n",
      "Validation Loss for fold 8: 0.12081807355086009\n",
      "Validation Loss for fold 8: 0.09903862327337265\n",
      "Validation Loss for fold 8: 0.1081789309779803\n",
      "Validation Loss for fold 8: 0.11429975430170695\n",
      "Validation Loss for fold 8: 0.10958165923754375\n",
      "Validation Loss for fold 8: 0.09985081851482391\n",
      "Validation Loss for fold 8: 0.10489386816819508\n",
      "Validation Loss for fold 8: 0.1130415474375089\n",
      "Validation Loss for fold 8: 0.10611173262198766\n",
      "Validation Loss for fold 8: 0.11029592404762904\n",
      "Validation Loss for fold 8: 0.1073510671655337\n",
      "Validation Loss for fold 8: 0.10326958199342091\n",
      "Validation Loss for fold 8: 0.1041578675309817\n",
      "Validation Loss for fold 8: 0.0988343358039856\n",
      "Validation Loss for fold 8: 0.09700397898753484\n",
      "Validation Loss for fold 8: 0.09869447598854701\n",
      "Validation Loss for fold 8: 0.09308262914419174\n",
      "Validation Loss for fold 8: 0.09921915332476298\n",
      "Validation Loss for fold 8: 0.10083545992771785\n",
      "Validation Loss for fold 8: 0.0943722774585088\n",
      "Validation Loss for fold 8: 0.10067801922559738\n",
      "Validation Loss for fold 8: 0.08884635816017787\n",
      "Validation Loss for fold 8: 0.08875350654125214\n",
      "Validation Loss for fold 8: 0.08993098388115565\n",
      "Validation Loss for fold 8: 0.09886256853739421\n",
      "Validation Loss for fold 8: 0.09965918709834416\n",
      "Validation Loss for fold 8: 0.09193951388200124\n",
      "Validation Loss for fold 8: 0.0833101396759351\n",
      "Validation Loss for fold 8: 0.08794049421946208\n",
      "Validation Loss for fold 8: 0.09706706305344899\n",
      "Validation Loss for fold 8: 0.084981935719649\n",
      "Validation Loss for fold 8: 0.08702163149913152\n",
      "Validation Loss for fold 8: 0.09970247248808543\n",
      "Validation Loss for fold 8: 0.08841673284769058\n",
      "Validation Loss for fold 8: 0.08468314011891682\n",
      "Validation Loss for fold 8: 0.08834177752335866\n",
      "Validation Loss for fold 8: 0.0872041384379069\n",
      "Validation Loss for fold 8: 0.08907458931207657\n",
      "Validation Loss for fold 8: 0.098698856929938\n",
      "Validation Loss for fold 8: 0.0827787642677625\n",
      "Validation Loss for fold 8: 0.09704999377330144\n",
      "Validation Loss for fold 8: 0.0833943784236908\n",
      "Validation Loss for fold 8: 0.08493756006161372\n",
      "Validation Loss for fold 8: 0.0885130986571312\n",
      "Validation Loss for fold 8: 0.0823713168501854\n",
      "Validation Loss for fold 8: 0.07356086745858192\n",
      "Validation Loss for fold 8: 0.08612886071205139\n",
      "Validation Loss for fold 8: 0.08385962247848511\n",
      "Validation Loss for fold 8: 0.08927732209364574\n",
      "Validation Loss for fold 8: 0.08224598069985707\n",
      "Validation Loss for fold 8: 0.08069078375895818\n",
      "Validation Loss for fold 8: 0.0807917242248853\n",
      "Validation Loss for fold 8: 0.07122522592544556\n",
      "Validation Loss for fold 8: 0.0779375210404396\n",
      "Validation Loss for fold 8: 0.07098752881089847\n",
      "Validation Loss for fold 8: 0.08088287214438121\n",
      "Validation Loss for fold 8: 0.07612577577431996\n",
      "Validation Loss for fold 8: 0.08107663939396541\n",
      "Validation Loss for fold 8: 0.08282896627982457\n",
      "Validation Loss for fold 8: 0.08745203167200089\n",
      "Validation Loss for fold 8: 0.07473729054133098\n",
      "Validation Loss for fold 8: 0.0827033097545306\n",
      "Validation Loss for fold 8: 0.07728753238916397\n",
      "Validation Loss for fold 8: 0.07571533570686977\n",
      "Validation Loss for fold 8: 0.07662973801294963\n",
      "Validation Loss for fold 8: 0.07820253819227219\n",
      "Validation Loss for fold 8: 0.08341314891974132\n",
      "Validation Loss for fold 8: 0.07717426617940266\n",
      "Validation Loss for fold 8: 0.0786977286140124\n",
      "Validation Loss for fold 8: 0.07926987111568451\n",
      "Validation Loss for fold 8: 0.07576400289932887\n",
      "Validation Loss for fold 8: 0.07054229825735092\n",
      "Validation Loss for fold 8: 0.07578216989835103\n",
      "Validation Loss for fold 8: 0.06861021493872006\n",
      "Validation Loss for fold 8: 0.06922839830319087\n",
      "Validation Loss for fold 8: 0.08299519990881284\n",
      "Validation Loss for fold 8: 0.07458877315123875\n",
      "Validation Loss for fold 8: 0.0677511878311634\n",
      "--------------------------------\n",
      "FOLD 9\n",
      "--------------------------------\n",
      "Validation Loss for fold 9: 0.2663743446270625\n",
      "Validation Loss for fold 9: 0.2507115850845973\n",
      "Validation Loss for fold 9: 0.20501511295636496\n",
      "Validation Loss for fold 9: 0.17218842109044394\n",
      "Validation Loss for fold 9: 0.18033609290917715\n",
      "Validation Loss for fold 9: 0.147371677060922\n",
      "Validation Loss for fold 9: 0.16276669998963675\n",
      "Validation Loss for fold 9: 0.15114615360895792\n",
      "Validation Loss for fold 9: 0.1535461743672689\n",
      "Validation Loss for fold 9: 0.1445480932792028\n",
      "Validation Loss for fold 9: 0.13691225896279016\n",
      "Validation Loss for fold 9: 0.13074981172879538\n",
      "Validation Loss for fold 9: 0.15578575183947882\n",
      "Validation Loss for fold 9: 0.12624998142321905\n",
      "Validation Loss for fold 9: 0.14692929138739905\n",
      "Validation Loss for fold 9: 0.12535744905471802\n",
      "Validation Loss for fold 9: 0.12258365998665492\n",
      "Validation Loss for fold 9: 0.12387734154860179\n",
      "Validation Loss for fold 9: 0.12182507663965225\n",
      "Validation Loss for fold 9: 0.1172321264942487\n",
      "Validation Loss for fold 9: 0.1299282362063726\n",
      "Validation Loss for fold 9: 0.10937696695327759\n",
      "Validation Loss for fold 9: 0.12350291510423024\n",
      "Validation Loss for fold 9: 0.12513293325901031\n",
      "Validation Loss for fold 9: 0.11487359801928203\n",
      "Validation Loss for fold 9: 0.11494207382202148\n",
      "Validation Loss for fold 9: 0.11105587830146153\n",
      "Validation Loss for fold 9: 0.10898732642332713\n",
      "Validation Loss for fold 9: 0.12101296832164128\n",
      "Validation Loss for fold 9: 0.11555242290099461\n",
      "Validation Loss for fold 9: 0.10250103722016017\n",
      "Validation Loss for fold 9: 0.10533498972654343\n",
      "Validation Loss for fold 9: 0.09986980259418488\n",
      "Validation Loss for fold 9: 0.10497244695822398\n",
      "Validation Loss for fold 9: 0.11104349543650945\n",
      "Validation Loss for fold 9: 0.11062781512737274\n",
      "Validation Loss for fold 9: 0.10245700180530548\n",
      "Validation Loss for fold 9: 0.10730104396740596\n",
      "Validation Loss for fold 9: 0.10824116319417953\n",
      "Validation Loss for fold 9: 0.10645498832066853\n",
      "Validation Loss for fold 9: 0.09551555663347244\n",
      "Validation Loss for fold 9: 0.09500393519798915\n",
      "Validation Loss for fold 9: 0.09615154564380646\n",
      "Validation Loss for fold 9: 0.10255424429972966\n",
      "Validation Loss for fold 9: 0.1044324313600858\n",
      "Validation Loss for fold 9: 0.10163671026627223\n",
      "Validation Loss for fold 9: 0.09381293008724849\n",
      "Validation Loss for fold 9: 0.10647294173638026\n",
      "Validation Loss for fold 9: 0.09047406166791916\n",
      "Validation Loss for fold 9: 0.09751306225856145\n",
      "Validation Loss for fold 9: 0.1029243916273117\n",
      "Validation Loss for fold 9: 0.09306550274292628\n",
      "Validation Loss for fold 9: 0.09436683108409245\n",
      "Validation Loss for fold 9: 0.10021619498729706\n",
      "Validation Loss for fold 9: 0.08760386953751247\n",
      "Validation Loss for fold 9: 0.0907986784974734\n",
      "Validation Loss for fold 9: 0.08946157743533452\n",
      "Validation Loss for fold 9: 0.09367436667283376\n",
      "Validation Loss for fold 9: 0.08428722868363063\n",
      "Validation Loss for fold 9: 0.08223907773693402\n",
      "Validation Loss for fold 9: 0.09379445264736812\n",
      "Validation Loss for fold 9: 0.08687933534383774\n",
      "Validation Loss for fold 9: 0.08240596329172452\n",
      "Validation Loss for fold 9: 0.08382425208886464\n",
      "Validation Loss for fold 9: 0.0864877129594485\n",
      "Validation Loss for fold 9: 0.0868028129140536\n",
      "Validation Loss for fold 9: 0.08099322269360225\n",
      "Validation Loss for fold 9: 0.09227420886357625\n",
      "Validation Loss for fold 9: 0.08239663889010747\n",
      "Validation Loss for fold 9: 0.09373180071512859\n",
      "Validation Loss for fold 9: 0.08401001741488774\n",
      "Validation Loss for fold 9: 0.08757683883110683\n",
      "Validation Loss for fold 9: 0.08350772162278493\n",
      "Validation Loss for fold 9: 0.09073926756779353\n",
      "Validation Loss for fold 9: 0.08785661806662877\n",
      "Validation Loss for fold 9: 0.08123386402924855\n",
      "Validation Loss for fold 9: 0.08884728948275249\n",
      "Validation Loss for fold 9: 0.08176679412523906\n",
      "Validation Loss for fold 9: 0.07835390667120616\n",
      "Validation Loss for fold 9: 0.07022236660122871\n",
      "Validation Loss for fold 9: 0.09140843401352565\n",
      "Validation Loss for fold 9: 0.08746267358462016\n",
      "Validation Loss for fold 9: 0.08374143888552983\n",
      "Validation Loss for fold 9: 0.08258145675063133\n",
      "Validation Loss for fold 9: 0.0820596640308698\n",
      "Validation Loss for fold 9: 0.07894583294788997\n",
      "Validation Loss for fold 9: 0.08039917796850204\n",
      "Validation Loss for fold 9: 0.0758450689415137\n",
      "Validation Loss for fold 9: 0.07918674995501836\n",
      "Validation Loss for fold 9: 0.07935836911201477\n",
      "Validation Loss for fold 9: 0.08056273808081944\n",
      "Validation Loss for fold 9: 0.0797814279794693\n",
      "Validation Loss for fold 9: 0.08874345074097316\n",
      "Validation Loss for fold 9: 0.07045987620949745\n",
      "Validation Loss for fold 9: 0.07311919083197911\n",
      "Validation Loss for fold 9: 0.07467050353686015\n",
      "Validation Loss for fold 9: 0.07453733434279759\n",
      "Validation Loss for fold 9: 0.07563312103350957\n",
      "Validation Loss for fold 9: 0.07814194510380428\n",
      "Validation Loss for fold 9: 0.07214268669486046\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Avg Validation Loss</td><td>▃▂▂▁█▃▂▁▄▂▂▂▃▂▁▂█▄▃▂▂▂▂▁▂▂▁▁▂▁▁▁▃▂▂▁▃▂▂▂</td></tr><tr><td>Avg. Training Loss</td><td>▆▂▁▁█▃▂▁█▃▂▂▅▂▂▁█▂▂▁▅▂▁▁▄▂▁▁▃▂▁▁▅▂▁▁▄▂▁▁</td></tr><tr><td>Epoch</td><td>▁▃▅▇▁▃▅▇▁▃▅▇▂▃▅▇▁▄▆▇▂▃▅█▂▄▆▇▂▄▆█▂▄▆█▂▄▆█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Avg Validation Loss</td><td>0.07214</td></tr><tr><td>Avg. Training Loss</td><td>0.05818</td></tr><tr><td>Epoch</td><td>99</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sage-waterfall-34</strong> at: <a href='https://wandb.ai/jcrich/collaborative%20filter%20model/runs/qoalaxpy' target=\"_blank\">https://wandb.ai/jcrich/collaborative%20filter%20model/runs/qoalaxpy</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240311_221428-qoalaxpy/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Correct variable names and logic\n",
    "for fold, (train_ids, val_ids) in enumerate(kfold.split(train_ds)):\n",
    "    print(f'FOLD {fold}')\n",
    "    print('--------------------------------')\n",
    "\n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "    val_subsampler = torch.utils.data.SubsetRandomSampler(val_ids)\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=train_subsampler)\n",
    "    valloader = torch.utils.data.DataLoader(validation_ds, batch_size=BATCH_SIZE, sampler=val_subsampler)\n",
    "\n",
    "    model = MLPCollaborativeFilter(num_users + 1, num_movies + 1, embedding_dim=FEATURES)\n",
    "    optimiser = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for user_indices, item_indices, ratings in trainloader:\n",
    "            optimiser.zero_grad()\n",
    "            outputs = model(user_indices, item_indices).squeeze()\n",
    "            loss = criterion(outputs, ratings)\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Average training loss over all batches\n",
    "        train_loss /= len(trainloader)\n",
    "        # Log training loss for the current epoch\n",
    "        wandb.log({\"Avg. Training Loss\": train_loss, \"Epoch\": epoch})\n",
    "        mlflow.log_metric(\"Avg. Training Loss\", train_loss, step=epoch)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for user_indices, item_indices, ratings in valloader:\n",
    "                outputs = model(user_indices, item_indices).squeeze()\n",
    "                loss = criterion(outputs, ratings)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        avg_validation_loss = val_loss / len(valloader)\n",
    "        print(f'Validation Loss for fold {fold}: {avg_validation_loss}')\n",
    "        \n",
    "        if avg_validation_loss < best_val_loss:\n",
    "            best_val_loss = avg_validation_loss\n",
    "            mlflow.log_metric(\"Best Validation Loss\", best_val_loss, step=epoch)\n",
    "            mlflow.pytorch.log_model(model, \"model\")\n",
    "            # Save model state\n",
    "            torch.save(model.state_dict(), 'best_model_state.pth')\n",
    "            # If you also want to save the optimizer state along with the model:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimiser_state_dict': optimiser.state_dict(),\n",
    "                'loss': avg_validation_loss,\n",
    "            }, 'best_col_model_checkpoint.pth')\n",
    "            # Log the model checkpoint as an artifact\n",
    "            mlflow.log_artifact('best_col_model_checkpoint.pth')\n",
    "        \n",
    "        # Log validation loss for the current epoch\n",
    "        wandb.log({\"Avg Validation Loss\": avg_validation_loss, \"Epoch\": epoch})\n",
    "        mlflow.log_metric(\"Avg Validation Loss\", avg_validation_loss, step=epoch)\n",
    "    \n",
    "    print('--------------------------------')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "63a5e18c-653c-4d10-9bac-b3669a38c95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07417693871416543\n"
     ]
    }
   ],
   "source": [
    "#test data\n",
    "model.eval()  # Switch to evaluation mode\n",
    "test_loss = 0\n",
    "correct_predictions = 0  # Fixed variable name for clarity\n",
    "total_contexts = 0\n",
    "\n",
    "with torch.no_grad():  # Disable gradient computation\n",
    "    for user_ids, movie_ids, ratings in test_data_loader:\n",
    "        predictions = model(user_ids,movie_ids)  # Generate predictions\n",
    "        # Calculate and accumulate loss\n",
    "        loss = criterion(predictions, ratings)\n",
    "        test_loss += loss.item()\n",
    "        \n",
    "        # # Reshape predictions to match [batch_size, context_size, vocab_size]\n",
    "        # predictions = predictions.view(-1, context_size, VOCAB_SIZE)\n",
    "        \n",
    "        # # Get top prediction for each context position\n",
    "        # top_predictions = predictions.argmax(dim=2)\n",
    "        \n",
    "        # # Calculate correct predictions\n",
    "        # correct_preds = (top_predictions == context).float().sum()\n",
    "        # correct_predictions += correct_preds.item()  # Accumulate correct predictions\n",
    "        \n",
    "        # total_contexts += context.numel()  # Total number of context word positions evaluated\n",
    "\n",
    "# Calculate final metrics\n",
    "test_loss /= len(test_data_loader)  # Average loss per batch\n",
    "# print('correct predictions = ',correct_predictions)\n",
    "# print('out of  = ',total_contexts)\n",
    "# accuracy = correct_predictions / total_contexts  # Compute accuracy\n",
    "\n",
    "# print(f'Test set: Average loss: {test_loss:.4f}, Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Evaluate the model\n",
    "# (This would involve using a separate validation set or performing cross-validation)\n",
    "print(test_loss)\n",
    "\n",
    "mlflow.log_metric('Post training test loss',test_loss)\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10373ba-5557-46bf-aa37-8f4234b43def",
   "metadata": {},
   "source": [
    "## Preference ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377c97ca-a1c8-4a79-ad7f-f844c2b6ab59",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_user_id = 140440102666832"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb9de61-95a2-4c4d-bae9-78f17254013c",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_uid = encoder.encode(target_user_id,encoder.user_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb44d161-b86c-48e5-a2a8-228ee34d61fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_features(uid):\n",
    "\n",
    "    for i in range(0,len(all_ds)):\n",
    "\n",
    "        u,m,r = all_ds[i]\n",
    "        # print(u)\n",
    "        if u == uid:\n",
    "            yield [m,r]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca61297-1afc-439e-9a3a-81bff33d384f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ratings(e_uid,e_mid,model):\n",
    "    e_uid_tensor = torch.tensor(e_uid, dtype=torch.int64).unsqueeze(0)\n",
    "    movie_eid_tensor = torch.tensor(e_mid,dtype=torch.int64).unsqueeze(0)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        user_rating = model(e_uid_tensor,movie_eid_tensor)\n",
    "        return movie_eid_tensor,user_rating\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3780e9b-1c32-4f2b-a45e-14bdb9d35b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get user and features\n",
    "seen_features = set(tuple(fe) for fe in get_user_features(e_uid)) # Get user features if needed\n",
    "seen_movies = set(sf[0].item() for sf in seen_features)\n",
    "all_movies = set([num for num in range(30)])\n",
    "unseen_movies = all_movies - seen_movies\n",
    "unseen_features = {tuple(predict_ratings(e_uid,um,model)) for um in unseen_movies}\n",
    "all_features = seen_features | unseen_features\n",
    "\n",
    "sorted_set = sorted(all_features, key=lambda x: x[1])\n",
    "top_n = 10\n",
    "recommendations = list(filter(lambda x: x[0].item() in unseen_movies,sorted_set))\n",
    "n_recommendations = list(map(lambda x: x[0],recommendations[-1:top_n*-1:-1]))\n",
    "n_recommendations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3470ac7f-df21-40aa-94aa-32aa93606c9a",
   "metadata": {},
   "source": [
    "# Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9289eb5-57c3-42bd-ba7f-7c519590d8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all users, with all movie ratings.\n",
    "def generate_all_ratings(model,num_users,num_movies):\n",
    "    #build matrix\n",
    "\n",
    "    #data already encoded....\n",
    "    all_data = [['dummy',mnm,unm,'dummy','0/10'] for unm in range(0,num_users) for mnm in range(0,num_movies)]\n",
    "    # print(all_data)\n",
    "    # print([em for em in all_data])\n",
    "    fake_encoder = Encoder([did[2] for did in all_data],[did[1] for did in all_data])\n",
    "    all_ds = CFDataset(all_data,fake_encoder)\n",
    "    \n",
    "    data_loader = DataLoader(all_ds, batch_size=1)\n",
    "    for uid,mid,ra in data_loader:\n",
    "        # print(uid,mid)\n",
    "        prediction = model(uid,mid)\n",
    "        yield (uid.item(),mid.item(),prediction.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788b3ed2-c265-4b06-a5b9-53fa0c63b904",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ratings_tensor(data, num_users, num_movies):\n",
    "    # Initialize an empty tensor to hold the ratings\n",
    "    ratings_tensor = torch.zeros(num_users, num_movies)\n",
    "    \n",
    "    # Iterate over the data and fill the tensor\n",
    "    for entry in data:\n",
    "        user_id, movie_id, rating = entry\n",
    "        # Convert rating to float\n",
    "        rating = float(rating)\n",
    "        ratings_tensor[user_id, movie_id] = rating\n",
    "    \n",
    "    return ratings_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd12391-a846-475e-be55-b2516a0ef7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Generate Predictions\n",
    "predictions = [ra for ra in generate_all_ratings(model,num_users,num_movies)]\n",
    "# print(predictions)\n",
    "ratings_tensor = create_ratings_tensor(predictions,num_users,num_movies)\n",
    "print(ratings_tensor)\n",
    "# predictions[0]\n",
    "\n",
    "# ratings_tensor.shape\n",
    "\n",
    "# ratings_tensor[0]\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Assuming predicted_ratings is your tensor of predicted ratings\n",
    "# predicted_ratings.shape should be (num_users, num_movies)\n",
    "\n",
    "# # Calculate cosine similarity\n",
    "# user_similarities = F.cosine_similarity(ratings_tensor, ratings_tensor, dim=1)\n",
    "\n",
    "# user_similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d0c474-3481-489e-b494-047c64b14925",
   "metadata": {},
   "source": [
    "## COSINE SIMILARITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cae1de-bc62-460e-b2db-f046961f4493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Normalize predicted ratings\n",
    "normalized_ratings = F.normalize(ratings_tensor, p=2, dim=1)\n",
    "\n",
    "# Step 2: Calculate cosine similarity\n",
    "user_similarities = torch.matmul(normalized_ratings, normalized_ratings.T)\n",
    "\n",
    "# Set diagonal elements to a large negative value to exclude self-similarity\n",
    "user_similarities.fill_diagonal_(-float('inf'))\n",
    "\n",
    "# You can optionally convert the similarities tensor to a numpy array for easier manipulation\n",
    "user_similarities_np = user_similarities.numpy()\n",
    "\n",
    "# Print or use the user similarities tensor\n",
    "print(user_similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb89e2b-202f-405a-a732-26cf09959925",
   "metadata": {},
   "source": [
    "### Matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e402f0-b39d-4599-9617-5fcf23d00a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_matches = {}\n",
    "for i in range(len(user_similarities)):\n",
    "    # Sort similarities for the current user i\n",
    "    ranked_users = torch.argsort(torch.tensor(user_similarities[i]), descending=True)\n",
    "    # Exclude self from top matches (optional)\n",
    "    top_matches[i] = ranked_users[ranked_users != i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d560016-14da-4170-be81-43e27b094043",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebc2399-8831-4bef-b722-b9405c776b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.decode(71,encoder.idx_to_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cb1dfe-e30f-407f-accb-367b5bce66a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#decoded\n",
    "\n",
    "decoded_matches = {encoder.decode(k,encoder.idx_to_user):[encoder.decode(ve,encoder.idx_to_user) for ve in v.tolist()] for k,v in top_matches.items()}\n",
    "decoded_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73632e60-b177-479e-8e78-c6eea82ea863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Select Top Matches\n",
    "N = 5  # Number of top matches to select\n",
    "for user, similar_users in decoded_matches.items():\n",
    "    decoded_matches[user] = similar_users[:N]\n",
    "\n",
    "decoded_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7f3715-96e0-44a4-9a49-eee4e94bfaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#decode...."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c7c0fa-b24f-4418-b5ca-e1713254be27",
   "metadata": {},
   "source": [
    "## FINALLY FIND THE MATCHES...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd884b67-4541-453a-8bdf-87adf9d4ca39",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_user = 140440102666832\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0929f29-14e7-4cfa-acc9-9a741b7a5187",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_matches[target_user]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ed138f-43a0-4e9c-a66d-c7030de6e662",
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbour = 140440115331424"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a6a7ef-f019-4273-bc7c-c63dae3a104d",
   "metadata": {},
   "source": [
    "## Now see how they relate to the database..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b89fe2d-944d-43bc-b2a2-81af64bd6ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = db.get_table_values('Reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df7b55f-d93f-4a8d-b4c3-f0b1ea9ddf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2b3f3b-a7f8-48a6-b772-23c522f5ab85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reviews_info(user_id,reviews):\n",
    "    # print(db.get_table_values('Users'))\n",
    "    info = {}\n",
    "    info[user_id] = list(filter(lambda x :x[0] == user_id,db.get_table_values('Users')))[0][1]\n",
    "\n",
    "    \n",
    "    #TODO finish work...\n",
    "    mov_tab = db.get_table_values('Movies')\n",
    "    \n",
    "    for review in reviews:\n",
    "        mov_id = review[1]\n",
    "        info[mov_id] = {'title':list(filter(lambda x :x[0] == mov_id,mov_tab))[0][1],'rating':review[4]}\n",
    "        \n",
    "    return info\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef46e616-1299-4659-8e09-f6d3fdf44b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89593c73-41e3-4a6e-8e9c-61fd41b372dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_reviews = list(filter(lambda rv: rv[2] == target_user,reviews))\n",
    "neighbour_reviews = list(filter(lambda rv: rv[2] == neighbour,reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fd7bda-4f06-41de-89d2-7e84ce851863",
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_info = reviews_info(target_user,target_reviews)\n",
    "neigh_info = reviews_info(neighbour,neighbour_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cf7940-a7bf-4443-87b6-abd8d7d47bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78c8ccb-97d3-439a-ac8d-28ebe5e8ba91",
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6864fda6-f897-4c0a-8cf8-405e8cadd208",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae878490-881c-4345-9040-934a38615e40",
   "metadata": {},
   "source": [
    "# SAVE_MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef1a3d5-a607-4a92-82a3-237a07f15ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653f0e2d-f90d-4bc4-b72c-2dfda1b2f542",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "menv",
   "language": "python",
   "name": "menv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
